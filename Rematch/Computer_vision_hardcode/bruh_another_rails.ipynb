{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acbfa59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inversed filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e3ddd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 01:56:26.815 Python[40138:28838161] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-05 01:56:26.815 Python[40138:28838161] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40  # red tint on rail mask\n",
    "\n",
    "# Color/size filter (RGB targets; converted to BGR internally)\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0      # color distance (BGR) – increase if too strict\n",
    "MIN_REGION_SIZE   = 30        # connected-component area in px\n",
    "MIN_REGION_HEIGHT = 150       # connected-component bbox height in px\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warmup)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(\n",
    "    _dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "    verbose=False, half=half\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# FAST color+size filter (vectorized + ROI-cropped)\n",
    "# =======================\n",
    "def highlight_rails_mask_only_fast(\n",
    "    img_bgr: np.ndarray,\n",
    "    rail_mask: np.ndarray,\n",
    "    target_colors_rgb: list[tuple[int,int,int]],\n",
    "    tolerance: float = 30.0,\n",
    "    min_region_size: int = 50,\n",
    "    min_region_height: int = 150\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns boolean mask of rail pixels that:\n",
    "      (1) color-match any target color within tolerance, AND\n",
    "      (2) lie inside rail_mask, AND\n",
    "      (3) pass connected-component size/height filters.\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    # Crop to rail ROI\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    # Vectorized color match\n",
    "    targets_bgr = np.array([(b,g,r) for r,g,b in target_colors_rgb], np.float32)\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]\n",
    "    dist2 = np.sum(diff * diff, axis=-1)\n",
    "    color_mask_roi = np.any(dist2 <= (tolerance * tolerance), axis=-1)\n",
    "\n",
    "    combined_roi = mask_roi & color_mask_roi\n",
    "\n",
    "    # Connected-component filter\n",
    "    comp_u8 = combined_roi.astype(np.uint8)\n",
    "    n_labels, labels, stats, _ = cv2.connectedComponentsWithStats(comp_u8, 8)\n",
    "\n",
    "    filtered_roi = np.zeros_like(combined_roi)\n",
    "    for lbl in range(1, n_labels):\n",
    "        area   = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        height = stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and height >= min_region_height:\n",
    "            filtered_roi[labels == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = filtered_roi\n",
    "    return filtered_full\n",
    "\n",
    "# =======================\n",
    "# Per-frame pipeline\n",
    "# =======================\n",
    "def process_frame(img_bgr: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - out        : original + red tint on rails + neon-green on unfiltered rails\n",
    "      - rail_mask  : boolean mask of all rails\n",
    "    \"\"\"\n",
    "    res = model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "        conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "        max_det=20, verbose=False, half=half\n",
    "    )[0]\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:\n",
    "        return img_bgr.copy(), np.zeros((H, W), bool)\n",
    "\n",
    "    # union of rail segments\n",
    "    m = res.masks.data\n",
    "    union = (m.sum(dim=0) > 0).float().cpu().numpy()\n",
    "    if union.shape != (H, W):\n",
    "        union = cv2.resize(union, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "    rail_mask = union.astype(bool)\n",
    "\n",
    "    # compute filtered (original neon-green) mask\n",
    "    filtered_mask = highlight_rails_mask_only_fast(\n",
    "        img_bgr, rail_mask,\n",
    "        TARGET_COLORS_RGB, TOLERANCE,\n",
    "        MIN_REGION_SIZE, MIN_REGION_HEIGHT\n",
    "    )\n",
    "\n",
    "    # inverted mask = rails minus the filtered zones\n",
    "    inverted_mask = rail_mask & ~filtered_mask\n",
    "\n",
    "    # compose visualization\n",
    "    out = img_bgr.copy()\n",
    "\n",
    "    # red tint for rails\n",
    "    overlay = out.copy()\n",
    "    overlay[rail_mask] = (0, 0, 255)\n",
    "    out = cv2.addWeighted(overlay, ALPHA, out, 1 - ALPHA, 0)\n",
    "\n",
    "    # neon-green on the *inverse* region\n",
    "    out[inverted_mask] = (0, 255, 0)\n",
    "\n",
    "    return out, rail_mask\n",
    "\n",
    "# =======================\n",
    "# Main: run & step through\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    image_dir = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(image_dir, \"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found in\", image_dir)\n",
    "        sys.exit(1)\n",
    "\n",
    "    for p in paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        vis, _ = process_frame(img)\n",
    "\n",
    "        # show original | inverted neon-green\n",
    "        combo = np.hstack((img, vis))\n",
    "        cv2.imshow(\"Original (Left) | Inverted Rails (Right)\", combo)\n",
    "\n",
    "        # SPACE → next, 'q' → quit\n",
    "        while True:\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            if key == 32:\n",
    "                break\n",
    "            elif key == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6ad26c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 01:30:39.590 Python[38613:28807275] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-05 01:30:39.590 Python[38613:28807275] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, glob, sys, time\n",
    "import cv2, torch, numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40  # red tint on rail mask\n",
    "\n",
    "# Color/size filter (RGB targets; converted to BGR internally)\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0      # color distance (BGR) – increase if too strict\n",
    "MIN_REGION_SIZE   = 30        # connected-component area in px\n",
    "MIN_REGION_HEIGHT = 150       # connected-component bbox height in px\n",
    "\n",
    "# Centerline settings\n",
    "N_LINES           = 3\n",
    "MIN_GAP_FRAC      = 0.15      # min horizontal separation between lines (fraction of width)\n",
    "BAND_FRAC         = 0.04      # half-width (as fraction of image width) for local centroid band\n",
    "Y_TOP_FRAC        = 0.25      # start tracing from this fraction downwards (perspective)\n",
    "BOTTOM_FOCUS_FRAC = 0.60      # use bottom 60% to score column density\n",
    "THICKNESS         = 6\n",
    "PURPLE            = (255, 0, 255)  # BGR\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warmup)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(\n",
    "    _dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "    verbose=False, half=half\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# FAST color+size filter (vectorized + ROI-cropped)\n",
    "# =======================\n",
    "def highlight_rails_mask_only_fast(\n",
    "    img_bgr: np.ndarray,\n",
    "    rail_mask: np.ndarray,\n",
    "    target_colors_rgb: list[tuple[int,int,int]],\n",
    "    tolerance: float = 30.0,\n",
    "    min_region_size: int = 50,\n",
    "    min_region_height: int = 150\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns boolean mask of rail pixels that:\n",
    "      (1) color-match any target color within tolerance, AND\n",
    "      (2) lie inside rail_mask, AND\n",
    "      (3) pass connected-component size/height filters.\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    # RGB -> BGR\n",
    "    targets_bgr = np.array([(b,g,r) for r,g,b in target_colors_rgb], np.float32)\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]\n",
    "    dist2 = np.sum(diff * diff, axis=-1)\n",
    "    color_mask_roi = np.any(dist2 <= (tolerance * tolerance), axis=-1)\n",
    "\n",
    "    combined_roi = mask_roi & color_mask_roi\n",
    "\n",
    "    comp_u8 = combined_roi.astype(np.uint8)\n",
    "    n_labels, labels, stats, _ = cv2.connectedComponentsWithStats(comp_u8, 8)\n",
    "\n",
    "    filtered_roi = np.zeros_like(combined_roi)\n",
    "    for lbl in range(1, n_labels):\n",
    "        area   = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        height = stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and height >= min_region_height:\n",
    "            filtered_roi[labels == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = filtered_roi\n",
    "    return filtered_full\n",
    "\n",
    "# =======================\n",
    "# Column-peak picking (no SciPy)\n",
    "# =======================\n",
    "def pick_top_columns(mask: np.ndarray, n: int, min_gap: int) -> list[int]:\n",
    "    \"\"\"\n",
    "    Greedily pick up to n columns with highest vertical density, enforcing min_gap separation.\n",
    "    Uses bottom portion of the image to score columns (more reliable).\n",
    "    \"\"\"\n",
    "    H, W = mask.shape\n",
    "    y_start = int(H * (1 - BOTTOM_FOCUS_FRAC))\n",
    "    dens = mask[y_start:, :].sum(axis=0).astype(np.float32)\n",
    "\n",
    "    order = np.argsort(-dens)  # descending\n",
    "    chosen = []\n",
    "    for x in order:\n",
    "        if dens[x] <= 0:\n",
    "            break\n",
    "        if all(abs(int(x) - int(c)) >= min_gap for c in chosen):\n",
    "            chosen.append(int(x))\n",
    "            if len(chosen) == n:\n",
    "                break\n",
    "    chosen.sort()\n",
    "    return chosen\n",
    "\n",
    "def trace_centerline(mask: np.ndarray, x0: int, band_w: int,\n",
    "                     y_top: int, step: int = 4, smooth: float = 0.6) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Follow dense green region from bottom to y_top by taking the centroid within [x0-band_w, x0+band_w].\n",
    "    Returns Nx1x2 array of int points for cv2.polylines.\n",
    "    \"\"\"\n",
    "    H, W = mask.shape\n",
    "    pts = []\n",
    "    x_prev = x0\n",
    "\n",
    "    for y in range(H-1, y_top-1, -step):\n",
    "        xl = max(0, x_prev - band_w)\n",
    "        xr = min(W, x_prev + band_w + 1)\n",
    "        row = mask[y, xl:xr]\n",
    "        if row.any():\n",
    "            xs = np.nonzero(row)[0] + xl\n",
    "            x_c = xs.mean()\n",
    "            x_prev = int(round(smooth * x_prev + (1.0 - smooth) * x_c))\n",
    "        # if no pixels, keep previous x_prev (continues straight)\n",
    "        pts.append([x_prev, y])\n",
    "\n",
    "    if len(pts) >= 2:\n",
    "        return np.array(pts, dtype=np.int32).reshape(-1, 1, 2)\n",
    "    return np.empty((0, 1, 2), dtype=np.int32)\n",
    "\n",
    "def draw_three_centerlines(out_img: np.ndarray, green_mask: np.ndarray) -> None:\n",
    "    \"\"\"\n",
    "    Find 3 dense green centerlines and draw them in purple on out_img in-place.\n",
    "    \"\"\"\n",
    "    H, W = green_mask.shape\n",
    "    min_gap = max(12, int(W * MIN_GAP_FRAC))\n",
    "    band_w  = max(6,  int(W * BAND_FRAC))\n",
    "    y_top   = int(H * Y_TOP_FRAC)\n",
    "\n",
    "    xs = pick_top_columns(green_mask, N_LINES, min_gap)\n",
    "    for x0 in xs:\n",
    "        poly = trace_centerline(green_mask, x0, band_w, y_top)\n",
    "        if poly.size > 0:\n",
    "            cv2.polylines(out_img, [poly], isClosed=False, color=PURPLE, thickness=THICKNESS, lineType=cv2.LINE_AA)\n",
    "\n",
    "# =======================\n",
    "# Per-frame pipeline\n",
    "# =======================\n",
    "def process_frame(img_bgr: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - out        : original + rails red tint + inverted green + 3 purple centerlines\n",
    "      - rail_mask  : boolean mask of all rails\n",
    "    \"\"\"\n",
    "    res = model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "        conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "        max_det=20, verbose=False, half=half\n",
    "    )[0]\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:\n",
    "        return img_bgr.copy(), np.zeros((H, W), bool)\n",
    "\n",
    "    # union of rail segments\n",
    "    m = res.masks.data\n",
    "    union = (m.sum(dim=0) > 0).float().cpu().numpy()\n",
    "    if union.shape != (H, W):\n",
    "        union = cv2.resize(union, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "    rail_mask = union.astype(bool)\n",
    "\n",
    "    # compute filtered (original neon-green) mask\n",
    "    filtered_mask = highlight_rails_mask_only_fast(\n",
    "        img_bgr, rail_mask,\n",
    "        TARGET_COLORS_RGB, TOLERANCE,\n",
    "        MIN_REGION_SIZE, MIN_REGION_HEIGHT\n",
    "    )\n",
    "\n",
    "    # inverted mask = rails minus the filtered zones (this is your green background)\n",
    "    inverted_mask = rail_mask & ~filtered_mask\n",
    "\n",
    "    # compose visualization\n",
    "    out = img_bgr.copy()\n",
    "\n",
    "    # red tint for rails\n",
    "    overlay = out.copy()\n",
    "    overlay[rail_mask] = (0, 0, 255)\n",
    "    out = cv2.addWeighted(overlay, ALPHA, out, 1 - ALPHA, 0)\n",
    "\n",
    "    # neon-green on the *inverse* region\n",
    "    out[inverted_mask] = (0, 255, 0)\n",
    "\n",
    "    # --- draw 3 purple lines in the densest green regions (no overlap) ---\n",
    "    draw_three_centerlines(out, inverted_mask)\n",
    "\n",
    "    return out, rail_mask\n",
    "\n",
    "# =======================\n",
    "# Main: run & step through\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    image_dir = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(image_dir, \"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found in\", image_dir)\n",
    "        sys.exit(1)\n",
    "\n",
    "    for p in paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        vis, _ = process_frame(img)\n",
    "\n",
    "        combo = np.hstack((img, vis))\n",
    "        cv2.imshow(\"Original (Left) | Inverted Rails + 3 Purple Lines (Right)\", combo)\n",
    "\n",
    "        # SPACE → next, 'q' → quit\n",
    "        while True:\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            if key == 32:\n",
    "                break\n",
    "            elif key == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39728424",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap straight "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "701f5aed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 02:03:22.587 Python[40446:28844547] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-05 02:03:22.587 Python[40446:28844547] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 206\u001b[39m\n\u001b[32m    204\u001b[39m \u001b[38;5;66;03m# Wait for SPACE (next) or 'q' (quit)\u001b[39;00m\n\u001b[32m    205\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m206\u001b[39m     key = \u001b[43mcv2\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m & \u001b[32m0xFF\u001b[39m\n\u001b[32m    207\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m key == \u001b[32m32\u001b[39m:        \u001b[38;5;66;03m# SPACE\u001b[39;00m\n\u001b[32m    208\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, glob, sys, time\n",
    "import cv2, torch, numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40  # red tint on rail mask\n",
    "\n",
    "# Color/size filter (RGB targets; converted to BGR internally)\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0      # color distance (BGR) – increase if too strict\n",
    "MIN_REGION_SIZE   = 30        # connected-component area in px\n",
    "MIN_REGION_HEIGHT = 150       # connected-component bbox height in px\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warmup)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(\n",
    "    _dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "    verbose=False, half=half\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# FAST color+size filter (vectorized + ROI-cropped)\n",
    "# =======================\n",
    "def highlight_rails_mask_only_fast(\n",
    "    img_bgr: np.ndarray,\n",
    "    rail_mask: np.ndarray,\n",
    "    target_colors_rgb: list[tuple[int,int,int]],\n",
    "    tolerance: float = 30.0,\n",
    "    min_region_size: int = 50,\n",
    "    min_region_height: int = 150\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns boolean mask of rail pixels that:\n",
    "      (1) color-match any target color within tolerance, AND\n",
    "      (2) lie inside rail_mask, AND\n",
    "      (3) pass connected-component size/height filters.\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(rbg[2], rbg[1], rbg[0]) for rbg in target_colors_rgb],\n",
    "                           dtype=np.float32)  # (K,3)\n",
    "\n",
    "    img_f = img_roi.astype(np.float32)                                  # (h,w,3)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]        # (h,w,K,3)\n",
    "    dist2 = np.sum(diff * diff, axis=-1)                                 # (h,w,K)\n",
    "    color_mask_roi = np.any(dist2 <= (tolerance * tolerance), axis=-1)   # (h,w)\n",
    "\n",
    "    combined_roi = mask_roi & color_mask_roi\n",
    "\n",
    "    comp_u8 = combined_roi.astype(np.uint8)\n",
    "    n_labels, labels, stats, _ = cv2.connectedComponentsWithStats(comp_u8, 8)\n",
    "\n",
    "    filtered_roi = np.zeros_like(combined_roi)\n",
    "    for lbl in range(1, n_labels):\n",
    "        area   = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        height = stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and height >= min_region_height:\n",
    "            filtered_roi[labels == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = filtered_roi\n",
    "    return filtered_full\n",
    "\n",
    "# =======================\n",
    "# Heatmap (red-dominance vs green)\n",
    "# =======================\n",
    "def red_vs_green_heatmap(red_mask: np.ndarray, green_mask: np.ndarray,\n",
    "                         blur_ksize: int = 51) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Build a heatmap where high values (red) indicate regions with more *red* pixels than green,\n",
    "    and low values (blue) indicate fewer red pixels. Uses a large blur for local density.\n",
    "    \"\"\"\n",
    "    H, W = red_mask.shape\n",
    "    # convert to float densities then smooth\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    red_d   = cv2.blur(red_mask.astype(np.float32),   k)\n",
    "    green_d = cv2.blur(green_mask.astype(np.float32), k)\n",
    "    diff = red_d - green_d  # >0 => red-dominant, <0 => green-dominant\n",
    "\n",
    "    # normalize to 0..255 with zero centered around ~128 using symmetric scaling\n",
    "    amax = float(np.max(np.abs(diff))) + 1e-6\n",
    "    norm = (diff / (2.0 * amax) + 0.5)  # 0..1 with 0.5 at tie\n",
    "    norm_u8 = np.clip(norm * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    heat = cv2.applyColorMap(norm_u8, cv2.COLORMAP_JET)  # blue -> red\n",
    "    return heat\n",
    "\n",
    "# =======================\n",
    "# Per-frame pipeline\n",
    "# =======================\n",
    "def process_frame(img_bgr: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - out_bgr     : original with red rail mask + neon green filtered regions\n",
    "      - rail_mask   : boolean rail mask\n",
    "      - heatmap_bgr : red-vs-green heatmap (blue=less red, red=more red)\n",
    "    \"\"\"\n",
    "    res = model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "        conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "        max_det=20, verbose=False, half=half\n",
    "    )[0]\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:\n",
    "        empty = np.zeros((H, W), bool)\n",
    "        heat  = red_vs_green_heatmap(empty, empty)\n",
    "        return img_bgr, empty, heat\n",
    "\n",
    "    # Union of rail masks\n",
    "    m = res.masks.data\n",
    "    union = (m.sum(dim=0) > 0).float().cpu().numpy()\n",
    "    if union.shape != (H, W):\n",
    "        union = cv2.resize(union, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "    rail_mask = union.astype(bool)\n",
    "\n",
    "    # Apply fast color/size filter inside rails (these are the green regions)\n",
    "    filtered_mask = highlight_rails_mask_only_fast(\n",
    "        img_bgr, rail_mask,\n",
    "        target_colors_rgb=TARGET_COLORS_RGB,\n",
    "        tolerance=TOLERANCE,\n",
    "        min_region_size=MIN_REGION_SIZE,\n",
    "        min_region_height=MIN_REGION_HEIGHT\n",
    "    )\n",
    "\n",
    "    # red area = rails but not filtered\n",
    "    red_area   = rail_mask & (~filtered_mask)\n",
    "    green_area = filtered_mask\n",
    "\n",
    "    # Compose visualization\n",
    "    out = img_bgr.copy()\n",
    "    overlay = out.copy()\n",
    "    overlay[rail_mask] = (0, 0, 255)       # red tint for all rails\n",
    "    out = cv2.addWeighted(overlay, ALPHA, out, 1 - ALPHA, 0)\n",
    "    out[filtered_mask] = (0, 255, 0)       # neon green for filtered regions\n",
    "\n",
    "    # Heatmap below: red-dominance vs green\n",
    "    heat = red_vs_green_heatmap(red_area, green_area)\n",
    "\n",
    "    return out, rail_mask, heat\n",
    "\n",
    "# =======================\n",
    "# Run over a folder: show side-by-side with heatmap underneath\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    image_dir = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(image_dir, \"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found in\", image_dir)\n",
    "        sys.exit(1)\n",
    "\n",
    "    for p in paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        vis, _rail, heat = process_frame(img)\n",
    "\n",
    "        # Top row: original | rails+filtered\n",
    "        top = np.hstack((img, vis))\n",
    "        # Bottom: heatmap stretched to same width\n",
    "        heat_resized = cv2.resize(heat, (top.shape[1], img.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        display_img = np.vstack((top, heat_resized))\n",
    "        cv2.imshow(\"Top: Original (Left) | Rails+Filtered (Right)  |  Bottom: Red-vs-Green Heatmap\", display_img)\n",
    "\n",
    "        # Wait for SPACE (next) or 'q' (quit)\n",
    "        while True:\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            if key == 32:        # SPACE\n",
    "                break\n",
    "            elif key == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343fcab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Heatmap overlay with rail ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b61b55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 02:14:05.905 Python[40926:28855408] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-05 02:14:05.905 Python[40926:28855408] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, glob, sys, time\n",
    "import cv2, torch, numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40  # red tint on rail mask\n",
    "\n",
    "# Color/size filter (RGB targets; converted to BGR internally)\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0      # color distance (BGR) – increase if too strict\n",
    "MIN_REGION_SIZE   = 30        # connected-component area in px\n",
    "MIN_REGION_HEIGHT = 150       # connected-component bbox height in px\n",
    "\n",
    "# ---- Heatmap→triangle picking params ----\n",
    "HEAT_BLUR_KSIZE     = 51            # local averaging\n",
    "RED_SCORE_THRESH    = 220           # 0..255; \"dark red\" threshold\n",
    "EXCLUDE_TOP_PX      = 400            # ignore top Y pixels\n",
    "EXCLUDE_BOTTOM_PX   = 120           # ignore bottom X pixels (from bottom up)\n",
    "MIN_DARK_RED_AREA   = 1200          # min area (px) for a dark-red blob\n",
    "TRI_SIZE_PX         = 18            # triangle size\n",
    "PURPLE              = (255, 0, 255) # BGR\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warmup)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(\n",
    "    _dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "    verbose=False, half=half\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# FAST color+size filter (vectorized + ROI-cropped)\n",
    "# =======================\n",
    "def highlight_rails_mask_only_fast(\n",
    "    img_bgr: np.ndarray,\n",
    "    rail_mask: np.ndarray,\n",
    "    target_colors_rgb: list[tuple[int,int,int]],\n",
    "    tolerance: float = 30.0,\n",
    "    min_region_size: int = 50,\n",
    "    min_region_height: int = 150\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns boolean mask of rail pixels that:\n",
    "      (1) color-match any target color within tolerance, AND\n",
    "      (2) lie inside rail_mask, AND\n",
    "      (3) pass connected-component size/height filters.\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    # RGB -> BGR targets\n",
    "    targets_bgr = np.array([(rbg[2], rbg[1], rbg[0]) for rbg in target_colors_rgb],\n",
    "                           dtype=np.float32)\n",
    "\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]\n",
    "    dist2 = np.sum(diff * diff, axis=-1)\n",
    "    color_mask_roi = np.any(dist2 <= (tolerance * tolerance), axis=-1)\n",
    "\n",
    "    combined_roi = mask_roi & color_mask_roi\n",
    "\n",
    "    comp_u8 = combined_roi.astype(np.uint8)\n",
    "    n_labels, labels, stats, _ = cv2.connectedComponentsWithStats(comp_u8, 8)\n",
    "\n",
    "    filtered_roi = np.zeros_like(combined_roi)\n",
    "    for lbl in range(1, n_labels):\n",
    "        area   = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        height = stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and height >= min_region_height:\n",
    "            filtered_roi[labels == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = filtered_roi\n",
    "    return filtered_full\n",
    "\n",
    "# =======================\n",
    "# Heatmap (red-dominance vs green) + score map\n",
    "# =======================\n",
    "def red_vs_green_heatmap(red_mask: np.ndarray, green_mask: np.ndarray,\n",
    "                         blur_ksize: int = 51) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Build a heatmap where high values (red) indicate regions with more *red* pixels than green,\n",
    "    and low values (blue) indicate fewer red pixels. Also returns the normalized score (0..255).\n",
    "    \"\"\"\n",
    "    # convert to float densities then smooth\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    red_d   = cv2.blur(red_mask.astype(np.float32),   k)\n",
    "    green_d = cv2.blur(green_mask.astype(np.float32), k)\n",
    "    diff = red_d - green_d  # >0 => red-dominant, <0 => green-dominant\n",
    "\n",
    "    # normalize to 0..255 with zero centered around ~128 using symmetric scaling\n",
    "    amax = float(np.max(np.abs(diff))) + 1e-6\n",
    "    norm = (diff / (2.0 * amax) + 0.5)  # 0..1 with 0.5 at tie\n",
    "    norm_u8 = np.clip(norm * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    heat = cv2.applyColorMap(norm_u8, cv2.COLORMAP_JET)  # blue -> red\n",
    "    return heat, norm_u8\n",
    "\n",
    "# =======================\n",
    "# Draw a small triangle marker with apex at (x, y)\n",
    "# =======================\n",
    "def draw_purple_triangle(img: np.ndarray, x: int, y: int, size: int = TRI_SIZE_PX):\n",
    "    h = int(size * 1.2)\n",
    "    pts = np.array([\n",
    "        [x, y],                    # apex\n",
    "        [x - size, y + h],         # base left\n",
    "        [x + size, y + h],         # base right\n",
    "    ], dtype=np.int32)\n",
    "    cv2.fillConvexPoly(img, pts, PURPLE)\n",
    "    cv2.polylines(img, [pts.reshape(-1,1,2)], True, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "# =======================\n",
    "# Per-frame pipeline\n",
    "# =======================\n",
    "def process_frame(img_bgr: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - out_bgr     : original with rail tint + green regions + purple triangles at rail ends\n",
    "      - rail_mask   : boolean rail mask\n",
    "      - heatmap_bgr : red-vs-green heatmap (blue=more green, red=more red)\n",
    "    \"\"\"\n",
    "    res = model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "        conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "        max_det=20, verbose=False, half=half\n",
    "    )[0]\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:\n",
    "        empty = np.zeros((H, W), bool)\n",
    "        heat, _ = red_vs_green_heatmap(empty, empty, blur_ksize=HEAT_BLUR_KSIZE)\n",
    "        return img_bgr.copy(), empty, heat\n",
    "\n",
    "    # union of rail segments\n",
    "    m = res.masks.data\n",
    "    union = (m.sum(dim=0) > 0).float().cpu().numpy()\n",
    "    if union.shape != (H, W):\n",
    "        union = cv2.resize(union, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "    rail_mask = union.astype(bool)\n",
    "\n",
    "    # compute filtered (green) mask\n",
    "    filtered_mask = highlight_rails_mask_only_fast(\n",
    "        img_bgr, rail_mask,\n",
    "        TARGET_COLORS_RGB, TOLERANCE,\n",
    "        MIN_REGION_SIZE, MIN_REGION_HEIGHT\n",
    "    )\n",
    "\n",
    "    # red area = rails minus the filtered zones\n",
    "    red_area   = rail_mask & (~filtered_mask)\n",
    "    green_area = filtered_mask\n",
    "\n",
    "    # Compose visualization\n",
    "    orig_marked = img_bgr.copy()             # triangles go on ORIGINAL\n",
    "    out = img_bgr.copy()                      # rails viz on the right panel\n",
    "    overlay = out.copy()\n",
    "    overlay[rail_mask] = (0, 0, 255)\n",
    "    out = cv2.addWeighted(overlay, ALPHA, out, 1 - ALPHA, 0)\n",
    "    out[filtered_mask] = (0, 255, 0)\n",
    "\n",
    "    # Heatmap + score (0..255, 255 = most red-dominant)\n",
    "    heat, score = red_vs_green_heatmap(red_area, green_area, blur_ksize=HEAT_BLUR_KSIZE)\n",
    "\n",
    "    # ---- pick dark-red blobs within Y-range and mark their uppermost point ----\n",
    "    score_mask = (score >= RED_SCORE_THRESH).astype(np.uint8)\n",
    "\n",
    "    # limit vertical search band\n",
    "    top = max(0, EXCLUDE_TOP_PX)\n",
    "    bot = max(0, EXCLUDE_BOTTOM_PX)\n",
    "    score_mask[:top, :] = 0\n",
    "    if bot > 0:\n",
    "        score_mask[H - bot:, :] = 0\n",
    "\n",
    "    # remove tiny speckle\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 9))\n",
    "    score_mask = cv2.morphologyEx(score_mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "\n",
    "    n_labels, labels, stats, _ = cv2.connectedComponentsWithStats(score_mask, 8)\n",
    "    for lbl in range(1, n_labels):\n",
    "        area = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        if area < MIN_DARK_RED_AREA:\n",
    "            continue\n",
    "\n",
    "        # uppermost y in this blob\n",
    "        mask_lbl = (labels == lbl)\n",
    "        ys, xs = np.where(mask_lbl)\n",
    "        y_min = int(ys.min())\n",
    "\n",
    "        # x at the top-most row for stability (mean of that row's xs)\n",
    "        xs_top = xs[ys == y_min]\n",
    "        x_mid = int(np.clip(int(np.round(xs_top.mean())), 0, W - 1))\n",
    "\n",
    "        # draw triangle on the ORIGINAL image\n",
    "        draw_purple_triangle(orig_marked, x_mid, y_min, TRI_SIZE_PX)\n",
    "\n",
    "    return (orig_marked, rail_mask, heat), out  # return both left(original+triangles) and right(viz), plus heat\n",
    "\n",
    "# =======================\n",
    "# Run over a folder: show side-by-side with heatmap underneath\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    image_dir = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(image_dir, \"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found in\", image_dir)\n",
    "        sys.exit(1)\n",
    "\n",
    "    for p in paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        (orig_with_tri, _rail, heat), rails_viz = process_frame(img)\n",
    "\n",
    "        # Top row: ORIGINAL(with triangles) | rails+filtered viz\n",
    "        top = np.hstack((orig_with_tri, rails_viz))\n",
    "\n",
    "        # Bottom: heatmap resized to match width\n",
    "        heat_resized = cv2.resize(heat, (top.shape[1], img.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "        display_img = np.vstack((top, heat_resized))\n",
    "        cv2.imshow(\"Top: Original + Rail-End Triangles (Left) | Rails+Filtered (Right)  |  Bottom: Red-vs-Green Heatmap\", display_img)\n",
    "\n",
    "        # SPACE → next, 'q' → quit\n",
    "        while True:\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            if key == 32:\n",
    "                break\n",
    "            elif key == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab96816f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#PEAK CODE BELOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba516b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 15:52:13.624 Python[41888:28864504] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-05 15:52:13.624 Python[41888:28864504] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, glob, sys, time\n",
    "import cv2, torch, numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40  # red tint on rail mask\n",
    "\n",
    "# Color/size filter (RGB targets; converted to BGR internally)\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0\n",
    "MIN_REGION_SIZE   = 30\n",
    "MIN_REGION_HEIGHT = 150\n",
    "\n",
    "# ---- Heatmap→triangle picking params ----\n",
    "HEAT_BLUR_KSIZE       = 51            # local averaging window\n",
    "RED_SCORE_THRESH      = 220           # 0..255; \"dark red\" threshold\n",
    "EXCLUDE_TOP_FRAC      = 0.40          # exclude top 10% of the image (before analysis)\n",
    "EXCLUDE_BOTTOM_FRAC   = 0.15          # exclude bottom 15% of the image (before analysis)\n",
    "MIN_DARK_RED_AREA     = 1200          # min area (px) for a dark-red blob\n",
    "TRI_SIZE_PX           = 18            # triangle size\n",
    "PURPLE                = (255, 0, 255) # BGR\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warmup)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(\n",
    "    _dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "    verbose=False, half=half\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# FAST color+size filter (vectorized + ROI-cropped)\n",
    "# =======================\n",
    "def highlight_rails_mask_only_fast(\n",
    "    img_bgr: np.ndarray,\n",
    "    rail_mask: np.ndarray,\n",
    "    target_colors_rgb: list[tuple[int,int,int]],\n",
    "    tolerance: float = 30.0,\n",
    "    min_region_size: int = 50,\n",
    "    min_region_height: int = 150\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Boolean mask of rail pixels that (1) color-match within tolerance,\n",
    "    (2) lie inside rail_mask, and (3) pass connected-component size/height filters.\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(rbg[2], rbg[1], rbg[0]) for rbg in target_colors_rgb],\n",
    "                           dtype=np.float32)\n",
    "\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]\n",
    "    dist2 = np.sum(diff * diff, axis=-1)\n",
    "    color_mask_roi = np.any(dist2 <= (tolerance * tolerance), axis=-1)\n",
    "\n",
    "    combined_roi = mask_roi & color_mask_roi\n",
    "\n",
    "    comp_u8 = combined_roi.astype(np.uint8)\n",
    "    n_labels, labels, stats, _ = cv2.connectedComponentsWithStats(comp_u8, 8)\n",
    "\n",
    "    filtered_roi = np.zeros_like(combined_roi)\n",
    "    for lbl in range(1, n_labels):\n",
    "        area   = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        height = stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and height >= min_region_height:\n",
    "            filtered_roi[labels == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = filtered_roi\n",
    "    return filtered_full\n",
    "\n",
    "# =======================\n",
    "# Heatmap (red-dominance vs green) + score map\n",
    "# =======================\n",
    "def red_vs_green_heatmap(red_mask: np.ndarray, green_mask: np.ndarray,\n",
    "                         blur_ksize: int = 51) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Heatmap (JET) and normalized score 0..255 where higher means more *red* than green.\n",
    "    \"\"\"\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    red_d   = cv2.blur(red_mask.astype(np.float32),   k)\n",
    "    green_d = cv2.blur(green_mask.astype(np.float32), k)\n",
    "    diff = red_d - green_d  # >0 => red-dominant, <0 => green-dominant\n",
    "\n",
    "    amax = float(np.max(np.abs(diff))) + 1e-6\n",
    "    norm = (diff / (2.0 * amax) + 0.5)  # 0..1 with 0.5 at tie\n",
    "    norm_u8 = np.clip(norm * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "    heat = cv2.applyColorMap(norm_u8, cv2.COLORMAP_JET)\n",
    "    return heat, norm_u8\n",
    "\n",
    "# =======================\n",
    "# Draw a small triangle marker with apex at (x, y)\n",
    "# =======================\n",
    "def draw_purple_triangle(img: np.ndarray, x: int, y: int, size: int = TRI_SIZE_PX):\n",
    "    h = int(size * 1.2)\n",
    "    pts = np.array([\n",
    "        [x, y],                    # apex\n",
    "        [x - size, y + h],         # base left\n",
    "        [x + size, y + h],         # base right\n",
    "    ], dtype=np.int32)\n",
    "    cv2.fillConvexPoly(img, pts, PURPLE)\n",
    "    cv2.polylines(img, [pts.reshape(-1,1,2)], True, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "# =======================\n",
    "# Per-frame pipeline\n",
    "# =======================\n",
    "def process_frame(img_bgr: np.ndarray) -> tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - orig_with_tri : ORIGINAL image with purple triangles at uppermost points of dark-red heat regions\n",
    "      - rail_mask     : boolean rail mask\n",
    "      - heatmap_bgr   : red-vs-green heatmap (blue=more green, red=more red)\n",
    "    \"\"\"\n",
    "    res = model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "        conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "        max_det=20, verbose=False, half=half\n",
    "    )[0]\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:\n",
    "        empty = np.zeros((H, W), bool)\n",
    "        heat, _ = red_vs_green_heatmap(empty, empty, blur_ksize=HEAT_BLUR_KSIZE)\n",
    "        return img_bgr.copy(), empty, heat\n",
    "\n",
    "    # union of rail segments\n",
    "    m = res.masks.data\n",
    "    union = (m.sum(dim=0) > 0).float().cpu().numpy()\n",
    "    if union.shape != (H, W):\n",
    "        union = cv2.resize(union, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "    rail_mask = union.astype(bool)\n",
    "\n",
    "    # compute filtered (green) mask\n",
    "    filtered_mask = highlight_rails_mask_only_fast(\n",
    "        img_bgr, rail_mask,\n",
    "        TARGET_COLORS_RGB, TOLERANCE,\n",
    "        MIN_REGION_SIZE, MIN_REGION_HEIGHT\n",
    "    )\n",
    "\n",
    "    # red area = rails minus the filtered zones\n",
    "    red_area   = rail_mask & (~filtered_mask)\n",
    "    green_area = filtered_mask\n",
    "\n",
    "    # Heatmap + score (0..255, 255 = most red-dominant)\n",
    "    heat, score = red_vs_green_heatmap(red_area, green_area, blur_ksize=HEAT_BLUR_KSIZE)\n",
    "\n",
    "    # ---- percentage-based exclusion applied BEFORE analysis for triangle plotting ----\n",
    "    top_rows = int(H * max(0.0, min(1.0, EXCLUDE_TOP_FRAC)))\n",
    "    bot_rows = int(H * max(0.0, min(1.0, EXCLUDE_BOTTOM_FRAC)))\n",
    "    score[:top_rows, :] = 0\n",
    "    if bot_rows > 0:\n",
    "        score[H - bot_rows:, :] = 0\n",
    "\n",
    "    # threshold dark-red regions\n",
    "    score_mask = (score >= RED_SCORE_THRESH).astype(np.uint8)\n",
    "\n",
    "    # denoise small speckle\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 9))\n",
    "    score_mask = cv2.morphologyEx(score_mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "\n",
    "    # find blobs and mark the uppermost point per blob\n",
    "    orig_marked = img_bgr.copy()\n",
    "    n_labels, labels, stats, _ = cv2.connectedComponentsWithStats(score_mask, 8)\n",
    "    for lbl in range(1, n_labels):\n",
    "        area = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        if area < MIN_DARK_RED_AREA:\n",
    "            continue\n",
    "        mask_lbl = (labels == lbl)\n",
    "        ys, xs = np.where(mask_lbl)\n",
    "        y_min = int(ys.min())\n",
    "        xs_top = xs[ys == y_min]\n",
    "        x_mid = int(np.clip(int(np.round(xs_top.mean())), 0, W - 1))\n",
    "        draw_purple_triangle(orig_marked, x_mid, y_min, TRI_SIZE_PX)\n",
    "\n",
    "    return orig_marked, rail_mask, heat\n",
    "\n",
    "# =======================\n",
    "# Run over a folder: show side-by-side with heatmap underneath\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    image_dir = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(image_dir, \"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found in\", image_dir)\n",
    "        sys.exit(1)\n",
    "\n",
    "    for p in paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        orig_with_tri, _rail, heat = process_frame(img)\n",
    "\n",
    "        # For reference, also render rails+filtered (right pane)\n",
    "        # (Optional: remove this block if you only want original + heatmap)\n",
    "        overlay = img.copy()\n",
    "        res_rails = model.predict(\n",
    "            img, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "            conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "            max_det=20, verbose=False, half=half\n",
    "        )[0]\n",
    "        if res_rails.masks is not None:\n",
    "            m = res_rails.masks.data\n",
    "            union = (m.sum(dim=0) > 0).float().cpu().numpy()\n",
    "            if union.shape != img.shape[:2]:\n",
    "                union = cv2.resize(union, (img.shape[1], img.shape[0]), interpolation=cv2.INTER_NEAREST)\n",
    "            rail_mask_viz = union.astype(bool)\n",
    "            overlay2 = img.copy()\n",
    "            overlay2[rail_mask_viz] = (0, 0, 255)\n",
    "            right = cv2.addWeighted(overlay2, ALPHA, img, 1 - ALPHA, 0)\n",
    "        else:\n",
    "            right = img.copy()\n",
    "\n",
    "        top = np.hstack((orig_with_tri, right))\n",
    "        heat_resized = cv2.resize(heat, (top.shape[1], img.shape[0]), interpolation=cv2.INTER_LINEAR)\n",
    "        display_img = np.vstack((top, heat_resized))\n",
    "\n",
    "        cv2.imshow(\"Top: Original + Purple Rail-End Triangles (Left) | Rails Viz (Right)  |  Bottom: Red-vs-Green Heatmap\", display_img)\n",
    "\n",
    "        # SPACE → next, 'q' → quit\n",
    "        while True:\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            if key == 32:\n",
    "                break\n",
    "            elif key == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b519f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Speed analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ee1d6a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n",
      "Frames processed  : 92\n",
      "Total time (s)    :   14.134\n",
      "Average/frame (s) :   0.1432\n",
      "Median/frame (s)  :   0.1409\n",
      "Min/frame (s)     :   0.0288\n",
      "Max/frame (s)     :   0.2234\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Benchmark rail-processing pipeline.\n",
    "Processes an entire folder of frames *without* any GUI,\n",
    "collects per-frame runtimes, and prints timing statistics.\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, sys, time\n",
    "import cv2, torch, numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40  # red tint on rail mask\n",
    "\n",
    "# Color/size filter (RGB targets; converted to BGR internally)\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0\n",
    "MIN_REGION_SIZE   = 30\n",
    "MIN_REGION_HEIGHT = 150\n",
    "\n",
    "# ---- Heatmap→triangle picking params ----\n",
    "HEAT_BLUR_KSIZE       = 51\n",
    "RED_SCORE_THRESH      = 220\n",
    "EXCLUDE_TOP_FRAC      = 0.40   # ignore top 40 % before triangle search\n",
    "EXCLUDE_BOTTOM_FRAC   = 0.15   # ignore bottom 15 %\n",
    "MIN_DARK_RED_AREA     = 1200\n",
    "TRI_SIZE_PX           = 18\n",
    "PURPLE                = (255, 0, 255)\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warm-up)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(\n",
    "    _dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "    verbose=False, half=half\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Helpers\n",
    "# =======================\n",
    "def highlight_rails_mask_only_fast(\n",
    "    img_bgr: np.ndarray,\n",
    "    rail_mask: np.ndarray,\n",
    "    target_colors_rgb: list[tuple[int,int,int]],\n",
    "    tolerance: float = 30.0,\n",
    "    min_region_size: int = 50,\n",
    "    min_region_height: int = 150\n",
    ") -> np.ndarray:\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(rbg[2], rbg[1], rbg[0]) for rbg in target_colors_rgb],\n",
    "                           dtype=np.float32)\n",
    "\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]\n",
    "    dist2 = np.sum(diff * diff, axis=-1)\n",
    "    color_mask_roi = np.any(dist2 <= (tolerance * tolerance), axis=-1)\n",
    "\n",
    "    combined_roi = mask_roi & color_mask_roi\n",
    "\n",
    "    comp_u8 = combined_roi.astype(np.uint8)\n",
    "    n_labels, labels, stats, _ = cv2.connectedComponentsWithStats(comp_u8, 8)\n",
    "\n",
    "    filtered_roi = np.zeros_like(combined_roi)\n",
    "    for lbl in range(1, n_labels):\n",
    "        area   = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        height = stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and height >= min_region_height:\n",
    "            filtered_roi[labels == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = filtered_roi\n",
    "    return filtered_full\n",
    "\n",
    "\n",
    "def red_vs_green_score(red_mask: np.ndarray,\n",
    "                       green_mask: np.ndarray,\n",
    "                       blur_ksize: int = 51) -> np.ndarray:\n",
    "    \"\"\"Return a 0-255 score map (higher = more red-dominant).\"\"\"\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    r = cv2.blur(red_mask.astype(np.float32), k)\n",
    "    g = cv2.blur(green_mask.astype(np.float32), k)\n",
    "    diff = r - g\n",
    "    amax = float(np.max(np.abs(diff))) + 1e-6\n",
    "    norm = (diff / (2.0 * amax) + 0.5)          # 0..1\n",
    "    return np.clip(norm * 255.0, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def process_frame(img_bgr: np.ndarray):\n",
    "    \"\"\"Process one frame; returns nothing (we only benchmark).\"\"\"\n",
    "    res = model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "        conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "        max_det=20, verbose=False, half=half\n",
    "    )[0]\n",
    "\n",
    "    if res.masks is None:\n",
    "        return\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    union = (res.masks.data.sum(dim=0) > 0).float().cpu().numpy()\n",
    "    if union.shape != (H, W):\n",
    "        union = cv2.resize(union, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "    rail_mask = union.astype(bool)\n",
    "\n",
    "    green = highlight_rails_mask_only_fast(\n",
    "        img_bgr, rail_mask, TARGET_COLORS_RGB,\n",
    "        TOLERANCE, MIN_REGION_SIZE, MIN_REGION_HEIGHT\n",
    "    )\n",
    "    red   = rail_mask & ~green\n",
    "\n",
    "    score = red_vs_green_score(red, green, HEAT_BLUR_KSIZE)\n",
    "\n",
    "    # Exclusion before any potential triangle logic (kept for timing parity)\n",
    "    top_rows = int(H * EXCLUDE_TOP_FRAC)\n",
    "    bot_rows = int(H * EXCLUDE_BOTTOM_FRAC)\n",
    "    score[:top_rows, :] = 0\n",
    "    if bot_rows:\n",
    "        score[H - bot_rows:, :] = 0\n",
    "\n",
    "    # Extract blobs, but we do NOT draw anything for the benchmark\n",
    "    mask = (score >= RED_SCORE_THRESH).astype(np.uint8)\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 9))\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    cv2.connectedComponentsWithStats(mask, 8)  # just to emulate same workload\n",
    "\n",
    "\n",
    "# =======================\n",
    "# Benchmark loop\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    image_dir = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(image_dir, \"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found in\", image_dir)\n",
    "        sys.exit(1)\n",
    "\n",
    "    per_frame = []\n",
    "    t0_all = time.perf_counter()\n",
    "\n",
    "    for p in paths[::5]:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "        t0 = time.perf_counter()\n",
    "        process_frame(img)\n",
    "        per_frame.append(time.perf_counter() - t0)\n",
    "\n",
    "    total = time.perf_counter() - t0_all\n",
    "    per_frame_np = np.array(per_frame)\n",
    "\n",
    "    # Print timing statistics\n",
    "    print(f\"Frames processed  : {len(per_frame)}\")\n",
    "    print(f\"Total time (s)    : {total:8.3f}\")\n",
    "    print(f\"Average/frame (s) : {per_frame_np.mean():8.4f}\")\n",
    "    print(f\"Median/frame (s)  : {np.median(per_frame_np):8.4f}\")\n",
    "    print(f\"Min/frame (s)     : {per_frame_np.min():8.4f}\")\n",
    "    print(f\"Max/frame (s)     : {per_frame_np.max():8.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbf1051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "cannot access local variable 'left' where it is not associated with a value",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnboundLocalError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 283\u001b[39m\n\u001b[32m    281\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[32m    282\u001b[39m     frame = cv2.imread(p);  t0 = time.time()\n\u001b[32m--> \u001b[39m\u001b[32m283\u001b[39m     left, right, heat = \u001b[43mprocess_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    284\u001b[39m     top = np.hstack((left, right))\n\u001b[32m    285\u001b[39m     canvas = np.vstack((top, heat))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 199\u001b[39m, in \u001b[36mprocess_frame\u001b[39m\u001b[34m(img_bgr)\u001b[39m\n\u001b[32m    183\u001b[39m obstacle_classes = {\n\u001b[32m    184\u001b[39m     \u001b[32m0\u001b[39m: (\u001b[33m\"\u001b[39m\u001b[33mBOOTS\u001b[39m\u001b[33m\"\u001b[39m,         (\u001b[32m255\u001b[39m, \u001b[32m153\u001b[39m,   \u001b[32m0\u001b[39m)),   \u001b[38;5;66;03m# orange\u001b[39;00m\n\u001b[32m    185\u001b[39m     \u001b[32m1\u001b[39m: (\u001b[33m\"\u001b[39m\u001b[33mGREYTRAIN\u001b[39m\u001b[33m\"\u001b[39m,     (\u001b[32m160\u001b[39m, \u001b[32m160\u001b[39m, \u001b[32m160\u001b[39m)),   \u001b[38;5;66;03m# grey\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    195\u001b[39m     \u001b[32m11\u001b[39m: (\u001b[33m\"\u001b[39m\u001b[33mYELLOWTRAIN\u001b[39m\u001b[33m\"\u001b[39m,  (  \u001b[32m0\u001b[39m, \u001b[32m255\u001b[39m, \u001b[32m255\u001b[39m))    \u001b[38;5;66;03m# bright cyan\u001b[39;00m\n\u001b[32m    196\u001b[39m }\n\u001b[32m    198\u001b[39m \u001b[38;5;66;03m# Overlay OTHER masks (everything that's NOT RAIL_ID)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m overlay = \u001b[43mleft\u001b[49m.copy()\n\u001b[32m    200\u001b[39m font_scale = \u001b[32m0.5\u001b[39m\n\u001b[32m    201\u001b[39m font_thickness = \u001b[32m1\u001b[39m\n",
      "\u001b[31mUnboundLocalError\u001b[39m: cannot access local variable 'left' where it is not associated with a value"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Visualise *all* segmentation masks.\n",
    "\n",
    "• Rails are still tinted red (right-hand panel) and used for the\n",
    "  heat-map + purple-triangle logic exactly as before.\n",
    "• Every other detected mask is over-laid on the *original* frame\n",
    "  (left-hand panel) with a distinct solid colour so you can see\n",
    "  what the network is segmenting besides the rails.\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, sys, time\n",
    "import cv2, torch, numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9                     # class-id for rails in the custom model\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40                  # red tint on rail mask (right pane)\n",
    "\n",
    "# Colour/size filter (RGB targets; converted to BGR internally)\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0\n",
    "MIN_REGION_SIZE   = 30\n",
    "MIN_REGION_HEIGHT = 150\n",
    "\n",
    "# ---- Heat-map → triangle params ----\n",
    "HEAT_BLUR_KSIZE     = 51\n",
    "RED_SCORE_THRESH    = 220\n",
    "EXCLUDE_TOP_FRAC    = 0.40   # ignore top   40 % before triangle search\n",
    "EXCLUDE_BOTTOM_FRAC = 0.15   # ignore bottom 15 %\n",
    "MIN_DARK_RED_AREA   = 1200\n",
    "TRI_SIZE_PX         = 18\n",
    "PURPLE              = (255, 0, 255)      # triangle colour (BGR)\n",
    "\n",
    "# Palette for **other** masks (cyclic if > len)\n",
    "OTHER_COLOURS = [\n",
    "    (  0,128,255),   # orange\n",
    "    (  0,255,255),   # yellow\n",
    "    ( 60,180, 75),   # green\n",
    "    (255,  0,127),   # pink\n",
    "    (255,255,  0),   # cyan\n",
    "    (128,  0,255),   # violet\n",
    "    (255,128,  0),   # sky blue\n",
    "]\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warm-up)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# one dummy forward so first real frame is fast\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "model.predict(_dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "              conf=CONF, iou=IOU, verbose=False, half=half)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Helper functions (unchanged rail filter + heat-map)\n",
    "# ------------------------------------------------------------------\n",
    "def highlight_rails_mask_only_fast(img_bgr, rail_mask, target_colors_rgb,\n",
    "                                   tolerance=30.0,\n",
    "                                   min_region_size=50,\n",
    "                                   min_region_height=150):\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb],\n",
    "                           dtype=np.float32)  # RGB->BGR swap\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    colour_hit = np.any(dist2 <= tolerance**2, axis=-1)\n",
    "\n",
    "    combined = colour_hit & mask_roi\n",
    "    comp = combined.astype(np.uint8)\n",
    "    n, lbls, stats, _ = cv2.connectedComponentsWithStats(comp, 8)\n",
    "\n",
    "    good = np.zeros_like(combined)\n",
    "    for lbl in range(1, n):\n",
    "        area, h = stats[lbl, cv2.CC_STAT_AREA], stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and h >= min_region_height:\n",
    "            good[lbls == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = good\n",
    "    return filtered_full\n",
    "\n",
    "\n",
    "def red_vs_green_score(red_mask, green_mask, blur_ksize=51):\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    r = cv2.blur(red_mask.astype(np.float32), k)\n",
    "    g = cv2.blur(green_mask.astype(np.float32), k)\n",
    "    diff = r - g\n",
    "    amax = float(np.max(np.abs(diff))) + 1e-6\n",
    "    norm = (diff / (2*amax) + 0.5)\n",
    "    return np.clip(norm * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def draw_triangle(img, x, y, size=TRI_SIZE_PX, colour=PURPLE):\n",
    "    h = int(size * 1.2)\n",
    "    pts = np.array([[x, y],\n",
    "                    [x-size, y+h],\n",
    "                    [x+size, y+h]], np.int32)\n",
    "    cv2.fillConvexPoly(img, pts, colour)\n",
    "    cv2.polylines(img, [pts.reshape(-1,1,2)], True, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Main per-frame pipeline\n",
    "# ------------------------------------------------------------------\n",
    "def process_frame(img_bgr):\n",
    "    res = model.predict(img_bgr, task=\"segment\", imgsz=IMG_SIZE,\n",
    "                        device=device, conf=CONF, iou=IOU,\n",
    "                        max_det=30, verbose=False, half=half)[0]\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:                         # no detections\n",
    "        return img_bgr.copy(), img_bgr.copy(), np.zeros_like(img_bgr)\n",
    "\n",
    "    masks = res.masks.data.cpu().numpy()          # (N, h, w)\n",
    "    classes = res.masks.cls.cpu().numpy() if hasattr(res.masks, \"cls\") \\\n",
    "              else res.boxes.cls.cpu().numpy()    # fallback\n",
    "    h_m, w_m = masks.shape[1:]\n",
    "\n",
    "    # ---------------- Rails mask / green filter / heat-map ------------\n",
    "    rail_union = np.zeros((h_m, w_m), bool)\n",
    "    for m, c in zip(masks, classes):\n",
    "        if int(c) == RAIL_ID:\n",
    "            rail_union |= m.astype(bool)\n",
    "\n",
    "    rail_mask = rail_union\n",
    "    if rail_mask.shape != (H, W):\n",
    "        rail_mask = cv2.resize(rail_mask.astype(np.uint8), (W, H),\n",
    "                               interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "    green = highlight_rails_mask_only_fast(img_bgr, rail_mask,\n",
    "                                           TARGET_COLORS_RGB,\n",
    "                                           TOLERANCE,\n",
    "                                           MIN_REGION_SIZE,\n",
    "                                           MIN_REGION_HEIGHT)\n",
    "    red = rail_mask & ~green\n",
    "    score = red_vs_green_score(red, green, HEAT_BLUR_KSIZE)\n",
    "\n",
    "    # exclusion\n",
    "    top_ex = int(H * EXCLUDE_TOP_FRAC)\n",
    "    bot_ex = int(H * EXCLUDE_BOTTOM_FRAC)\n",
    "    score[:top_ex, :] = 0\n",
    "    if bot_ex: score[H-bot_ex:, :] = 0\n",
    "\n",
    "    dark = (score >= RED_SCORE_THRESH).astype(np.uint8)\n",
    "    dark = cv2.morphologyEx(dark, cv2.MORPH_OPEN,\n",
    "                            cv2.getStructuringElement(cv2.MORPH_RECT, (5,9)),\n",
    "                            iterations=1)\n",
    "\n",
    "    # ---------------- left panel: original + other-mask colours + triangles ----\n",
    "    left = img_bgr.copy()\n",
    "\n",
    "    # Overlay OTHER masks (everything that's NOT RAIL_ID)\n",
    "    overlay = left.copy()\n",
    "    colour_idx = 0\n",
    "    for m, c in zip(masks, classes):\n",
    "        if int(c) == RAIL_ID:\n",
    "            continue\n",
    "        mask_full = m\n",
    "        if mask_full.shape != (H, W):\n",
    "            mask_full = cv2.resize(mask_full.astype(np.uint8), (W, H),\n",
    "                                   interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "        colour = OTHER_COLOURS[colour_idx % len(OTHER_COLOURS)]\n",
    "        overlay[mask_full] = colour\n",
    "        colour_idx += 1\n",
    "\n",
    "    # alpha-blend overlay on left frame\n",
    "    left = cv2.addWeighted(overlay, 0.6, left, 0.4, 0)\n",
    "\n",
    "    # Purple triangles (after overlay so they stay visible)\n",
    "    n_lbl, lbl_mat, stats, _ = cv2.connectedComponentsWithStats(dark, 8)\n",
    "    for lbl in range(1, n_lbl):\n",
    "        if stats[lbl, cv2.CC_STAT_AREA] < MIN_DARK_RED_AREA:\n",
    "            continue\n",
    "        ys, xs = np.where(lbl_mat == lbl)\n",
    "        y_top = ys.min()\n",
    "        x_mid = int(xs[ys == y_top].mean())\n",
    "        draw_triangle(left, x_mid, y_top)\n",
    "\n",
    "    # ---------------- right panel: rails tinted red -------------------\n",
    "    right = img_bgr.copy()\n",
    "    rails_tinted = right.copy()\n",
    "    if rail_mask.shape != right.shape[:2]:\n",
    "        rail_mask = rail_mask.astype(np.uint8)\n",
    "    rails_tinted[rail_mask] = (0,0,255)\n",
    "    right = cv2.addWeighted(rails_tinted, ALPHA, right, 1-ALPHA, 0)\n",
    "    right[green] = (0,255,0)\n",
    "\n",
    "    # ---------------- heat-map visual ----------------\n",
    "    heat_col = cv2.applyColorMap(score, cv2.COLORMAP_JET)\n",
    "    heat_col = cv2.resize(heat_col, (left.shape[1]+right.shape[1], H),\n",
    "                          interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return left, right, heat_col\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Drive over folder\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    folder = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(folder, \"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    for p in paths:\n",
    "        frame = cv2.imread(p);  t0 = time.time()\n",
    "        left, right, heat = process_frame(frame)\n",
    "        top = np.hstack((left, right))\n",
    "        canvas = np.vstack((top, heat))\n",
    "        cv2.imshow(\"Left: Original + coloured other-masks + triangles | \"\n",
    "                   \"Right: Rails tinted   |  Bottom: heat-map\", canvas)\n",
    "        print(f\"{os.path.basename(p):>20s}   proc {1000*(time.time()-t0):5.1f} ms\")\n",
    "        key = cv2.waitKey(0) & 0xFF\n",
    "        if key in (ord('q'), 27):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aeabcb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n",
      "      frame_0000.jpg   proc 924.8 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 18:05:02.409 Python[54812:29192831] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-05 18:05:02.409 Python[54812:29192831] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      frame_0001.jpg   proc 812.6 ms\n",
      "      frame_0002.jpg   proc 369.0 ms\n",
      "      frame_0003.jpg   proc 387.9 ms\n",
      "      frame_0004.jpg   proc 364.1 ms\n",
      "      frame_0005.jpg   proc 348.4 ms\n",
      "      frame_0006.jpg   proc 335.4 ms\n",
      "      frame_0007.jpg   proc 383.6 ms\n",
      "      frame_0008.jpg   proc 352.4 ms\n",
      "      frame_0009.jpg   proc 307.5 ms\n",
      "      frame_0010.jpg   proc 325.8 ms\n",
      "      frame_0011.jpg   proc 319.4 ms\n",
      "      frame_0012.jpg   proc 320.7 ms\n",
      "      frame_0013.jpg   proc 228.2 ms\n",
      "      frame_0014.jpg   proc 318.3 ms\n",
      "      frame_0015.jpg   proc 346.7 ms\n",
      "      frame_0016.jpg   proc 228.7 ms\n",
      "      frame_0017.jpg   proc 201.9 ms\n",
      "      frame_0018.jpg   proc 325.9 ms\n",
      "      frame_0019.jpg   proc 250.0 ms\n",
      "      frame_0020.jpg   proc 244.8 ms\n",
      "      frame_0021.jpg   proc 321.3 ms\n",
      "      frame_0022.jpg   proc 260.6 ms\n",
      "      frame_0023.jpg   proc 269.6 ms\n",
      "      frame_0024.jpg   proc 263.9 ms\n",
      "      frame_0025.jpg   proc 287.6 ms\n",
      "      frame_0026.jpg   proc 316.9 ms\n",
      "      frame_0027.jpg   proc 292.5 ms\n",
      "      frame_0028.jpg   proc 278.9 ms\n",
      "      frame_0029.jpg   proc 246.0 ms\n",
      "      frame_0030.jpg   proc 254.2 ms\n",
      "      frame_0031.jpg   proc 315.5 ms\n",
      "      frame_0032.jpg   proc 313.6 ms\n",
      "      frame_0033.jpg   proc 265.1 ms\n",
      "      frame_0034.jpg   proc 322.3 ms\n",
      "      frame_0035.jpg   proc 350.8 ms\n",
      "      frame_0036.jpg   proc 321.7 ms\n",
      "      frame_0037.jpg   proc 292.0 ms\n",
      "      frame_0038.jpg   proc 314.9 ms\n",
      "      frame_0039.jpg   proc 224.2 ms\n",
      "      frame_0040.jpg   proc 401.6 ms\n",
      "      frame_0041.jpg   proc 325.9 ms\n",
      "      frame_0042.jpg   proc 317.6 ms\n",
      "      frame_0043.jpg   proc 263.0 ms\n",
      "      frame_0044.jpg   proc 391.6 ms\n",
      "      frame_0045.jpg   proc 375.6 ms\n",
      "      frame_0046.jpg   proc 232.2 ms\n",
      "      frame_0047.jpg   proc 344.5 ms\n",
      "      frame_0048.jpg   proc 339.0 ms\n",
      "      frame_0049.jpg   proc 318.7 ms\n",
      "      frame_0050.jpg   proc 280.9 ms\n",
      "      frame_0051.jpg   proc 186.3 ms\n",
      "      frame_0052.jpg   proc 274.7 ms\n",
      "      frame_0053.jpg   proc 329.0 ms\n",
      "      frame_0054.jpg   proc 237.8 ms\n",
      "      frame_0055.jpg   proc 281.8 ms\n",
      "      frame_0056.jpg   proc 348.9 ms\n",
      "      frame_0057.jpg   proc 249.9 ms\n",
      "      frame_0058.jpg   proc 231.1 ms\n",
      "      frame_0059.jpg   proc 339.1 ms\n",
      "      frame_0060.jpg   proc 243.9 ms\n",
      "      frame_0061.jpg   proc 253.6 ms\n",
      "      frame_0062.jpg   proc 554.0 ms\n",
      "      frame_0063.jpg   proc 298.0 ms\n",
      "      frame_0064.jpg   proc 297.4 ms\n",
      "      frame_0065.jpg   proc 214.0 ms\n",
      "      frame_0066.jpg   proc 226.0 ms\n",
      "      frame_0067.jpg   proc 351.2 ms\n",
      "      frame_0068.jpg   proc 368.5 ms\n",
      "      frame_0069.jpg   proc 334.4 ms\n",
      "      frame_0070.jpg   proc 396.3 ms\n",
      "      frame_0071.jpg   proc 304.6 ms\n",
      "      frame_0072.jpg   proc 262.5 ms\n",
      "      frame_0073.jpg   proc 321.9 ms\n",
      "      frame_0074.jpg   proc 329.6 ms\n",
      "      frame_0075.jpg   proc 242.0 ms\n",
      "      frame_0076.jpg   proc 269.8 ms\n",
      "      frame_0077.jpg   proc 371.5 ms\n",
      "      frame_0078.jpg   proc 727.1 ms\n",
      "      frame_0079.jpg   proc 264.9 ms\n",
      "      frame_0080.jpg   proc 258.5 ms\n",
      "      frame_0081.jpg   proc 261.9 ms\n",
      "      frame_0082.jpg   proc 343.7 ms\n",
      "      frame_0083.jpg   proc 230.1 ms\n",
      "      frame_0084.jpg   proc 188.3 ms\n",
      "      frame_0085.jpg   proc 238.6 ms\n",
      "      frame_0086.jpg   proc 339.8 ms\n",
      "      frame_0087.jpg   proc 212.2 ms\n",
      "      frame_0088.jpg   proc 283.0 ms\n",
      "      frame_0089.jpg   proc 350.4 ms\n",
      "      frame_0090.jpg   proc 260.6 ms\n",
      "      frame_0091.jpg   proc 300.5 ms\n",
      "      frame_0092.jpg   proc 293.6 ms\n",
      "      frame_0093.jpg   proc 228.6 ms\n",
      "      frame_0094.jpg   proc 299.3 ms\n",
      "      frame_0095.jpg   proc 276.6 ms\n",
      "      frame_0096.jpg   proc 345.1 ms\n",
      "      frame_0097.jpg   proc 231.3 ms\n",
      "      frame_0098.jpg   proc 319.9 ms\n",
      "      frame_0099.jpg   proc 262.8 ms\n",
      "      frame_0100.jpg   proc 291.4 ms\n",
      "      frame_0101.jpg   proc 367.3 ms\n",
      "      frame_0102.jpg   proc 370.4 ms\n",
      "      frame_0103.jpg   proc 331.8 ms\n",
      "      frame_0104.jpg   proc 371.9 ms\n",
      "      frame_0105.jpg   proc 215.5 ms\n",
      "      frame_0106.jpg   proc 171.0 ms\n",
      "      frame_0107.jpg   proc 309.3 ms\n",
      "      frame_0108.jpg   proc 363.1 ms\n",
      "      frame_0109.jpg   proc 377.6 ms\n",
      "      frame_0110.jpg   proc 362.2 ms\n",
      "      frame_0111.jpg   proc 232.6 ms\n",
      "      frame_0112.jpg   proc 363.1 ms\n",
      "      frame_0113.jpg   proc 289.7 ms\n",
      "      frame_0114.jpg   proc 292.3 ms\n",
      "      frame_0115.jpg   proc 281.2 ms\n",
      "      frame_0116.jpg   proc 239.6 ms\n",
      "      frame_0117.jpg   proc 286.9 ms\n",
      "      frame_0118.jpg   proc 218.8 ms\n",
      "      frame_0119.jpg   proc 262.3 ms\n",
      "      frame_0120.jpg   proc 200.3 ms\n",
      "      frame_0121.jpg   proc 194.1 ms\n",
      "      frame_0122.jpg   proc 275.6 ms\n",
      "      frame_0123.jpg   proc 266.5 ms\n",
      "      frame_0124.jpg   proc 190.0 ms\n",
      "      frame_0125.jpg   proc 200.4 ms\n",
      "      frame_0126.jpg   proc 302.1 ms\n",
      "      frame_0127.jpg   proc 274.0 ms\n",
      "      frame_0128.jpg   proc 253.4 ms\n",
      "      frame_0129.jpg   proc 333.2 ms\n",
      "      frame_0130.jpg   proc 277.8 ms\n",
      "      frame_0131.jpg   proc 319.7 ms\n",
      "      frame_0132.jpg   proc 294.1 ms\n",
      "      frame_0133.jpg   proc 359.2 ms\n",
      "      frame_0134.jpg   proc 303.9 ms\n",
      "      frame_0135.jpg   proc 229.8 ms\n",
      "      frame_0136.jpg   proc 227.7 ms\n",
      "      frame_0137.jpg   proc 175.8 ms\n",
      "      frame_0138.jpg   proc 179.9 ms\n",
      "      frame_0139.jpg   proc 305.3 ms\n",
      "      frame_0140.jpg   proc 239.1 ms\n",
      "      frame_0141.jpg   proc 534.4 ms\n",
      "      frame_0142.jpg   proc 295.3 ms\n",
      "      frame_0143.jpg   proc 278.0 ms\n",
      "      frame_0144.jpg   proc 285.2 ms\n",
      "      frame_0145.jpg   proc 225.2 ms\n",
      "      frame_0146.jpg   proc 254.2 ms\n",
      "      frame_0147.jpg   proc 362.0 ms\n",
      "      frame_0148.jpg   proc 275.7 ms\n",
      "      frame_0149.jpg   proc 262.4 ms\n",
      "      frame_0150.jpg   proc 278.4 ms\n",
      "      frame_0151.jpg   proc 372.8 ms\n",
      "      frame_0152.jpg   proc 632.2 ms\n",
      "      frame_0153.jpg   proc 313.9 ms\n",
      "      frame_0154.jpg   proc 342.7 ms\n",
      "      frame_0155.jpg   proc 331.0 ms\n",
      "      frame_0156.jpg   proc 367.2 ms\n",
      "      frame_0157.jpg   proc 297.8 ms\n",
      "      frame_0158.jpg   proc 306.7 ms\n",
      "      frame_0159.jpg   proc 295.5 ms\n",
      "      frame_0160.jpg   proc 207.1 ms\n",
      "      frame_0161.jpg   proc 277.7 ms\n",
      "      frame_0162.jpg   proc 259.1 ms\n",
      "      frame_0163.jpg   proc 195.0 ms\n",
      "      frame_0164.jpg   proc 271.7 ms\n",
      "      frame_0165.jpg   proc 300.6 ms\n",
      "      frame_0166.jpg   proc 248.0 ms\n",
      "      frame_0167.jpg   proc 263.1 ms\n",
      "      frame_0168.jpg   proc 266.6 ms\n",
      "      frame_0169.jpg   proc 283.5 ms\n",
      "      frame_0170.jpg   proc 306.2 ms\n",
      "      frame_0171.jpg   proc 300.8 ms\n",
      "      frame_0172.jpg   proc 393.5 ms\n",
      "      frame_0173.jpg   proc 378.8 ms\n",
      "      frame_0174.jpg   proc 316.8 ms\n",
      "      frame_0175.jpg   proc 444.9 ms\n",
      "      frame_0176.jpg   proc 325.6 ms\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3840 and the array at index 1 has size 1920",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 219\u001b[39m\n\u001b[32m    217\u001b[39m left, right, heat = process_frame(frame)\n\u001b[32m    218\u001b[39m top = np.hstack((left, right))\n\u001b[32m--> \u001b[39m\u001b[32m219\u001b[39m canvas = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheat\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    220\u001b[39m cv2.imshow(\u001b[33m\"\u001b[39m\u001b[33mLeft: Labelled masks | Right: Rails tinted | Bottom: heat-map\u001b[39m\u001b[33m\"\u001b[39m, canvas)\n\u001b[32m    221\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos.path.basename(p)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m>20s\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m   proc \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[32m1000\u001b[39m*(time.time()-t0)\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m5.1f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ms\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/GitHub/Ai-plays-SubwaySurfers/.venv/lib/python3.13/site-packages/numpy/_core/shape_base.py:292\u001b[39m, in \u001b[36mvstack\u001b[39m\u001b[34m(tup, dtype, casting)\u001b[39m\n\u001b[32m    290\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arrs, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    291\u001b[39m     arrs = (arrs,)\n\u001b[32m--> \u001b[39m\u001b[32m292\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_nx\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43marrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcasting\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValueError\u001b[39m: all the input array dimensions except for the concatenation axis must match exactly, but along dimension 1, the array at index 0 has size 3840 and the array at index 1 has size 1920"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Visualise *all* segmentation masks with labelled overlays.\n",
    "\n",
    "• Rails are still tinted red (right-hand panel) and used for the\n",
    "  heat-map + purple-triangle logic exactly as before.\n",
    "• Every other detected mask is over-laid on the *original* frame\n",
    "  with its true label and a consistent colour.\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, sys, time\n",
    "import cv2, torch, numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40\n",
    "\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0\n",
    "MIN_REGION_SIZE   = 30\n",
    "MIN_REGION_HEIGHT = 150\n",
    "\n",
    "HEAT_BLUR_KSIZE     = 51\n",
    "RED_SCORE_THRESH    = 220\n",
    "EXCLUDE_TOP_FRAC    = 0.40\n",
    "EXCLUDE_BOTTOM_FRAC = 0.15\n",
    "MIN_DARK_RED_AREA   = 1200\n",
    "TRI_SIZE_PX         = 18\n",
    "PURPLE              = (255, 0, 255)\n",
    "\n",
    "obstacle_classes = {\n",
    "    0: \"BOOTS\", 1: \"GREYTRAIN\", 2: \"HIGHBARRIER1\", 3: \"JUMP\", 4: \"LOWBARRIER1\",\n",
    "    5: \"LOWBARRIER2\", 6: \"ORANGETRAIN\", 7: \"PILLAR\", 8: \"RAMP\", 9: \"RAILS\",\n",
    "    10: \"SIDEWALK\", 11: \"YELLOWTRAIN\"\n",
    "}\n",
    "\n",
    "CLASS_COLOURS = {\n",
    "    0: (255, 255, 0), 1: (192, 192, 192), 2: (0, 128, 255), 3: (0, 255, 0),\n",
    "    4: (255, 0, 255), 5: (0, 255, 255), 6: (255, 128, 0), 7: (128, 0, 255),\n",
    "    8: (0, 0, 128), 10: (128, 128, 0), 11: (255, 255, 102)\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "model = YOLO(weights)\n",
    "try: model.fuse()\n",
    "except Exception: pass\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "model.predict(_dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "              conf=CONF, iou=IOU, verbose=False, half=half)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def highlight_rails_mask_only_fast(img_bgr, rail_mask, target_colors_rgb,\n",
    "                                   tolerance=30.0,\n",
    "                                   min_region_size=50,\n",
    "                                   min_region_height=150):\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any(): return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb], dtype=np.float32)\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    colour_hit = np.any(dist2 <= tolerance**2, axis=-1)\n",
    "\n",
    "    combined = colour_hit & mask_roi\n",
    "    comp = combined.astype(np.uint8)\n",
    "    n, lbls, stats, _ = cv2.connectedComponentsWithStats(comp, 8)\n",
    "\n",
    "    good = np.zeros_like(combined)\n",
    "    for lbl in range(1, n):\n",
    "        area, h = stats[lbl, cv2.CC_STAT_AREA], stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and h >= min_region_height:\n",
    "            good[lbls == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = good\n",
    "    return filtered_full\n",
    "\n",
    "\n",
    "def red_vs_green_score(red_mask, green_mask, blur_ksize=51):\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    r = cv2.blur(red_mask.astype(np.float32), k)\n",
    "    g = cv2.blur(green_mask.astype(np.float32), k)\n",
    "    diff = r - g\n",
    "    amax = float(np.max(np.abs(diff))) + 1e-6\n",
    "    norm = (diff / (2*amax) + 0.5)\n",
    "    return np.clip(norm * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def draw_triangle(img, x, y, size=TRI_SIZE_PX, colour=PURPLE):\n",
    "    h = int(size * 1.2)\n",
    "    pts = np.array([[x, y], [x-size, y+h], [x+size, y+h]], np.int32)\n",
    "    cv2.fillConvexPoly(img, pts, colour)\n",
    "    cv2.polylines(img, [pts.reshape(-1,1,2)], True, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def process_frame(img_bgr):\n",
    "    res = model.predict(img_bgr, task=\"segment\", imgsz=IMG_SIZE,\n",
    "                        device=device, conf=CONF, iou=IOU,\n",
    "                        max_det=30, verbose=False, half=half)[0]\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:\n",
    "        return img_bgr.copy(), img_bgr.copy(), np.zeros_like(img_bgr)\n",
    "\n",
    "    masks = res.masks.data.cpu().numpy()\n",
    "    classes = res.masks.cls.cpu().numpy() if hasattr(res.masks, \"cls\") \\\n",
    "              else res.boxes.cls.cpu().numpy()\n",
    "    h_m, w_m = masks.shape[1:]\n",
    "\n",
    "    rail_union = np.zeros((h_m, w_m), bool)\n",
    "    for m, c in zip(masks, classes):\n",
    "        if int(c) == RAIL_ID:\n",
    "            rail_union |= m.astype(bool)\n",
    "\n",
    "    rail_mask = rail_union\n",
    "    if rail_mask.shape != (H, W):\n",
    "        rail_mask = cv2.resize(rail_mask.astype(np.uint8), (W, H), interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "    green = highlight_rails_mask_only_fast(img_bgr, rail_mask,\n",
    "                                           TARGET_COLORS_RGB,\n",
    "                                           TOLERANCE,\n",
    "                                           MIN_REGION_SIZE,\n",
    "                                           MIN_REGION_HEIGHT)\n",
    "    red = rail_mask & ~green\n",
    "    score = red_vs_green_score(red, green, HEAT_BLUR_KSIZE)\n",
    "\n",
    "    top_ex = int(H * EXCLUDE_TOP_FRAC)\n",
    "    bot_ex = int(H * EXCLUDE_BOTTOM_FRAC)\n",
    "    score[:top_ex, :] = 0\n",
    "    if bot_ex: score[H-bot_ex:, :] = 0\n",
    "\n",
    "    dark = (score >= RED_SCORE_THRESH).astype(np.uint8)\n",
    "    dark = cv2.morphologyEx(dark, cv2.MORPH_OPEN,\n",
    "                            cv2.getStructuringElement(cv2.MORPH_RECT, (5,9)),\n",
    "                            iterations=1)\n",
    "\n",
    "    left = img_bgr.copy()\n",
    "    overlay = left.copy()\n",
    "    for m, c in zip(masks, classes):\n",
    "        cid = int(c)\n",
    "        if cid == RAIL_ID:\n",
    "            continue\n",
    "        mask_full = m\n",
    "        if mask_full.shape != (H, W):\n",
    "            mask_full = cv2.resize(mask_full.astype(np.uint8), (W, H), interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "        label = obstacle_classes.get(cid, f\"CLASS {cid}\")\n",
    "        colour = CLASS_COLOURS.get(cid, (255, 255, 255))\n",
    "        overlay[mask_full] = colour\n",
    "\n",
    "        ys, xs = np.where(mask_full)\n",
    "        if len(xs):\n",
    "            x_c, y_c = int(xs.mean()), int(ys.mean())\n",
    "            cv2.putText(overlay, label, (x_c - 40, y_c),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(overlay, label, (x_c - 40, y_c),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)\n",
    "\n",
    "    left = cv2.addWeighted(overlay, 0.6, left, 0.4, 0)\n",
    "\n",
    "    n_lbl, lbl_mat, stats, _ = cv2.connectedComponentsWithStats(dark, 8)\n",
    "    for lbl in range(1, n_lbl):\n",
    "        if stats[lbl, cv2.CC_STAT_AREA] < MIN_DARK_RED_AREA:\n",
    "            continue\n",
    "        ys, xs = np.where(lbl_mat == lbl)\n",
    "        y_top = ys.min()\n",
    "        x_mid = int(xs[ys == y_top].mean())\n",
    "        draw_triangle(left, x_mid, y_top)\n",
    "\n",
    "    right = img_bgr.copy()\n",
    "    rails_tinted = right.copy()\n",
    "    if rail_mask.shape != right.shape[:2]:\n",
    "        rail_mask = rail_mask.astype(np.uint8)\n",
    "    rails_tinted[rail_mask] = (0,0,255)\n",
    "    right = cv2.addWeighted(rails_tinted, ALPHA, right, 1-ALPHA, 0)\n",
    "    right[green] = (0,255,0)\n",
    "\n",
    "    heat_col = cv2.applyColorMap(score, cv2.COLORMAP_JET)\n",
    "    heat_col = cv2.resize(heat_col, (left.shape[1]+right.shape[1], H), interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return left, right, heat_col\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    folder = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(folder, \"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    for p in paths:\n",
    "        frame = cv2.imread(p);  t0 = time.time()\n",
    "        left, right, heat = process_frame(frame)\n",
    "        top = np.hstack((left, right))\n",
    "        canvas = np.vstack((top, heat))\n",
    "        cv2.imshow(\"Left: Labelled masks | Right: Rails tinted | Bottom: heat-map\", canvas)\n",
    "        print(f\"{os.path.basename(p):>20s}   proc {1000*(time.time()-t0):5.1f} ms\")\n",
    "        key = cv2.waitKey(0) & 0xFF\n",
    "        if key in (ord('q'), 27):\n",
    "            break\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f2c4644",
   "metadata": {},
   "outputs": [],
   "source": [
    "#BLUESTACKS dimensions uses mobile resolution vertical frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4215ed10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 21:08:14.712 Python[64470:29392739] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-05 21:08:14.712 Python[64470:29392739] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Visualise *all* segmentation masks with labelled overlays.\n",
    "\n",
    "• Rails are tinted red (right) and used for the heat-map + purple-triangle.\n",
    "• Other masks are labelled and coloured on the original (left).\n",
    "• Bottom panel shows the heat-map.\n",
    "\n",
    "Controls:\n",
    "  SPACE: next frame   |   q / ESC: quit\n",
    "\n",
    "Notes:\n",
    "- Uses a responsive event loop (waitKey(1)) so keys register immediately.\n",
    "- Canvas is auto-rescaled to avoid huge window draw latency.\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, sys, time\n",
    "import cv2, torch, numpy as np\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40\n",
    "\n",
    "TARGET_COLORS_RGB  = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE          = 20.0\n",
    "MIN_REGION_SIZE    = 30\n",
    "MIN_REGION_HEIGHT  = 150\n",
    "\n",
    "HEAT_BLUR_KSIZE     = 51\n",
    "RED_SCORE_THRESH    = 220\n",
    "EXCLUDE_TOP_FRAC    = 0.40\n",
    "EXCLUDE_BOTTOM_FRAC = 0.15\n",
    "MIN_DARK_RED_AREA   = 1200\n",
    "TRI_SIZE_PX         = 18\n",
    "PURPLE              = (255, 0, 255)\n",
    "\n",
    "# UI / display constraints\n",
    "WIN_NAME        = \"Left: labels | Right: rails | Bottom: heat-map (SPACE=next, q=quit)\"\n",
    "MAX_DISPLAY_W   = 1600   # shrink canvas if wider than this to avoid huge blits\n",
    "TEXT_CLR        = (255, 255, 255)\n",
    "\n",
    "obstacle_classes = {\n",
    "    0: \"BOOTS\", 1: \"GREYTRAIN\", 2: \"HIGHBARRIER1\", 3: \"JUMP\", 4: \"LOWBARRIER1\",\n",
    "    5: \"LOWBARRIER2\", 6: \"ORANGETRAIN\", 7: \"PILLAR\", 8: \"RAMP\", 9: \"RAILS\",\n",
    "    10: \"SIDEWALK\", 11: \"YELLOWTRAIN\"\n",
    "}\n",
    "CLASS_COLOURS = {\n",
    "    0: (255,255,0), 1: (192,192,192), 2: (0,128,255), 3: (0,255,0),\n",
    "    4: (255,0,255), 5: (0,255,255), 6: (255,128,0), 7: (128,0,255),\n",
    "    8: (0,0,128), 10: (128,128,0), 11: (255,255,102)\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# Optional: reduce OpenCV CPU thread contention with PyTorch\n",
    "try:\n",
    "    cv2.setNumThreads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# Warm-up so the first real frame isn't laggy\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(_dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "                  conf=CONF, iou=IOU, verbose=False, half=half)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def highlight_rails_mask_only_fast(img_bgr, rail_mask, target_colors_rgb,\n",
    "                                   tolerance=30.0,\n",
    "                                   min_region_size=50,\n",
    "                                   min_region_height=150):\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max()+1\n",
    "    x0, x1 = xs.min(), xs.max()+1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb],\n",
    "                           dtype=np.float32)\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    colour_hit = np.any(dist2 <= tolerance**2, axis=-1)\n",
    "\n",
    "    combined = colour_hit & mask_roi\n",
    "    comp = combined.astype(np.uint8)\n",
    "    n, lbls, stats, _ = cv2.connectedComponentsWithStats(comp, 8)\n",
    "\n",
    "    good = np.zeros_like(combined)\n",
    "    for lbl in range(1, n):\n",
    "        area, h = stats[lbl, cv2.CC_STAT_AREA], stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and h >= min_region_height:\n",
    "            good[lbls == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = good\n",
    "    return filtered_full\n",
    "\n",
    "\n",
    "def red_vs_green_score(red_mask, green_mask, blur_ksize=51):\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    r = cv2.blur(red_mask.astype(np.float32), k)\n",
    "    g = cv2.blur(green_mask.astype(np.float32), k)\n",
    "    diff = r - g\n",
    "    amax = float(np.max(np.abs(diff))) + 1e-6\n",
    "    norm = (diff / (2*amax) + 0.5)\n",
    "    return np.clip(norm * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def draw_triangle(img, x, y, size=TRI_SIZE_PX, colour=PURPLE):\n",
    "    h = int(size * 1.2)\n",
    "    pts = np.array([[x, y], [x-size, y+h], [x+size, y+h]], np.int32)\n",
    "    cv2.fillConvexPoly(img, pts, colour)\n",
    "    cv2.polylines(img, [pts.reshape(-1,1,2)], True, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def process_frame(img_bgr):\n",
    "    \"\"\"Returns (left, right, heat_col) – all uint8 BGR images.\"\"\"\n",
    "    res = model.predict(img_bgr, task=\"segment\", imgsz=IMG_SIZE,\n",
    "                        device=device, conf=CONF, iou=IOU,\n",
    "                        max_det=30, verbose=False, half=half)[0]\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:\n",
    "        blank_heat = np.zeros((H, W), np.uint8)\n",
    "        return img_bgr.copy(), img_bgr.copy(), cv2.applyColorMap(blank_heat, cv2.COLORMAP_JET)\n",
    "\n",
    "    masks   = res.masks.data.cpu().numpy()\n",
    "    classes = res.masks.cls.cpu().numpy() if hasattr(res.masks, \"cls\") \\\n",
    "              else res.boxes.cls.cpu().numpy()\n",
    "    h_m, w_m = masks.shape[1:]\n",
    "\n",
    "    rail_union = np.zeros((h_m, w_m), bool)\n",
    "    for m, c in zip(masks, classes):\n",
    "        if int(c) == RAIL_ID:\n",
    "            rail_union |= m.astype(bool)\n",
    "\n",
    "    rail_mask = rail_union\n",
    "    if rail_mask.shape != (H, W):\n",
    "        rail_mask = cv2.resize(rail_mask.astype(np.uint8), (W, H),\n",
    "                               interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "    green = highlight_rails_mask_only_fast(img_bgr, rail_mask,\n",
    "                                           TARGET_COLORS_RGB, TOLERANCE,\n",
    "                                           MIN_REGION_SIZE, MIN_REGION_HEIGHT)\n",
    "    red   = rail_mask & ~green\n",
    "    score = red_vs_green_score(red, green, HEAT_BLUR_KSIZE)\n",
    "\n",
    "    top_ex = int(H * EXCLUDE_TOP_FRAC)\n",
    "    bot_ex = int(H * EXCLUDE_BOTTOM_FRAC)\n",
    "    score[:top_ex, :] = 0\n",
    "    if bot_ex: score[H-bot_ex:, :] = 0\n",
    "\n",
    "    dark = (score >= RED_SCORE_THRESH).astype(np.uint8)\n",
    "    dark = cv2.morphologyEx(\n",
    "        dark, cv2.MORPH_OPEN,\n",
    "        cv2.getStructuringElement(cv2.MORPH_RECT, (5, 9)),\n",
    "        iterations=1\n",
    "    )\n",
    "\n",
    "    # left: labels overlaid\n",
    "    left    = img_bgr.copy()\n",
    "    overlay = left.copy()\n",
    "    for m, c in zip(masks, classes):\n",
    "        cid = int(c)\n",
    "        if cid == RAIL_ID:\n",
    "            continue\n",
    "        mask_full = m\n",
    "        if mask_full.shape != (H, W):\n",
    "            mask_full = cv2.resize(mask_full.astype(np.uint8), (W, H),\n",
    "                                   interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "        label  = obstacle_classes.get(cid, f\"CLASS {cid}\")\n",
    "        colour = CLASS_COLOURS.get(cid, (255, 255, 255))\n",
    "        overlay[mask_full] = colour\n",
    "\n",
    "        ys, xs = np.where(mask_full)\n",
    "        if len(xs):\n",
    "            x_c, y_c = int(xs.mean()), int(ys.mean())\n",
    "            cv2.putText(overlay, label, (x_c - 40, y_c),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(overlay, label, (x_c - 40, y_c),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)\n",
    "    left = cv2.addWeighted(overlay, 0.6, left, 0.4, 0)\n",
    "\n",
    "    # purple triangle warnings\n",
    "    n_lbl, lbl_mat, stats, _ = cv2.connectedComponentsWithStats(dark, 8)\n",
    "    for lbl in range(1, n_lbl):\n",
    "        if stats[lbl, cv2.CC_STAT_AREA] < MIN_DARK_RED_AREA: continue\n",
    "        ys, xs = np.where(lbl_mat == lbl)\n",
    "        y_top, x_mid = ys.min(), int(xs[ys == ys.min()].mean())\n",
    "        draw_triangle(left, x_mid, y_top)\n",
    "\n",
    "    # right: rails tinted\n",
    "    right = img_bgr.copy()\n",
    "    rails_tinted = right.copy()\n",
    "    rails_tinted[rail_mask] = (0,0,255)\n",
    "    right = cv2.addWeighted(rails_tinted, ALPHA, right, 1-ALPHA, 0)\n",
    "    right[green] = (0,255,0)\n",
    "\n",
    "    # bottom: heat map (width = left+right, height = H)\n",
    "    heat_col = cv2.applyColorMap(score, cv2.COLORMAP_JET)\n",
    "    heat_col = cv2.resize(heat_col, (left.shape[1] + right.shape[1], H),\n",
    "                          interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return left, right, heat_col\n",
    "\n",
    "def assemble_canvas(left, right, heat):\n",
    "    top    = np.hstack((left, right))\n",
    "    canvas = np.vstack((top, heat))\n",
    "    return canvas\n",
    "\n",
    "def maybe_downscale(img, max_w=MAX_DISPLAY_W):\n",
    "    h, w = img.shape[:2]\n",
    "    if w <= max_w:\n",
    "        return img\n",
    "    scale = max_w / float(w)\n",
    "    new_size = (int(w*scale), int(h*scale))\n",
    "    return cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    folder = Path.home() / \"Documents\" / \"GitHub\" / \"Ai-plays-SubwaySurfers\" / \"frames\"\n",
    "    if not folder.is_dir():\n",
    "        print(f\"frames folder not found: {folder}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    paths = sorted(\n",
    "        glob.glob(str(folder / \"frame_*.jpg\")) +\n",
    "        glob.glob(str(folder / \"frame_*.png\")) +\n",
    "        glob.glob(str(folder / \"*.jpg\")) +\n",
    "        glob.glob(str(folder / \"*.png\"))\n",
    "    )\n",
    "    if not paths:\n",
    "        print(f\"No frame images found in {folder}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    cv2.namedWindow(WIN_NAME, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    i = 0\n",
    "    n = len(paths)\n",
    "    while i < n:\n",
    "        p = paths[i]\n",
    "        frame = cv2.imread(p)\n",
    "        if frame is None:\n",
    "            print(f\"[WARN] unreadable image: {p}\")\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            left, right, heat = process_frame(frame)\n",
    "            proc_ms = (time.perf_counter() - t0) * 1000.0\n",
    "            canvas = assemble_canvas(left, right, heat)\n",
    "            canvas = maybe_downscale(canvas, MAX_DISPLAY_W)\n",
    "\n",
    "            # On-screen text for processing time\n",
    "            cv2.putText(canvas, f\"{os.path.basename(p)}  |  proc: {proc_ms:.1f} ms\",\n",
    "                        (12, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 3, cv2.LINE_AA)\n",
    "            cv2.putText(canvas, f\"{os.path.basename(p)}  |  proc: {proc_ms:.1f} ms\",\n",
    "                        (12, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, TEXT_CLR, 1, cv2.LINE_AA)\n",
    "            cv2.putText(canvas, \"SPACE: next   q/ESC: quit\",\n",
    "                        (12, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 3, cv2.LINE_AA)\n",
    "            cv2.putText(canvas, \"SPACE: next   q/ESC: quit\",\n",
    "                        (12, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, TEXT_CLR, 1, cv2.LINE_AA)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] error on {os.path.basename(p)}: {e}\")\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Responsive event loop: keep pumping events so SPACE is captured\n",
    "        while True:\n",
    "            cv2.imshow(WIN_NAME, canvas)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            # Handle window close (on some platforms returns -1 repeatedly after close)\n",
    "            if cv2.getWindowProperty(WIN_NAME, cv2.WND_PROP_VISIBLE) < 1:\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "            if key == 32:          # SPACE → next\n",
    "                i += 1\n",
    "                break\n",
    "            elif key in (ord('q'), 27):  # q or ESC\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "            # else: loop continues; window remains responsive\n",
    "\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "757c9b34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n",
      "0.1871293750591576\n",
      "0.1501049166545272\n",
      "0.1860005841590464\n",
      "0.15129170892760158\n",
      "0.18120850017294288\n",
      "0.19755674991756678\n",
      "0.22067958302795887\n",
      "0.17453087493777275\n",
      "0.18634033296257257\n",
      "0.18385350005701184\n",
      "0.15613700030371547\n",
      "0.14952054200693965\n",
      "0.11876887502148747\n",
      "0.12271262519061565\n",
      "0.07884454168379307\n",
      "0.07677166583016515\n",
      "0.16376441717147827\n",
      "0.16159408260136843\n",
      "0.16883049998432398\n",
      "0.16453683329746127\n",
      "0.16375254187732935\n",
      "0.11659608315676451\n",
      "0.24315987480804324\n",
      "0.18780637485906482\n",
      "0.18973691575229168\n",
      "0.1846552500501275\n",
      "0.17420045798644423\n",
      "0.18478129198774695\n",
      "0.16833641566336155\n",
      "0.1670771250501275\n",
      "0.15630604093894362\n",
      "0.1598291671834886\n",
      "0.260602124966681\n",
      "0.1984051251783967\n",
      "0.21390216704457998\n",
      "0.2956170840188861\n",
      "0.34607204236090183\n",
      "0.1842692499049008\n",
      "0.2282446250319481\n",
      "0.1795513746328652\n",
      "0.1732957921922207\n",
      "0.1530039170756936\n",
      "0.14450700022280216\n",
      "0.13982208305969834\n",
      "0.18467141641303897\n",
      "0.21589562483131886\n",
      "0.1689679161645472\n",
      "0.16078633395954967\n",
      "0.16237020771950483\n",
      "0.14578545792028308\n",
      "0.1454701661132276\n",
      "0.14393449993804097\n",
      "0.1948925000615418\n",
      "0.18941045878455043\n",
      "0.18408225011080503\n",
      "0.17690666671842337\n",
      "0.1566117499023676\n",
      "0.11650216719135642\n",
      "0.23560962500050664\n",
      "0.18867941666394472\n",
      "0.13423291686922312\n",
      "0.1259847916662693\n",
      "0.19264391670003533\n",
      "0.1803787499666214\n",
      "0.24333833390846848\n",
      "0.18430062476545572\n",
      "0.1858448339626193\n",
      "0.1794752087444067\n",
      "0.06436983309686184\n",
      "0.06201279070228338\n",
      "0.11941241705790162\n",
      "0.10660716611891985\n",
      "0.1670698751695454\n",
      "0.11966666718944907\n",
      "0.11791166616603732\n",
      "0.110898416955024\n",
      "0.1555898329243064\n",
      "0.10969070810824633\n",
      "0.09566162480041385\n",
      "0.08449599985033274\n",
      "0.15517245791852474\n",
      "0.14648154191672802\n",
      "0.11291833268478513\n",
      "0.10421754186972976\n",
      "0.11807708395645022\n",
      "0.11525754164904356\n",
      "0.12864662474021316\n",
      "0.12132875015959144\n",
      "0.2191902082413435\n",
      "0.17295037489384413\n",
      "0.17052283277735114\n",
      "0.16832091705873609\n",
      "0.13426120905205607\n",
      "0.0970145002938807\n",
      "0.19320420874282718\n",
      "0.14880783297121525\n",
      "0.16762095829471946\n",
      "0.16507229069247842\n",
      "0.29955783300101757\n",
      "0.17337195808067918\n",
      "0.1828517080284655\n",
      "0.18300087517127395\n",
      "0.23003141628578305\n",
      "0.1933768750168383\n",
      "0.15251041669398546\n",
      "0.1522268750704825\n",
      "0.19347708392888308\n",
      "0.18403716664761305\n",
      "0.192523957695812\n",
      "0.18193079065531492\n",
      "0.22489333432167768\n",
      "0.18151420913636684\n",
      "0.179428624920547\n",
      "0.1814429173246026\n",
      "0.05612966604530811\n",
      "0.056811250280588865\n",
      "0.10544404201209545\n",
      "0.05833375034853816\n",
      "0.1522903749719262\n",
      "0.15704954182729125\n",
      "0.18362287525087595\n",
      "0.18268924998119473\n",
      "0.18777562491595745\n",
      "0.17014124989509583\n",
      "0.09822108270600438\n",
      "0.060986250173300505\n",
      "0.1315369587391615\n",
      "0.12502258364111185\n",
      "0.15389212500303984\n",
      "0.16458833403885365\n",
      "0.22738320799544454\n",
      "0.17968437541276217\n",
      "0.18121804110705853\n",
      "0.18170070787891746\n",
      "0.24153550015762448\n",
      "0.19091370841488242\n",
      "0.22435533441603184\n",
      "0.18397862510755658\n",
      "0.20488920900970697\n",
      "0.1943470831029117\n",
      "0.10760304098948836\n",
      "0.10154470801353455\n",
      "0.03388245822861791\n",
      "0.027523708064109087\n",
      "0.024049666244536638\n",
      "0.021737665869295597\n",
      "0.023531374987214804\n",
      "0.021836249623447657\n",
      "\n",
      "───── Speed-test summary (warm-up skipped) ─────\n",
      "Total frames           : 158\n",
      "Warm-up frames ignored : 10\n",
      "Frames timed           : 148\n",
      "Total wall-clock time  : 31.92 s\n",
      "Average / frame        : 158.41 ms  (6.31 FPS)\n",
      "Median                 : 167.35 ms\n",
      "Fastest                : 21.74 ms\n",
      "Slowest                : 346.07 ms\n",
      "────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Speed-test: run the full segmentation/overlay pipeline on every image in\n",
    "~/Documents/GitHub/Ai-plays-SubwaySurfers/frames and print a timing\n",
    "summary at the end.\n",
    "\n",
    "Changes in this version\n",
    "────────────────────────\n",
    "• The first WARMUP_FRAMES (default 10) are *executed* but **not timed**,\n",
    "  so model/kernel warm-up and disk cache effects don’t skew results.\n",
    "• Everything else is unchanged: no GUI overhead, concise summary.\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, glob, time, statistics\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ───────────────────────────────────────────────────────── Config ──\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40\n",
    "\n",
    "TARGET_COLORS_RGB  = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE          = 20.0\n",
    "MIN_REGION_SIZE    = 30\n",
    "MIN_REGION_HEIGHT  = 150\n",
    "\n",
    "HEAT_BLUR_KSIZE     = 51\n",
    "RED_SCORE_THRESH    = 220\n",
    "EXCLUDE_TOP_FRAC    = 0.40\n",
    "EXCLUDE_BOTTOM_FRAC = 0.15\n",
    "MIN_DARK_RED_AREA   = 1200\n",
    "TRI_SIZE_PX         = 18\n",
    "PURPLE              = (255, 0, 255)\n",
    "\n",
    "WARMUP_FRAMES       = 10   # how many frames to skip from timing stats\n",
    "\n",
    "obstacle_classes = {\n",
    "    0: \"BOOTS\", 1: \"GREYTRAIN\", 2: \"HIGHBARRIER1\", 3: \"JUMP\", 4: \"LOWBARRIER1\",\n",
    "    5: \"LOWBARRIER2\", 6: \"ORANGETRAIN\", 7: \"PILLAR\", 8: \"RAMP\", 9: \"RAILS\",\n",
    "    10: \"SIDEWALK\", 11: \"YELLOWTRAIN\"\n",
    "}\n",
    "CLASS_COLOURS = {\n",
    "    0: (255,255,0), 1: (192,192,192), 2: (0,128,255), 3: (0,255,0),\n",
    "    4: (255,0,255), 5: (0,255,255), 6: (255,128,0), 7: (128,0,255),\n",
    "    8: (0,0,128), 10: (128,128,0), 11: (255,255,102)\n",
    "}\n",
    "\n",
    "# ─────────────────────────────────────── Device / model init ─\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "model = YOLO(weights)\n",
    "try: model.fuse()\n",
    "except Exception: pass\n",
    "model.predict(\n",
    "    np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8),\n",
    "    task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, verbose=False, half=half\n",
    ")\n",
    "\n",
    "# ───────────────────────────── Helper processing functions ─\n",
    "def highlight_rails_mask_only_fast(img_bgr, rail_mask, target_colors_rgb,\n",
    "                                   tolerance=30.0,\n",
    "                                   min_region_size=50,\n",
    "                                   min_region_height=150):\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max()+1\n",
    "    x0, x1 = xs.min(), xs.max()+1\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb],\n",
    "                           dtype=np.float32)\n",
    "    diff  = img_roi.astype(np.float32)[:, :, None, :] - targets_bgr[None, None]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    colour_hit = np.any(dist2 <= tolerance**2, axis=-1)\n",
    "\n",
    "    combined = colour_hit & mask_roi\n",
    "    comp = combined.astype(np.uint8)\n",
    "    n, lbls, stats, _ = cv2.connectedComponentsWithStats(comp, 8)\n",
    "\n",
    "    good = np.zeros_like(combined)\n",
    "    for lbl in range(1, n):\n",
    "        area, h = stats[lbl, cv2.CC_STAT_AREA], stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and h >= min_region_height:\n",
    "            good[lbls == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = good\n",
    "    return filtered_full\n",
    "\n",
    "\n",
    "def red_vs_green_score(red_mask, green_mask, blur_ksize=51):\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    diff = (cv2.blur(red_mask.astype(np.float32), k) -\n",
    "            cv2.blur(green_mask.astype(np.float32), k))\n",
    "    norm = (diff / (2*(np.max(np.abs(diff))+1e-6)) + 0.5)\n",
    "    return np.clip(norm * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def process_frame(img_bgr):\n",
    "    res = model.predict(img_bgr, task=\"segment\", imgsz=IMG_SIZE,\n",
    "                        device=device, conf=CONF, iou=IOU,\n",
    "                        max_det=30, verbose=False, half=half)[0]\n",
    "    if res.masks is None:\n",
    "        return\n",
    "\n",
    "    masks   = res.masks.data.cpu().numpy()\n",
    "    classes = res.masks.cls.cpu().numpy() if hasattr(res.masks, \"cls\") \\\n",
    "              else res.boxes.cls.cpu().numpy()\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    rail_union = np.zeros(masks.shape[1:], bool)\n",
    "    for m, c in zip(masks, classes):\n",
    "        if int(c) == RAIL_ID:\n",
    "            rail_union |= m.astype(bool)\n",
    "\n",
    "    rail_mask = cv2.resize(rail_union.astype(np.uint8), (W, H),\n",
    "                           interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "    green = highlight_rails_mask_only_fast(img_bgr, rail_mask,\n",
    "                                           TARGET_COLORS_RGB, TOLERANCE,\n",
    "                                           MIN_REGION_SIZE, MIN_REGION_HEIGHT)\n",
    "    red   = rail_mask & ~green\n",
    "    score = red_vs_green_score(red, green, HEAT_BLUR_KSIZE)\n",
    "\n",
    "    # exclude top/bottom\n",
    "    score[:int(H*EXCLUDE_TOP_FRAC), :] = 0\n",
    "    score[H-int(H*EXCLUDE_BOTTOM_FRAC):, :] = 0\n",
    "\n",
    "    dark = (score >= RED_SCORE_THRESH).astype(np.uint8)\n",
    "    cv2.morphologyEx(\n",
    "        dark, cv2.MORPH_OPEN,\n",
    "        cv2.getStructuringElement(cv2.MORPH_RECT, (5, 9)),\n",
    "        dst=dark, iterations=1\n",
    "    )\n",
    "    # outputs are not displayed; function exists for timing only.\n",
    "\n",
    "# ─────────────────────────────────────────────────────── Main test ──\n",
    "if __name__ == \"__main__\":\n",
    "    folder = (Path.home() / \"Documents\" / \"GitHub\" /\n",
    "              \"Ai-plays-SubwaySurfers\" / \"frames\")\n",
    "\n",
    "    if not folder.is_dir():\n",
    "        print(f\"[ERROR] frames folder not found: {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    paths = sorted(\n",
    "        glob.glob(str(folder / \"frame_*.jpg\")) +\n",
    "        glob.glob(str(folder / \"frame_*.png\")) +\n",
    "        glob.glob(str(folder / \"*.jpg\")) +\n",
    "        glob.glob(str(folder / \"*.png\"))\n",
    "    )\n",
    "    if not paths:\n",
    "        print(f\"[ERROR] No frame images found in {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    times = []\n",
    "    t_start = time.perf_counter()\n",
    "\n",
    "    for idx, p in enumerate(paths):\n",
    "        frame = cv2.imread(p)\n",
    "\n",
    "        if idx < WARMUP_FRAMES:\n",
    "            # Warm-up: run but don't time\n",
    "            process_frame(frame)\n",
    "            continue\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        process_frame(frame)\n",
    "        times.append(time.perf_counter() - t0)\n",
    "        print(time.perf_counter() - t0)\n",
    "\n",
    "    total_time = time.perf_counter() - t_start\n",
    "    timed_frames = len(times)\n",
    "\n",
    "    # ─────────────────────────────── Summary ─\n",
    "    ms = [t * 1_000 for t in times]\n",
    "    if timed_frames == 0:\n",
    "        print(\"\\n[WARNING] fewer frames than WARMUP_FRAMES; nothing timed.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    print(\"\\n───── Speed-test summary (warm-up skipped) ─────\")\n",
    "    print(f\"Total frames           : {len(paths)}\")\n",
    "    print(f\"Warm-up frames ignored : {WARMUP_FRAMES}\")\n",
    "    print(f\"Frames timed           : {timed_frames}\")\n",
    "    print(f\"Total wall-clock time  : {total_time:,.2f} s\")\n",
    "    print(f\"Average / frame        : {statistics.mean(ms):,.2f} ms\"\n",
    "          f\"  ({1_000/statistics.mean(ms):,.2f} FPS)\")\n",
    "    print(f\"Median                 : {statistics.median(ms):,.2f} ms\")\n",
    "    print(f\"Fastest                : {min(ms):,.2f} ms\")\n",
    "    print(f\"Slowest                : {max(ms):,.2f} ms\")\n",
    "    print(\"────────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "953e4279",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtering out smaller red regions to minimise misreads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef260ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 23:36:48.296 Python[71293:29572909] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-05 23:36:48.296 Python[71293:29572909] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Visualise *all* segmentation masks with labelled overlays.\n",
    "\n",
    "• Rails are tinted red (right) and used for the heat-map + purple-triangle.\n",
    "• Other masks are labelled and coloured on the original (left).\n",
    "• Bottom panel shows the heat-map.\n",
    "\n",
    "Controls:\n",
    "  SPACE: next frame   |   q / ESC: quit\n",
    "\n",
    "Loads images from:\n",
    "    ~/Documents/GitHub/Ai-plays-SubwaySurfers/frames\n",
    "\n",
    "Change requested:\n",
    "  Before plotting a purple triangle, require that the component's dark-red\n",
    "  area is at least 15% of the *total* dark-red area observed for the frame.\n",
    "  Implemented with negligible overhead and no other logic changes.\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, sys, time\n",
    "import cv2, torch, numpy as np\n",
    "from pathlib import Path\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40\n",
    "\n",
    "TARGET_COLORS_RGB  = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE          = 20.0\n",
    "MIN_REGION_SIZE    = 30\n",
    "MIN_REGION_HEIGHT  = 150\n",
    "\n",
    "HEAT_BLUR_KSIZE     = 51\n",
    "RED_SCORE_THRESH    = 220\n",
    "EXCLUDE_TOP_FRAC    = 0.40\n",
    "EXCLUDE_BOTTOM_FRAC = 0.15\n",
    "MIN_DARK_RED_AREA   = 1200\n",
    "TRI_SIZE_PX         = 18\n",
    "PURPLE              = (255, 0, 255)\n",
    "\n",
    "# New: minimum fraction of total dark-red area a blob must contribute\n",
    "MIN_DARK_FRACTION   = 0.15  # 15%\n",
    "\n",
    "# UI / display constraints\n",
    "WIN_NAME        = \"Left: labels | Right: rails | Bottom: heat-map (SPACE=next, q=quit)\"\n",
    "MAX_DISPLAY_W   = 1600\n",
    "TEXT_CLR        = (255, 255, 255)\n",
    "\n",
    "obstacle_classes = {\n",
    "    0: \"BOOTS\", 1: \"GREYTRAIN\", 2: \"HIGHBARRIER1\", 3: \"JUMP\", 4: \"LOWBARRIER1\",\n",
    "    5: \"LOWBARRIER2\", 6: \"ORANGETRAIN\", 7: \"PILLAR\", 8: \"RAMP\", 9: \"RAILS\",\n",
    "    10: \"SIDEWALK\", 11: \"YELLOWTRAIN\"\n",
    "}\n",
    "CLASS_COLOURS = {\n",
    "    0: (255,255,0), 1: (192,192,192), 2: (0,128,255), 3: (0,255,0),\n",
    "    4: (255,0,255), 5: (0,255,255), 6: (255,128,0), 7: (128,0,255),\n",
    "    8: (0,0,128), 10: (128,128,0), 11: (255,255,102)\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# Optional: reduce OpenCV CPU thread contention with PyTorch\n",
    "try:\n",
    "    cv2.setNumThreads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "model = YOLO(weights)\n",
    "try: model.fuse()\n",
    "except Exception: pass\n",
    "\n",
    "# warm up once so the first frame isn't laggy\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(_dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "                  conf=CONF, iou=IOU, verbose=False, half=half)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def highlight_rails_mask_only_fast(img_bgr, rail_mask, target_colors_rgb,\n",
    "                                   tolerance=30.0,\n",
    "                                   min_region_size=50,\n",
    "                                   min_region_height=150):\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max()+1\n",
    "    x0, x1 = xs.min(), xs.max()+1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb],\n",
    "                           dtype=np.float32)\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    colour_hit = np.any(dist2 <= tolerance**2, axis=-1)\n",
    "\n",
    "    combined = colour_hit & mask_roi\n",
    "    comp = combined.astype(np.uint8)\n",
    "    n, lbls, stats, _ = cv2.connectedComponentsWithStats(comp, 8)\n",
    "\n",
    "    good = np.zeros_like(combined)\n",
    "    for lbl in range(1, n):\n",
    "        area, h = stats[lbl, cv2.CC_STAT_AREA], stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and h >= min_region_height:\n",
    "            good[lbls == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = good\n",
    "    return filtered_full\n",
    "\n",
    "\n",
    "def red_vs_green_score(red_mask, green_mask, blur_ksize=51):\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    r = cv2.blur(red_mask.astype(np.float32), k)\n",
    "    g = cv2.blur(green_mask.astype(np.float32), k)\n",
    "    diff = r - g\n",
    "    amax = float(np.max(np.abs(diff))) + 1e-6\n",
    "    norm = (diff / (2*amax) + 0.5)\n",
    "    return np.clip(norm * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def draw_triangle(img, x, y, size=TRI_SIZE_PX, colour=PURPLE):\n",
    "    h = int(size * 1.2)\n",
    "    pts = np.array([[x, y], [x-size, y+h], [x+size, y+h]], np.int32)\n",
    "    cv2.fillConvexPoly(img, pts, colour)\n",
    "    cv2.polylines(img, [pts.reshape(-1,1,2)], True, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def process_frame(img_bgr):\n",
    "    \"\"\"Returns (left, right, heat_col) – all uint8 BGR images.\"\"\"\n",
    "    res = model.predict(img_bgr, task=\"segment\", imgsz=IMG_SIZE,\n",
    "                        device=device, conf=CONF, iou=IOU,\n",
    "                        max_det=30, verbose=False, half=half)[0]\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:\n",
    "        blank_heat = np.zeros((H, W), np.uint8)\n",
    "        return img_bgr.copy(), img_bgr.copy(), cv2.applyColorMap(blank_heat, cv2.COLORMAP_JET)\n",
    "\n",
    "    masks   = res.masks.data.cpu().numpy()\n",
    "    classes = res.masks.cls.cpu().numpy() if hasattr(res.masks, \"cls\") \\\n",
    "              else res.boxes.cls.cpu().numpy()\n",
    "    h_m, w_m = masks.shape[1:]\n",
    "\n",
    "    rail_union = np.zeros((h_m, w_m), bool)\n",
    "    for m, c in zip(masks, classes):\n",
    "        if int(c) == RAIL_ID:\n",
    "            rail_union |= m.astype(bool)\n",
    "\n",
    "    rail_mask = rail_union\n",
    "    if rail_mask.shape != (H, W):\n",
    "        rail_mask = cv2.resize(rail_mask.astype(np.uint8), (W, H),\n",
    "                               interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "    green = highlight_rails_mask_only_fast(img_bgr, rail_mask,\n",
    "                                           TARGET_COLORS_RGB, TOLERANCE,\n",
    "                                           MIN_REGION_SIZE, MIN_REGION_HEIGHT)\n",
    "    red   = rail_mask & ~green\n",
    "    score = red_vs_green_score(red, green, HEAT_BLUR_KSIZE)\n",
    "\n",
    "    top_ex = int(H * EXCLUDE_TOP_FRAC)\n",
    "    bot_ex = int(H * EXCLUDE_BOTTOM_FRAC)\n",
    "    score[:top_ex, :] = 0\n",
    "    if bot_ex: score[H-bot_ex:, :] = 0\n",
    "\n",
    "    dark = (score >= RED_SCORE_THRESH).astype(np.uint8)\n",
    "    dark = cv2.morphologyEx(\n",
    "        dark, cv2.MORPH_OPEN,\n",
    "        cv2.getStructuringElement(cv2.MORPH_RECT, (5, 9)),\n",
    "        iterations=1\n",
    "    )\n",
    "\n",
    "    # ================== New tiny-overhead filter ==================\n",
    "    # Require a component to be at least MIN_DARK_FRACTION of total dark area.\n",
    "    total_dark_area = int(dark.sum())\n",
    "    frac_area_thresh = int(np.ceil(MIN_DARK_FRACTION * total_dark_area))\n",
    "    # =============================================================\n",
    "\n",
    "    # left: labels overlaid\n",
    "    left    = img_bgr.copy()\n",
    "    overlay = left.copy()\n",
    "    for m, c in zip(masks, classes):\n",
    "        cid = int(c)\n",
    "        if cid == RAIL_ID:\n",
    "            continue\n",
    "        mask_full = m\n",
    "        if mask_full.shape != (H, W):\n",
    "            mask_full = cv2.resize(mask_full.astype(np.uint8), (W, H),\n",
    "                                   interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "        label  = obstacle_classes.get(cid, f\"CLASS {cid}\")\n",
    "        colour = CLASS_COLOURS.get(cid, (255, 255, 255))\n",
    "        overlay[mask_full] = colour\n",
    "\n",
    "        ys, xs = np.where(mask_full)\n",
    "        if len(xs):\n",
    "            x_c, y_c = int(xs.mean()), int(ys.mean())\n",
    "            cv2.putText(overlay, label, (x_c - 40, y_c),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2, cv2.LINE_AA)\n",
    "            cv2.putText(overlay, label, (x_c - 40, y_c),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)\n",
    "    left = cv2.addWeighted(overlay, 0.6, left, 0.4, 0)\n",
    "\n",
    "    # purple triangle warnings\n",
    "    n_lbl, lbl_mat, stats, _ = cv2.connectedComponentsWithStats(dark, 8)\n",
    "    for lbl in range(1, n_lbl):\n",
    "        area = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        if area < MIN_DARK_RED_AREA or area < frac_area_thresh:\n",
    "            continue\n",
    "        ys, xs = np.where(lbl_mat == lbl)\n",
    "        y_top, x_mid = ys.min(), int(xs[ys == ys.min()].mean())\n",
    "        draw_triangle(left, x_mid, y_top)\n",
    "\n",
    "    # right: rails tinted\n",
    "    right = img_bgr.copy()\n",
    "    rails_tinted = right.copy()\n",
    "    rails_tinted[rail_mask] = (0,0,255)\n",
    "    right = cv2.addWeighted(rails_tinted, ALPHA, right, 1-ALPHA, 0)\n",
    "    right[green] = (0,255,0)\n",
    "\n",
    "    # bottom: heat map (width = left+right, height = H)\n",
    "    heat_col = cv2.applyColorMap(score, cv2.COLORMAP_JET)\n",
    "    heat_col = cv2.resize(heat_col, (left.shape[1] + right.shape[1], H),\n",
    "                          interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return left, right, heat_col\n",
    "\n",
    "def assemble_canvas(left, right, heat):\n",
    "    top    = np.hstack((left, right))\n",
    "    canvas = np.vstack((top, heat))\n",
    "    return canvas\n",
    "\n",
    "def maybe_downscale(img, max_w=MAX_DISPLAY_W):\n",
    "    h, w = img.shape[:2]\n",
    "    if w <= max_w:\n",
    "        return img\n",
    "    scale = max_w / float(w)\n",
    "    new_size = (int(w*scale), int(h*scale))\n",
    "    return cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    folder = Path.home() / \"Documents\" / \"GitHub\" / \"Ai-plays-SubwaySurfers\" / \"frames\"\n",
    "    if not folder.is_dir():\n",
    "        print(f\"frames folder not found: {folder}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    paths = sorted(\n",
    "        glob.glob(str(folder / \"frame_*.jpg\")) +\n",
    "        glob.glob(str(folder / \"frame_*.png\")) +\n",
    "        glob.glob(str(folder / \"*.jpg\")) +\n",
    "        glob.glob(str(folder / \"*.png\"))\n",
    "    )\n",
    "    if not paths:\n",
    "        print(f\"No frame images found in {folder}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    cv2.namedWindow(WIN_NAME, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    i = 0\n",
    "    n = len(paths)\n",
    "    while i < n:\n",
    "        p = paths[i]\n",
    "        frame = cv2.imread(p)\n",
    "        if frame is None:\n",
    "            print(f\"[WARN] unreadable image: {p}\")\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            left, right, heat = process_frame(frame)\n",
    "            proc_ms = (time.perf_counter() - t0) * 1000.0\n",
    "            canvas = assemble_canvas(left, right, heat)\n",
    "            canvas = maybe_downscale(canvas, MAX_DISPLAY_W)\n",
    "\n",
    "            # On-screen text for processing time\n",
    "            cv2.putText(canvas, f\"{os.path.basename(p)}  |  proc: {proc_ms:.1f} ms\",\n",
    "                        (12, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 3, cv2.LINE_AA)\n",
    "            cv2.putText(canvas, f\"{os.path.basename(p)}  |  proc: {proc_ms:.1f} ms\",\n",
    "                        (12, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, TEXT_CLR, 1, cv2.LINE_AA)\n",
    "            cv2.putText(canvas, \"SPACE: next   q/ESC: quit\",\n",
    "                        (12, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 3, cv2.LINE_AA)\n",
    "            cv2.putText(canvas, \"SPACE: next   q/ESC: quit\",\n",
    "                        (12, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, TEXT_CLR, 1, cv2.LINE_AA)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] error on {os.path.basename(p)}: {e}\")\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Responsive event loop\n",
    "        while True:\n",
    "            cv2.imshow(WIN_NAME, canvas)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            # Handle window close\n",
    "            if cv2.getWindowProperty(WIN_NAME, cv2.WND_PROP_VISIBLE) < 1:\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "            if key == 32:          # SPACE → next\n",
    "                i += 1\n",
    "                break\n",
    "            elif key in (ord('q'), 27):  # q or ESC\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7856ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n",
      "Loading pretrain weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4316.)\n",
      "2025-08-05 23:36:23.846 Python[68509:29500591] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-05 23:36:23.846 Python[68509:29500591] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Visualise *all* segmentation masks with labelled overlays + Jake bbox.\n",
    "\n",
    "Additions (no double work, minimal overhead):\n",
    "• Runs RF-DETR once per frame to detect Jake (green rectangle).\n",
    "• Finds the YOLO mask with the largest overlap under Jake's bbox and\n",
    "  tints that object PINK so you can see what he's under.\n",
    "• Reuses the same YOLO segmentation output for everything (no duplicate inference).\n",
    "• Keeps the responsive SPACE-next UI and the purple-triangle logic (with 15% area filter).\n",
    "\n",
    "Controls:\n",
    "  SPACE: next frame   |   q / ESC: quit\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, sys, time\n",
    "# ── Enable CPU fallback before importing torch (for MPS gaps) ────────────────\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from rfdetr import RFDETRBase\n",
    "\n",
    "# ── Monkey-patch interpolate to disable antialias (avoids unsupported MPS op) ─\n",
    "_original_interpolate = F.interpolate\n",
    "def _interpolate_no_antialias(input, *args, **kwargs):\n",
    "    kwargs['antialias'] = False\n",
    "    return _original_interpolate(input, *args, **kwargs)\n",
    "F.interpolate = _interpolate_no_antialias\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "yolo_weights = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "jake_weights = f\"{home}/downloads/weightsjake.pt\"\n",
    "\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40\n",
    "\n",
    "TARGET_COLORS_RGB  = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE          = 20.0\n",
    "MIN_REGION_SIZE    = 30\n",
    "MIN_REGION_HEIGHT  = 150\n",
    "\n",
    "HEAT_BLUR_KSIZE     = 51\n",
    "RED_SCORE_THRESH    = 220\n",
    "EXCLUDE_TOP_FRAC    = 0.40\n",
    "EXCLUDE_BOTTOM_FRAC = 0.15\n",
    "MIN_DARK_RED_AREA   = 1200\n",
    "TRI_SIZE_PX         = 18\n",
    "PURPLE              = (255, 0, 255)\n",
    "\n",
    "# Purple-triangle extra filter: blob must be ≥ this fraction of total dark area\n",
    "MIN_DARK_FRACTION   = 0.15  # 15%\n",
    "\n",
    "# UI / display constraints\n",
    "WIN_NAME        = \"Left: labels | Right: rails | Bottom: heat (SPACE=next, q=quit)\"\n",
    "MAX_DISPLAY_W   = 1600\n",
    "TEXT_CLR        = (255, 255, 255)\n",
    "JAKE_BOX_CLR    = (0, 255, 0)   # green bbox around Jake\n",
    "UNDER_TINT_BGR  = (255, 0, 255) # pink/magenta tint for object Jake is under\n",
    "\n",
    "obstacle_classes = {\n",
    "    0: \"BOOTS\", 1: \"GREYTRAIN\", 2: \"HIGHBARRIER1\", 3: \"JUMP\", 4: \"LOWBARRIER1\",\n",
    "    5: \"LOWBARRIER2\", 6: \"ORANGETRAIN\", 7: \"PILLAR\", 8: \"RAMP\", 9: \"RAILS\",\n",
    "    10: \"SIDEWALK\", 11: \"YELLOWTRAIN\"\n",
    "}\n",
    "CLASS_COLOURS = {\n",
    "    0: (255,255,0), 1: (192,192,192), 2: (0,128,255), 3: (0,255,0),\n",
    "    4: (255,0,255), 5: (0,255,255), 6: (255,128,0), 7: (128,0,255),\n",
    "    8: (0,0,128), 10: (128,128,0), 11: (255,255,102)\n",
    "}\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "# YOLO device: CUDA > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    yolo_device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    yolo_device, half = \"mps\", False\n",
    "else:\n",
    "    yolo_device, half = \"cpu\", False\n",
    "\n",
    "# RF-DETR device: prefer MPS if present, else CPU (it throws NotImplementedError for some ops)\n",
    "det_device = \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Optional: reduce OpenCV CPU thread contention with PyTorch\n",
    "try:\n",
    "    cv2.setNumThreads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# =======================\n",
    "# Load models + warmup\n",
    "# =======================\n",
    "yolo_model = YOLO(yolo_weights)\n",
    "try: yolo_model.fuse()\n",
    "except Exception: pass\n",
    "\n",
    "jake_model = RFDETRBase(pretrain_weights=jake_weights, num_classes=3)\n",
    "\n",
    "# Warm-up YOLO so first frame isn't laggy\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = yolo_model.predict(_dummy, task=\"segment\", imgsz=IMG_SIZE,\n",
    "                       device=yolo_device, conf=CONF, iou=IOU,\n",
    "                       verbose=False, half=half)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def highlight_rails_mask_only_fast(img_bgr, rail_mask, target_colors_rgb,\n",
    "                                   tolerance=30.0,\n",
    "                                   min_region_size=50,\n",
    "                                   min_region_height=150):\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max()+1\n",
    "    x0, x1 = xs.min(), xs.max()+1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb],\n",
    "                           dtype=np.float32)\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    colour_hit = np.any(dist2 <= tolerance**2, axis=-1)\n",
    "\n",
    "    combined = colour_hit & mask_roi\n",
    "    comp = combined.astype(np.uint8)\n",
    "    n, lbls, stats, _ = cv2.connectedComponentsWithStats(comp, 8)\n",
    "\n",
    "    good = np.zeros_like(combined)\n",
    "    for lbl in range(1, n):\n",
    "        area, h = stats[lbl, cv2.CC_STAT_AREA], stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and h >= min_region_height:\n",
    "            good[lbls == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = good\n",
    "    return filtered_full\n",
    "\n",
    "\n",
    "def red_vs_green_score(red_mask, green_mask, blur_ksize=51):\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    r = cv2.blur(red_mask.astype(np.float32), k)\n",
    "    g = cv2.blur(green_mask.astype(np.float32), k)\n",
    "    diff = r - g\n",
    "    amax = float(np.max(np.abs(diff))) + 1e-6\n",
    "    norm = (diff / (2*amax) + 0.5)\n",
    "    return np.clip(norm * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def draw_triangle(img, x, y, size=TRI_SIZE_PX, colour=PURPLE):\n",
    "    h = int(size * 1.2)\n",
    "    pts = np.array([[x, y], [x-size, y+h], [x+size, y+h]], np.int32)\n",
    "    cv2.fillConvexPoly(img, pts, colour)\n",
    "    cv2.polylines(img, [pts.reshape(-1,1,2)], True, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "\n",
    "def assemble_canvas(left, right, heat):\n",
    "    top    = np.hstack((left, right))\n",
    "    canvas = np.vstack((top, heat))\n",
    "    return canvas\n",
    "\n",
    "def maybe_downscale(img, max_w=MAX_DISPLAY_W):\n",
    "    h, w = img.shape[:2]\n",
    "    if w <= max_w:\n",
    "        return img\n",
    "    scale = max_w / float(w)\n",
    "    new_size = (int(w*scale), int(h*scale))\n",
    "    return cv2.resize(img, new_size, interpolation=cv2.INTER_AREA)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "def process_frame(img_bgr):\n",
    "    \"\"\"\n",
    "    One-stop processing:\n",
    "    • RF-DETR (Jake bbox)\n",
    "    • YOLO-seg once\n",
    "    • build left/right/heat\n",
    "    • tint object under Jake pink on the LEFT pane, draw Jake bbox\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "\n",
    "    # RF-DETR: Jake bbox (use PIL once; reuse the same pixels)\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "    try:\n",
    "        dets = jake_model.predict(pil_img, threshold=0.5, device=det_device)[0]\n",
    "    except NotImplementedError:\n",
    "        dets = jake_model.predict(pil_img, threshold=0.5, device=\"cpu\")[0]\n",
    "\n",
    "    jake_xyxy = None\n",
    "    if hasattr(dets, \"xyxy\") and len(dets.xyxy) > 0:\n",
    "        x1, y1, x2, y2 = dets.xyxy[0].astype(int).tolist()\n",
    "        # clamp to image\n",
    "        x1 = max(0, min(W-1, x1)); x2 = max(0, min(W-1, x2))\n",
    "        y1 = max(0, min(H-1, y1)); y2 = max(0, min(H-1, y2))\n",
    "        if x2 > x1 and y2 > y1:\n",
    "            jake_xyxy = (x1, y1, x2, y2)\n",
    "\n",
    "    # YOLO segmentation ONCE (use original BGR frame)\n",
    "    res = yolo_model.predict(img_bgr, task=\"segment\", imgsz=IMG_SIZE,\n",
    "                             device=yolo_device, conf=CONF, iou=IOU,\n",
    "                             max_det=30, verbose=False, half=half)[0]\n",
    "\n",
    "    if res.masks is None:\n",
    "        # nothing detected; return pass-through panes\n",
    "        left = img_bgr.copy()\n",
    "        right = img_bgr.copy()\n",
    "        heat_col = cv2.applyColorMap(np.zeros((H, W), np.uint8), cv2.COLORMAP_JET)\n",
    "        # draw Jake bbox even if no masks\n",
    "        if jake_xyxy:\n",
    "            x1, y1, x2, y2 = jake_xyxy\n",
    "            cv2.rectangle(left, (x1, y1), (x2, y2), JAKE_BOX_CLR, 2)\n",
    "        return left, right, heat_col\n",
    "\n",
    "    masks   = res.masks.data.cpu().numpy()  # (N, h_m, w_m)\n",
    "    classes = res.masks.cls.cpu().numpy() if hasattr(res.masks, \"cls\") \\\n",
    "              else res.boxes.cls.cpu().numpy()\n",
    "    classes = classes.astype(int)\n",
    "    h_m, w_m = masks.shape[1:]\n",
    "\n",
    "    # Build rail mask union at low-res then resize once\n",
    "    rail_union = np.zeros((h_m, w_m), dtype=bool)\n",
    "    for m, c in zip(masks, classes):\n",
    "        if int(c) == RAIL_ID:\n",
    "            rail_union |= m.astype(bool)\n",
    "    rail_mask = cv2.resize(rail_union.astype(np.uint8), (W, H),\n",
    "                           interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "    # Green/red rail segmentation\n",
    "    green = highlight_rails_mask_only_fast(img_bgr, rail_mask,\n",
    "                                           TARGET_COLORS_RGB, TOLERANCE,\n",
    "                                           MIN_REGION_SIZE, MIN_REGION_HEIGHT)\n",
    "    red   = rail_mask & ~green\n",
    "    score = red_vs_green_score(red, green, HEAT_BLUR_KSIZE)\n",
    "\n",
    "    # Exclude top/bottom bands\n",
    "    top_ex = int(H * EXCLUDE_TOP_FRAC)\n",
    "    bot_ex = int(H * EXCLUDE_BOTTOM_FRAC)\n",
    "    score[:top_ex, :] = 0\n",
    "    if bot_ex: score[H-bot_ex:, :] = 0\n",
    "\n",
    "    dark = (score >= RED_SCORE_THRESH).astype(np.uint8)\n",
    "    dark = cv2.morphologyEx(\n",
    "        dark, cv2.MORPH_OPEN,\n",
    "        cv2.getStructuringElement(cv2.MORPH_RECT, (5, 9)),\n",
    "        iterations=1\n",
    "    )\n",
    "\n",
    "    # Total dark area + 15% fraction threshold (tiny overhead)\n",
    "    total_dark_area   = int(dark.sum())\n",
    "    frac_area_thresh  = int(np.ceil(MIN_DARK_FRACTION * total_dark_area))\n",
    "\n",
    "    # LEFT PANE: labels overlaid + Jake tinting of object above him\n",
    "    left    = img_bgr.copy()\n",
    "    overlay = left.copy()\n",
    "\n",
    "    # First pass: overlay class masks (non-rail) with fixed colours + labels\n",
    "    # We'll also compute which mask Jake is under with low-res overlap (no per-mask resize).\n",
    "    best_idx, best_area, best_cid = None, 0, None\n",
    "    if jake_xyxy:\n",
    "        x1, y1, x2, y2 = jake_xyxy\n",
    "        sx, sy = w_m / W, h_m / H\n",
    "        mx1, mx2 = max(0, int(x1 * sx)), min(w_m, int(x2 * sx))\n",
    "        my1, my2 = max(0, int(y1 * sy)), min(h_m, int(y2 * sy))\n",
    "    else:\n",
    "        mx1 = mx2 = my1 = my2 = None\n",
    "\n",
    "    for idx, cid in enumerate(classes):\n",
    "        if cid != RAIL_ID:\n",
    "            m_low = masks[idx]  # low-res\n",
    "            # Draw class overlay (need full-res mask just for drawing)\n",
    "            mask_full = cv2.resize(m_low.astype(np.uint8), (W, H),\n",
    "                                   interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "            colour = CLASS_COLOURS.get(int(cid), (255, 255, 255))\n",
    "            overlay[mask_full] = colour\n",
    "\n",
    "            # label position\n",
    "            ys, xs = np.where(mask_full)\n",
    "            if len(xs):\n",
    "                x_c, y_c = int(xs.mean()), int(ys.mean())\n",
    "                label = obstacle_classes.get(int(cid), f\"CLASS {int(cid)}\")\n",
    "                cv2.putText(overlay, label, (x_c-40, y_c),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0,0,0), 2, cv2.LINE_AA)\n",
    "                cv2.putText(overlay, label, (x_c-40, y_c),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255,255,255), 1, cv2.LINE_AA)\n",
    "\n",
    "            # If Jake exists, compute overlap at low-res without resizing every mask\n",
    "            if jake_xyxy and (mx2 > mx1) and (my2 > my1):\n",
    "                area = int(m_low[my1:my2, mx1:mx2].sum())\n",
    "                if area > best_area:\n",
    "                    best_idx, best_area, best_cid = idx, area, int(cid)\n",
    "\n",
    "    left = cv2.addWeighted(overlay, 0.6, left, 0.4, 0)\n",
    "\n",
    "    # Purple-triangle warnings on LEFT (apply area thresholds, including 15%)\n",
    "    n_lbl, lbl_mat, stats, _ = cv2.connectedComponentsWithStats(dark, 8)\n",
    "    for lbl in range(1, n_lbl):\n",
    "        area = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        if area < MIN_DARK_RED_AREA or area < frac_area_thresh:\n",
    "            continue\n",
    "        ys, xs = np.where(lbl_mat == lbl)\n",
    "        y_top  = ys.min()\n",
    "        x_mid  = int(xs[ys == ys.min()].mean())\n",
    "        draw_triangle(left, x_mid, y_top)\n",
    "\n",
    "    # If Jake was detected, draw his bbox and tint the object he's under PINK on LEFT\n",
    "    if jake_xyxy:\n",
    "        x1, y1, x2, y2 = jake_xyxy\n",
    "        cv2.rectangle(left, (x1, y1), (x2, y2), JAKE_BOX_CLR, 2)\n",
    "        if best_idx is not None:\n",
    "            # Resize only the best mask once to full-res for the tint\n",
    "            best_mask_full = cv2.resize(masks[best_idx].astype(np.uint8), (W, H),\n",
    "                                        interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "            pink_layer = left.copy()\n",
    "            pink_layer[best_mask_full] = UNDER_TINT_BGR\n",
    "            left = cv2.addWeighted(pink_layer, 0.35, left, 0.65, 0)\n",
    "\n",
    "    # RIGHT PANE: rails tinted (red) and green highlights\n",
    "    right = img_bgr.copy()\n",
    "    rails_tinted = right.copy()\n",
    "    rails_tinted[rail_mask] = (0,0,255)          # red tint\n",
    "    right = cv2.addWeighted(rails_tinted, ALPHA, right, 1-ALPHA, 0)\n",
    "    right[green] = (0,255,0)                     # green for colour-matched\n",
    "\n",
    "    # BOTTOM: heat-map\n",
    "    heat_col = cv2.applyColorMap(score, cv2.COLORMAP_JET)\n",
    "    heat_col = cv2.resize(heat_col, (left.shape[1] + right.shape[1], H),\n",
    "                          interpolation=cv2.INTER_LINEAR)\n",
    "\n",
    "    return left, right, heat_col\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    folder = Path.home() / \"Documents\" / \"GitHub\" / \"Ai-plays-SubwaySurfers\" / \"frames\"\n",
    "    if not folder.is_dir():\n",
    "        print(f\"frames folder not found: {folder}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    paths = sorted(\n",
    "        glob.glob(str(folder / \"frame_*.jpg\")) +\n",
    "        glob.glob(str(folder / \"frame_*.png\")) +\n",
    "        glob.glob(str(folder / \"*.jpg\")) +\n",
    "        glob.glob(str(folder / \"*.png\"))\n",
    "    )\n",
    "    if not paths:\n",
    "        print(f\"No frame images found in {folder}\", file=sys.stderr); sys.exit(1)\n",
    "\n",
    "    cv2.namedWindow(WIN_NAME, cv2.WINDOW_NORMAL)\n",
    "\n",
    "    i = 0\n",
    "    n = len(paths)\n",
    "    while i < n:\n",
    "        p = paths[i]\n",
    "        frame = cv2.imread(p)\n",
    "        if frame is None:\n",
    "            print(f\"[WARN] unreadable image: {p}\")\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            t0 = time.perf_counter()\n",
    "            left, right, heat = process_frame(frame)\n",
    "            proc_ms = (time.perf_counter() - t0) * 1000.0\n",
    "            canvas = assemble_canvas(left, right, heat)\n",
    "            canvas = maybe_downscale(canvas, MAX_DISPLAY_W)\n",
    "\n",
    "            # On-screen text for processing time\n",
    "            cv2.putText(canvas, f\"{os.path.basename(p)}  |  proc: {proc_ms:.1f} ms\",\n",
    "                        (12, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0,0,0), 3, cv2.LINE_AA)\n",
    "            cv2.putText(canvas, f\"{os.path.basename(p)}  |  proc: {proc_ms:.1f} ms\",\n",
    "                        (12, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, TEXT_CLR, 1, cv2.LINE_AA)\n",
    "            cv2.putText(canvas, \"SPACE: next   q/ESC: quit\",\n",
    "                        (12, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0,0,0), 3, cv2.LINE_AA)\n",
    "            cv2.putText(canvas, \"SPACE: next   q/ESC: quit\",\n",
    "                        (12, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, TEXT_CLR, 1, cv2.LINE_AA)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] error on {os.path.basename(p)}: {e}\")\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # Responsive event loop: SPACE to advance, q/ESC to quit\n",
    "        while True:\n",
    "            cv2.imshow(WIN_NAME, canvas)\n",
    "            key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "            if cv2.getWindowProperty(WIN_NAME, cv2.WND_PROP_VISIBLE) < 1:\n",
    "                cv2.destroyAllWindows(); sys.exit(0)\n",
    "\n",
    "            if key == 32:          # SPACE → next\n",
    "                i += 1\n",
    "                break\n",
    "            elif key in (ord('q'), 27):  # q or ESC\n",
    "                cv2.destroyAllWindows(); sys.exit(0)\n",
    "\n",
    "    cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "820e7061",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n",
      "Loading pretrain weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4316.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     frame_00005.png   325.92 ms\n",
      "     frame_00005.png   294.13 ms\n",
      "     frame_00006.png   322.56 ms\n",
      "     frame_00006.png   291.36 ms\n",
      "     frame_00007.png   371.37 ms\n",
      "     frame_00007.png   321.62 ms\n",
      "     frame_00008.png   369.48 ms\n",
      "     frame_00008.png   335.42 ms\n",
      "     frame_00009.png   352.40 ms\n",
      "     frame_00009.png   548.38 ms\n",
      "     frame_00010.png   352.49 ms\n",
      "     frame_00010.png   326.17 ms\n",
      "     frame_00011.png   269.33 ms\n",
      "     frame_00011.png   252.66 ms\n",
      "     frame_00012.png   220.66 ms\n",
      "     frame_00012.png   227.22 ms\n",
      "     frame_00013.png   301.66 ms\n",
      "     frame_00013.png   316.42 ms\n",
      "     frame_00014.png   332.65 ms\n",
      "     frame_00014.png   335.53 ms\n",
      "     frame_00015.png   403.03 ms\n",
      "     frame_00015.png   278.27 ms\n",
      "     frame_00016.png   379.18 ms\n",
      "     frame_00016.png   324.31 ms\n",
      "     frame_00017.png   338.92 ms\n",
      "     frame_00017.png   306.40 ms\n",
      "     frame_00018.png   329.55 ms\n",
      "     frame_00018.png   298.18 ms\n",
      "     frame_00019.png   298.13 ms\n",
      "     frame_00019.png   333.32 ms\n",
      "     frame_00020.png   286.01 ms\n",
      "     frame_00020.png   338.50 ms\n",
      "     frame_00021.png   354.95 ms\n",
      "     frame_00021.png   310.02 ms\n",
      "     frame_00022.png   329.39 ms\n",
      "     frame_00022.png   302.57 ms\n",
      "     frame_00023.png   632.03 ms\n",
      "     frame_00023.png   389.00 ms\n",
      "     frame_00024.png   375.47 ms\n",
      "     frame_00024.png   372.19 ms\n",
      "     frame_00025.png   293.58 ms\n",
      "     frame_00025.png   298.39 ms\n",
      "     frame_00026.png   285.29 ms\n",
      "     frame_00026.png   268.91 ms\n",
      "     frame_00027.png   320.12 ms\n",
      "     frame_00027.png   322.33 ms\n",
      "     frame_00028.png   285.48 ms\n",
      "     frame_00028.png   361.62 ms\n",
      "     frame_00029.png   359.45 ms\n",
      "     frame_00029.png   323.68 ms\n",
      "     frame_00030.png   290.84 ms\n",
      "     frame_00030.png   282.81 ms\n",
      "     frame_00031.png   327.22 ms\n",
      "     frame_00031.png   305.52 ms\n",
      "     frame_00032.png   293.13 ms\n",
      "     frame_00032.png   304.11 ms\n",
      "     frame_00033.png   287.06 ms\n",
      "     frame_00033.png   252.27 ms\n",
      "     frame_00034.png   380.37 ms\n",
      "     frame_00034.png   305.93 ms\n",
      "     frame_00035.png   372.52 ms\n",
      "     frame_00035.png   275.26 ms\n",
      "     frame_00036.png   314.52 ms\n",
      "     frame_00036.png   318.10 ms\n",
      "     frame_00037.png   345.27 ms\n",
      "     frame_00037.png   306.64 ms\n",
      "     frame_00038.png   314.64 ms\n",
      "     frame_00038.png   310.92 ms\n",
      "     frame_00039.png   219.35 ms\n",
      "     frame_00039.png   225.84 ms\n",
      "     frame_00040.png   260.81 ms\n",
      "     frame_00040.png   244.21 ms\n",
      "     frame_00041.png   396.84 ms\n",
      "     frame_00041.png   263.97 ms\n",
      "     frame_00042.png   267.00 ms\n",
      "     frame_00042.png   254.58 ms\n",
      "     frame_00043.png   350.70 ms\n",
      "     frame_00043.png   256.40 ms\n",
      "     frame_00044.png   265.89 ms\n",
      "     frame_00044.png   230.70 ms\n",
      "     frame_00045.png   294.56 ms\n",
      "     frame_00045.png   277.41 ms\n",
      "     frame_00046.png   279.63 ms\n",
      "     frame_00046.png   246.16 ms\n",
      "     frame_00047.png   259.79 ms\n",
      "     frame_00047.png   263.47 ms\n",
      "     frame_00048.png   265.41 ms\n",
      "     frame_00048.png   256.07 ms\n",
      "     frame_00049.png   335.98 ms\n",
      "     frame_00049.png   391.09 ms\n",
      "     frame_00050.png   354.17 ms\n",
      "     frame_00050.png   294.55 ms\n",
      "     frame_00051.png   282.70 ms\n",
      "     frame_00051.png   255.11 ms\n",
      "     frame_00052.png   327.16 ms\n",
      "     frame_00052.png   316.37 ms\n",
      "     frame_00053.png   312.70 ms\n",
      "     frame_00053.png   443.58 ms\n",
      "     frame_00054.png   358.21 ms\n",
      "     frame_00054.png   360.39 ms\n",
      "     frame_00055.png   356.90 ms\n",
      "     frame_00055.png   320.66 ms\n",
      "     frame_00056.png   359.81 ms\n",
      "     frame_00056.png   320.84 ms\n",
      "     frame_00057.png   315.88 ms\n",
      "     frame_00057.png   281.25 ms\n",
      "     frame_00058.png   319.37 ms\n",
      "     frame_00058.png   322.35 ms\n",
      "     frame_00059.png   409.12 ms\n",
      "     frame_00059.png   410.30 ms\n",
      "     frame_00060.png   361.52 ms\n",
      "     frame_00060.png   314.22 ms\n",
      "     frame_00061.png   318.43 ms\n",
      "     frame_00061.png   321.55 ms\n",
      "     frame_00062.png   220.55 ms\n",
      "     frame_00062.png   225.24 ms\n",
      "     frame_00063.png   259.41 ms\n",
      "     frame_00063.png   218.18 ms\n",
      "     frame_00064.png   492.62 ms\n",
      "     frame_00064.png   336.66 ms\n",
      "     frame_00065.png   363.63 ms\n",
      "     frame_00065.png   326.09 ms\n",
      "     frame_00066.png   344.19 ms\n",
      "     frame_00066.png   531.39 ms\n",
      "     frame_00067.png   290.80 ms\n",
      "     frame_00067.png   238.18 ms\n",
      "     frame_00068.png   262.97 ms\n",
      "     frame_00068.png   328.60 ms\n",
      "     frame_00069.png   306.80 ms\n",
      "     frame_00069.png   293.72 ms\n",
      "     frame_00070.png   341.34 ms\n",
      "     frame_00070.png   309.59 ms\n",
      "     frame_00071.png   339.14 ms\n",
      "     frame_00071.png   317.29 ms\n",
      "     frame_00072.png   367.59 ms\n",
      "     frame_00072.png   316.62 ms\n",
      "     frame_00073.png   361.80 ms\n",
      "     frame_00073.png   313.71 ms\n",
      "     frame_00074.png   385.13 ms\n",
      "     frame_00074.png   360.13 ms\n",
      "     frame_00075.png   250.46 ms\n",
      "     frame_00075.png   258.47 ms\n",
      "     frame_00076.png   185.84 ms\n",
      "     frame_00076.png   196.79 ms\n",
      "     frame_00077.png   185.44 ms\n",
      "     frame_00077.png   186.76 ms\n",
      "     frame_00078.png   176.78 ms\n",
      "     frame_00078.png   170.51 ms\n",
      "\n",
      "───── Speed-test summary (warm-up skipped) ─────\n",
      "Total frames           : 158\n",
      "Warm-up frames ignored : 10\n",
      "Frames timed           : 148\n",
      "Total wall-clock time  : 56.23 s\n",
      "Average / frame        : 313.23 ms  (3.19 FPS)\n",
      "Median                 : 314.58 ms\n",
      "Fastest                : 170.51 ms\n",
      "Slowest                : 632.03 ms\n",
      "───────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Speed-test the integrated RF-DETR (Jake bbox) + YOLO-seg pipeline.\n",
    "\n",
    "• Uses the same processing logic as the interactive viewer, but with NO GUI.\n",
    "• Runs through all images in: ~/Documents/GitHub/Ai-plays-SubwaySurfers/frames\n",
    "• First 10 frames are executed as WARMUP (not timed).\n",
    "• Prints per-frame processing time (after warmup) and a summary at the end.\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, glob, time, statistics\n",
    "# ── Enable CPU fallback before importing torch (for MPS gaps) ────────────────\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from rfdetr import RFDETRBase\n",
    "\n",
    "# ── Monkey-patch interpolate to disable antialias (avoids unsupported MPS op) ─\n",
    "_original_interpolate = F.interpolate\n",
    "def _interpolate_no_antialias(input, *args, **kwargs):\n",
    "    kwargs['antialias'] = False\n",
    "    return _original_interpolate(input, *args, **kwargs)\n",
    "F.interpolate = _interpolate_no_antialias\n",
    "\n",
    "# ─────────────────────────────────────────────────────────── Config ──\n",
    "home     = os.path.expanduser(\"~\")\n",
    "yolo_weights = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "jake_weights = f\"{home}/downloads/weightsjake.pt\"\n",
    "\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40\n",
    "\n",
    "TARGET_COLORS_RGB  = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE          = 20.0\n",
    "MIN_REGION_SIZE    = 30\n",
    "MIN_REGION_HEIGHT  = 150\n",
    "\n",
    "HEAT_BLUR_KSIZE     = 51\n",
    "RED_SCORE_THRESH    = 220\n",
    "EXCLUDE_TOP_FRAC    = 0.40\n",
    "EXCLUDE_BOTTOM_FRAC = 0.15\n",
    "MIN_DARK_RED_AREA   = 1200\n",
    "TRI_SIZE_PX         = 18\n",
    "PURPLE              = (255, 0, 255)\n",
    "\n",
    "# Purple-triangle extra filter: blob must be ≥ this fraction of total dark area\n",
    "MIN_DARK_FRACTION   = 0.15  # 15%\n",
    "\n",
    "WARMUP_FRAMES       = 10\n",
    "\n",
    "# ─────────────────────────────────────────────── Device / model init ─\n",
    "# YOLO device: CUDA > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    yolo_device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    yolo_device, half = \"mps\", False\n",
    "else:\n",
    "    yolo_device, half = \"cpu\", False\n",
    "\n",
    "# RF-DETR device: prefer MPS if present, else CPU\n",
    "det_device = \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Optional: reduce OpenCV CPU thread contention with PyTorch\n",
    "try:\n",
    "    cv2.setNumThreads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "yolo_model = YOLO(yolo_weights)\n",
    "try: yolo_model.fuse()\n",
    "except Exception: pass\n",
    "\n",
    "jake_model = RFDETRBase(pretrain_weights=jake_weights, num_classes=3)\n",
    "\n",
    "# Warm-up YOLO so first frame isn't laggy\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = yolo_model.predict(_dummy, task=\"segment\", imgsz=IMG_SIZE,\n",
    "                       device=yolo_device, conf=CONF, iou=IOU,\n",
    "                       verbose=False, half=half)\n",
    "\n",
    "# ─────────────────────────────────────── Helper processing functions ─\n",
    "def highlight_rails_mask_only_fast(img_bgr, rail_mask, target_colors_rgb,\n",
    "                                   tolerance=30.0,\n",
    "                                   min_region_size=50,\n",
    "                                   min_region_height=150):\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max()+1\n",
    "    x0, x1 = xs.min(), xs.max()+1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb],\n",
    "                           dtype=np.float32)\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    colour_hit = np.any(dist2 <= tolerance**2, axis=-1)\n",
    "\n",
    "    combined = colour_hit & mask_roi\n",
    "    comp = combined.astype(np.uint8)\n",
    "    n, lbls, stats, _ = cv2.connectedComponentsWithStats(comp, 8)\n",
    "\n",
    "    good = np.zeros_like(combined)\n",
    "    for lbl in range(1, n):\n",
    "        area, h = stats[lbl, cv2.CC_STAT_AREA], stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and h >= min_region_height:\n",
    "            good[lbls == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = good\n",
    "    return filtered_full\n",
    "\n",
    "\n",
    "def red_vs_green_score(red_mask, green_mask, blur_ksize=51):\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    r = cv2.blur(red_mask.astype(np.float32), k)\n",
    "    g = cv2.blur(green_mask.astype(np.float32), k)\n",
    "    diff = r - g\n",
    "    amax = float(np.max(np.abs(diff))) + 1e-6\n",
    "    norm = (diff / (2*amax) + 0.5)\n",
    "    return np.clip(norm * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def draw_triangle(img, x, y, size=TRI_SIZE_PX, colour=PURPLE):\n",
    "    h = int(size * 1.2)\n",
    "    pts = np.array([[x, y], [x-size, y+h], [x+size, y+h]], np.int32)\n",
    "    cv2.fillConvexPoly(img, pts, colour)\n",
    "    cv2.polylines(img, [pts.reshape(-1,1,2)], True, (0,0,0), 1, cv2.LINE_AA)\n",
    "\n",
    "# ─────────────────────────────────────────── Pipeline per frame ─\n",
    "def process_frame(img_bgr):\n",
    "    \"\"\"\n",
    "    Core pipeline used for timing:\n",
    "    • RF-DETR (Jake bbox) once\n",
    "    • YOLO-seg once\n",
    "    • rail green/red + heat computation\n",
    "    • purple triangle logic with 15% total dark-area filter\n",
    "    We do NOT build/display the 3-panel canvas here (no GUI).\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "\n",
    "    # RF-DETR: Jake bbox (PIL once)\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "    try:\n",
    "        dets = jake_model.predict(pil_img, threshold=0.5, device=det_device)[0]\n",
    "    except NotImplementedError:\n",
    "        dets = jake_model.predict(pil_img, threshold=0.5, device=\"cpu\")[0]\n",
    "\n",
    "    # YOLO segmentation ONCE (use original BGR frame)\n",
    "    res = yolo_model.predict(img_bgr, task=\"segment\", imgsz=IMG_SIZE,\n",
    "                             device=yolo_device, conf=CONF, iou=IOU,\n",
    "                             max_det=30, verbose=False, half=half)[0]\n",
    "\n",
    "    if res.masks is None:\n",
    "        return  # nothing detected; timing still counts\n",
    "\n",
    "    masks   = res.masks.data.cpu().numpy()  # (N, h_m, w_m)\n",
    "    classes = res.masks.cls.cpu().numpy() if hasattr(res.masks, \"cls\") \\\n",
    "              else res.boxes.cls.cpu().numpy()\n",
    "    classes = classes.astype(int)\n",
    "    h_m, w_m = masks.shape[1:]\n",
    "\n",
    "    # Rail union\n",
    "    rail_union = np.zeros((h_m, w_m), dtype=bool)\n",
    "    for m, c in zip(masks, classes):\n",
    "        if int(c) == RAIL_ID:\n",
    "            rail_union |= m.astype(bool)\n",
    "    rail_mask = cv2.resize(rail_union.astype(np.uint8), (W, H),\n",
    "                           interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "    # Green/red rails + heat\n",
    "    green = highlight_rails_mask_only_fast(img_bgr, rail_mask,\n",
    "                                           TARGET_COLORS_RGB, TOLERANCE,\n",
    "                                           MIN_REGION_SIZE, MIN_REGION_HEIGHT)\n",
    "    red   = rail_mask & ~green\n",
    "    score = red_vs_green_score(red, green, HEAT_BLUR_KSIZE)\n",
    "\n",
    "    # Exclude top/bottom bands\n",
    "    top_ex = int(H * EXCLUDE_TOP_FRAC)\n",
    "    bot_ex = int(H * EXCLUDE_BOTTOM_FRAC)\n",
    "    score[:top_ex, :] = 0\n",
    "    if bot_ex: score[H-bot_ex:, :] = 0\n",
    "\n",
    "    dark = (score >= RED_SCORE_THRESH).astype(np.uint8)\n",
    "    dark = cv2.morphologyEx(\n",
    "        dark, cv2.MORPH_OPEN,\n",
    "        cv2.getStructuringElement(cv2.MORPH_RECT, (5, 9)),\n",
    "        iterations=1\n",
    "    )\n",
    "\n",
    "    # purple triangles (area checks incl. 15% of total dark area)\n",
    "    total_dark_area  = int(dark.sum())\n",
    "    frac_area_thresh = int(np.ceil(MIN_DARK_FRACTION * total_dark_area))\n",
    "    n_lbl, lbl_mat, stats, _ = cv2.connectedComponentsWithStats(dark, 8)\n",
    "    # (we don't draw here – timing only; this loop is kept for parity with viewer)\n",
    "    for lbl in range(1, n_lbl):\n",
    "        area = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        if area < MIN_DARK_RED_AREA or area < frac_area_thresh:\n",
    "            continue\n",
    "        # would draw triangle in viewer version\n",
    "\n",
    "# ──────────────────────────────────────────────────────── Main test ──\n",
    "if __name__ == \"__main__\":\n",
    "    folder = (Path.home() / \"Documents\" / \"GitHub\" /\n",
    "              \"Ai-plays-SubwaySurfers\" / \"frames\")\n",
    "\n",
    "    if not folder.is_dir():\n",
    "        print(f\"[ERROR] frames folder not found: {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    paths = sorted(\n",
    "        glob.glob(str(folder / \"frame_*.jpg\")) +\n",
    "        glob.glob(str(folder / \"frame_*.png\")) +\n",
    "        glob.glob(str(folder / \"*.jpg\")) +\n",
    "        glob.glob(str(folder / \"*.png\"))\n",
    "    )\n",
    "    if not paths:\n",
    "        print(f\"[ERROR] No frame images found in {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    times = []\n",
    "    t_start = time.perf_counter()\n",
    "\n",
    "    # Run with warmup\n",
    "    for idx, p in enumerate(paths):\n",
    "        frame = cv2.imread(p)\n",
    "        if frame is None:\n",
    "            print(f\"[WARN] unreadable image: {p}\")\n",
    "            continue\n",
    "\n",
    "        if idx < WARMUP_FRAMES:\n",
    "            # Warm-up: execute but don't time or print\n",
    "            process_frame(frame)\n",
    "            continue\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        process_frame(frame)\n",
    "        dt = time.perf_counter() - t0\n",
    "        times.append(dt)\n",
    "\n",
    "        # Per-frame timing print (post-warmup only)\n",
    "        print(f\"{os.path.basename(p):>20s}  {dt*1000:7.2f} ms\")\n",
    "\n",
    "    total_time = time.perf_counter() - t_start\n",
    "    timed_frames = len(times)\n",
    "\n",
    "    # ─────────────────────────────── Summary ─\n",
    "    if timed_frames == 0:\n",
    "        print(\"\\n[WARNING] fewer frames than WARMUP_FRAMES; nothing timed.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    ms = [t * 1_000 for t in times]\n",
    "    avg = statistics.mean(ms)\n",
    "    med = statistics.median(ms)\n",
    "    print(\"\\n───── Speed-test summary (warm-up skipped) ─────\")\n",
    "    print(f\"Total frames           : {len(paths)}\")\n",
    "    print(f\"Warm-up frames ignored : {WARMUP_FRAMES}\")\n",
    "    print(f\"Frames timed           : {timed_frames}\")\n",
    "    print(f\"Total wall-clock time  : {total_time:,.2f} s\")\n",
    "    print(f\"Average / frame        : {avg:,.2f} ms  ({1_000/avg:,.2f} FPS)\")\n",
    "    print(f\"Median                 : {med:,.2f} ms\")\n",
    "    print(f\"Fastest                : {min(ms):,.2f} ms\")\n",
    "    print(f\"Slowest                : {max(ms):,.2f} ms\")\n",
    "    print(\"───────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b047eea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#More gassss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76347d62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n",
      "Loading pretrain weights\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model is not optimized for inference. Latency may be higher than expected. You can optimize the model for inference by calling model.optimize_for_inference().\n",
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:4316.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     frame_00005.png   315.91 ms\n",
      "     frame_00005.png   385.31 ms\n",
      "     frame_00006.png   317.76 ms\n",
      "     frame_00006.png   277.83 ms\n",
      "     frame_00007.png   334.29 ms\n",
      "     frame_00007.png   298.99 ms\n",
      "     frame_00008.png   329.43 ms\n",
      "     frame_00008.png   290.31 ms\n",
      "     frame_00009.png   340.79 ms\n",
      "     frame_00009.png   301.95 ms\n",
      "     frame_00010.png   322.35 ms\n",
      "     frame_00010.png   288.72 ms\n",
      "     frame_00011.png   258.81 ms\n",
      "     frame_00011.png   258.52 ms\n",
      "     frame_00012.png   457.53 ms\n",
      "     frame_00012.png   249.35 ms\n",
      "     frame_00013.png   315.67 ms\n",
      "     frame_00013.png   305.48 ms\n",
      "     frame_00014.png   325.71 ms\n",
      "     frame_00014.png   300.69 ms\n",
      "     frame_00015.png   298.78 ms\n",
      "     frame_00015.png   256.90 ms\n",
      "     frame_00016.png   353.92 ms\n",
      "     frame_00016.png   317.43 ms\n",
      "     frame_00017.png   314.21 ms\n",
      "     frame_00017.png   316.09 ms\n",
      "     frame_00018.png   334.27 ms\n",
      "     frame_00018.png   296.30 ms\n",
      "     frame_00019.png   285.73 ms\n",
      "     frame_00019.png   289.75 ms\n",
      "     frame_00020.png   277.62 ms\n",
      "     frame_00020.png   279.73 ms\n",
      "     frame_00021.png   327.47 ms\n",
      "     frame_00021.png   298.66 ms\n",
      "     frame_00022.png   323.96 ms\n",
      "     frame_00022.png   292.91 ms\n",
      "     frame_00023.png   397.32 ms\n",
      "     frame_00023.png   298.26 ms\n",
      "     frame_00024.png   328.34 ms\n",
      "     frame_00024.png   299.47 ms\n",
      "     frame_00025.png   287.05 ms\n",
      "     frame_00025.png   275.54 ms\n",
      "     frame_00026.png   269.05 ms\n",
      "     frame_00026.png   276.86 ms\n",
      "     frame_00027.png   298.24 ms\n",
      "     frame_00027.png   296.74 ms\n",
      "     frame_00028.png   286.92 ms\n",
      "     frame_00028.png   288.42 ms\n",
      "     frame_00029.png   285.03 ms\n",
      "     frame_00029.png   273.56 ms\n",
      "     frame_00030.png   305.47 ms\n",
      "     frame_00030.png   303.38 ms\n",
      "     frame_00031.png   305.64 ms\n",
      "     frame_00031.png   318.92 ms\n",
      "     frame_00032.png   294.73 ms\n",
      "     frame_00032.png   295.65 ms\n",
      "     frame_00033.png   288.64 ms\n",
      "     frame_00033.png   252.14 ms\n",
      "     frame_00034.png   345.10 ms\n",
      "     frame_00034.png   311.69 ms\n",
      "     frame_00035.png   393.85 ms\n",
      "     frame_00035.png   259.62 ms\n",
      "     frame_00036.png   296.72 ms\n",
      "     frame_00036.png   294.54 ms\n",
      "     frame_00037.png   331.02 ms\n",
      "     frame_00037.png   303.04 ms\n",
      "     frame_00038.png   306.71 ms\n",
      "     frame_00038.png   305.34 ms\n",
      "     frame_00039.png   191.75 ms\n",
      "     frame_00039.png   201.60 ms\n",
      "     frame_00040.png   261.59 ms\n",
      "     frame_00040.png   250.06 ms\n",
      "     frame_00041.png   354.88 ms\n",
      "     frame_00041.png   268.37 ms\n",
      "     frame_00042.png   248.70 ms\n",
      "     frame_00042.png   259.63 ms\n",
      "     frame_00043.png   361.23 ms\n",
      "     frame_00043.png   256.25 ms\n",
      "     frame_00044.png   269.90 ms\n",
      "     frame_00044.png   238.65 ms\n",
      "     frame_00045.png   282.62 ms\n",
      "     frame_00045.png   270.94 ms\n",
      "     frame_00046.png   288.10 ms\n",
      "     frame_00046.png   246.80 ms\n",
      "     frame_00047.png   269.40 ms\n",
      "     frame_00047.png   284.14 ms\n",
      "     frame_00048.png   264.36 ms\n",
      "     frame_00048.png   282.43 ms\n",
      "     frame_00049.png   326.21 ms\n",
      "     frame_00049.png   291.11 ms\n",
      "     frame_00050.png   302.15 ms\n",
      "     frame_00050.png   296.33 ms\n",
      "     frame_00051.png   271.61 ms\n",
      "     frame_00051.png   306.52 ms\n",
      "     frame_00052.png   337.79 ms\n",
      "     frame_00052.png   285.24 ms\n",
      "     frame_00053.png   281.15 ms\n",
      "     frame_00053.png   299.65 ms\n",
      "     frame_00054.png   332.31 ms\n",
      "     frame_00054.png   305.62 ms\n",
      "     frame_00055.png   336.51 ms\n",
      "     frame_00055.png   308.68 ms\n",
      "     frame_00056.png   339.48 ms\n",
      "     frame_00056.png   297.33 ms\n",
      "     frame_00057.png   278.82 ms\n",
      "     frame_00057.png   273.80 ms\n",
      "     frame_00058.png   341.00 ms\n",
      "     frame_00058.png   297.00 ms\n",
      "     frame_00059.png   313.31 ms\n",
      "     frame_00059.png   315.90 ms\n",
      "     frame_00060.png   340.36 ms\n",
      "     frame_00060.png   303.14 ms\n",
      "     frame_00061.png   317.74 ms\n",
      "     frame_00061.png   300.71 ms\n",
      "     frame_00062.png   184.88 ms\n",
      "     frame_00062.png   203.37 ms\n",
      "     frame_00063.png   227.58 ms\n",
      "     frame_00063.png   198.43 ms\n",
      "     frame_00064.png   283.50 ms\n",
      "     frame_00064.png   275.98 ms\n",
      "     frame_00065.png   296.46 ms\n",
      "     frame_00065.png   299.01 ms\n",
      "     frame_00066.png   290.18 ms\n",
      "     frame_00066.png   288.63 ms\n",
      "     frame_00067.png   223.49 ms\n",
      "     frame_00067.png   190.34 ms\n",
      "     frame_00068.png   264.11 ms\n",
      "     frame_00068.png   251.47 ms\n",
      "     frame_00069.png   276.02 ms\n",
      "     frame_00069.png   280.99 ms\n",
      "     frame_00070.png   316.28 ms\n",
      "     frame_00070.png   281.77 ms\n",
      "     frame_00071.png   301.92 ms\n",
      "     frame_00071.png   328.70 ms\n",
      "     frame_00072.png   589.53 ms\n",
      "     frame_00072.png   347.90 ms\n",
      "     frame_00073.png   415.16 ms\n",
      "     frame_00073.png   323.28 ms\n",
      "     frame_00074.png   352.49 ms\n",
      "     frame_00074.png   329.28 ms\n",
      "     frame_00075.png   261.26 ms\n",
      "     frame_00075.png   308.36 ms\n",
      "     frame_00076.png   193.24 ms\n",
      "     frame_00076.png   187.58 ms\n",
      "     frame_00077.png   186.93 ms\n",
      "     frame_00077.png   180.32 ms\n",
      "     frame_00078.png   195.16 ms\n",
      "     frame_00078.png   293.28 ms\n",
      "\n",
      "───── Speed-test summary (warm-up skipped) ─────\n",
      "Total frames           : 158\n",
      "Warm-up frames ignored : 10\n",
      "Frames timed           : 148\n",
      "Total wall-clock time  : 52.69 s\n",
      "Average / frame        : 294.53 ms  (3.40 FPS)\n",
      "Median                 : 296.40 ms\n",
      "Fastest                : 180.32 ms\n",
      "Slowest                : 589.53 ms\n",
      "───────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Speed-test the integrated RF-DETR (Jake bbox) + YOLO-seg pipeline.\n",
    "\n",
    "• Same processing logic as the interactive viewer, but with NO GUI.\n",
    "• Scans:  ~/Documents/GitHub/Ai-plays-SubwaySurfers/frames\n",
    "• First 10 frames are WARMUP (not timed).\n",
    "• Prints per-frame time (after warmup) and a summary at the end.\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, glob, time, statistics\n",
    "# ── Enable CPU fallback before importing torch (for MPS gaps) ────────────────\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "from ultralytics import YOLO\n",
    "from rfdetr import RFDETRBase\n",
    "\n",
    "# ── Monkey-patch interpolate to disable antialias (avoids unsupported MPS op) ─\n",
    "_original_interpolate = F.interpolate\n",
    "def _interpolate_no_antialias(input, *args, **kwargs):\n",
    "    kwargs['antialias'] = False\n",
    "    return _original_interpolate(input, *args, **kwargs)\n",
    "F.interpolate = _interpolate_no_antialias\n",
    "\n",
    "# ─────────────────────────────────────────────────────────── Config ──\n",
    "home     = os.path.expanduser(\"~\")\n",
    "yolo_weights = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "jake_weights = f\"{home}/downloads/weightsjake.pt\"\n",
    "\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40  # (kept for parity; no GUI draw here)\n",
    "\n",
    "TARGET_COLORS_RGB  = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE          = 20.0\n",
    "MIN_REGION_SIZE    = 30\n",
    "MIN_REGION_HEIGHT  = 150\n",
    "\n",
    "HEAT_BLUR_KSIZE     = 51\n",
    "RED_SCORE_THRESH    = 220\n",
    "EXCLUDE_TOP_FRAC    = 0.40\n",
    "EXCLUDE_BOTTOM_FRAC = 0.15\n",
    "MIN_DARK_RED_AREA   = 1200\n",
    "TRI_SIZE_PX         = 18\n",
    "PURPLE              = (255, 0, 255)\n",
    "\n",
    "# Purple-triangle extra filter: blob must be ≥ this fraction of total dark area\n",
    "MIN_DARK_FRACTION   = 0.15  # 15%\n",
    "\n",
    "WARMUP_FRAMES       = 10\n",
    "\n",
    "# ─────────────────────────────────────────────── Device / model init ─\n",
    "# YOLO device: CUDA > MPS > CPU\n",
    "if torch.cuda.is_available():\n",
    "    yolo_device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    yolo_device, half = \"mps\", False\n",
    "else:\n",
    "    yolo_device, half = \"cpu\", False\n",
    "\n",
    "# RF-DETR device: prefer MPS if present, else CPU\n",
    "det_device = \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# Optional: reduce OpenCV CPU thread contention with PyTorch\n",
    "try:\n",
    "    cv2.setNumThreads(1)\n",
    "except Exception:\n",
    "    pass\n",
    "cv2.setUseOptimized(True)\n",
    "\n",
    "# ── Load models once ────────────────────────────────────────────────────────\n",
    "yolo_model = YOLO(yolo_weights)\n",
    "try:\n",
    "    yolo_model.fuse()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "jake_model = RFDETRBase(pretrain_weights=jake_weights, num_classes=3)\n",
    "\n",
    "# ── Precompute constants used every frame ───────────────────────────────────\n",
    "# target colors -> BGR float32, tolerance^2\n",
    "_TARGETS_BGR_F32 = np.array([(r, g, b)[::-1] for (r, g, b) in TARGET_COLORS_RGB], dtype=np.float32)\n",
    "_TOL2 = float(TOLERANCE * TOLERANCE)\n",
    "# morphology kernel reused\n",
    "_MORPH_KERNEL = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 9))\n",
    "\n",
    "# ── Warm-up both models so first timed frame isn't polluted ─────────────────\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "with torch.inference_mode():\n",
    "    _ = yolo_model.predict(\n",
    "        _dummy, task=\"segment\", imgsz=IMG_SIZE,\n",
    "        device=yolo_device, conf=CONF, iou=IOU,\n",
    "        verbose=False, half=half\n",
    "    )\n",
    "# RF-DETR expects PIL; use tiny gray image to compile weights/graphs\n",
    "try:\n",
    "    _pil_dummy = Image.fromarray(np.zeros((IMG_SIZE, IMG_SIZE, 3), dtype=np.uint8))\n",
    "    _ = jake_model.predict(_pil_dummy, threshold=0.5, device=det_device)\n",
    "except NotImplementedError:\n",
    "    _ = jake_model.predict(_pil_dummy, threshold=0.5, device=\"cpu\")\n",
    "\n",
    "# ─────────────────────────────────────── Helper processing functions ─\n",
    "def highlight_rails_mask_only_fast(img_bgr, rail_mask):\n",
    "    \"\"\"\n",
    "    Color + size filter inside rail_mask.\n",
    "    Uses precomputed _TARGETS_BGR_F32 and _TOL2, avoids Python loops.\n",
    "    \"\"\"\n",
    "    # Quick reject\n",
    "    if rail_mask is None or not rail_mask.any():\n",
    "        return np.zeros(rail_mask.shape, dtype=bool)\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    # Compute min color distance to any target in a vectorized way\n",
    "    img_f = img_roi.astype(np.float32, copy=False)\n",
    "    diff  = img_f[:, :, None, :] - _TARGETS_BGR_F32[None, None, :, :]\n",
    "    dist2 = np.einsum(\"...c,...c->...\", diff, diff)  # sum over BGR\n",
    "    # dist2 shape: (h, w, n_targets) -> min across targets\n",
    "    if dist2.ndim == 3:\n",
    "        min_dist2 = dist2.min(axis=2)\n",
    "    else:\n",
    "        min_dist2 = dist2  # just in case\n",
    "\n",
    "    colour_hit = (min_dist2 <= _TOL2)\n",
    "    combined = colour_hit & mask_roi\n",
    "\n",
    "    if not combined.any():\n",
    "        out = np.zeros((H, W), dtype=bool)\n",
    "        return out\n",
    "\n",
    "    comp = combined.view(np.uint8)\n",
    "    n, lbls, stats, _ = cv2.connectedComponentsWithStats(comp, 8)\n",
    "\n",
    "    if n <= 1:\n",
    "        out = np.zeros((H, W), dtype=bool)\n",
    "        return out\n",
    "\n",
    "    good = np.zeros_like(comp, dtype=np.uint8)\n",
    "    # Vectorized filter for components (skip label 0)\n",
    "    areas  = stats[1:, cv2.CC_STAT_AREA]\n",
    "    hs     = stats[1:, cv2.CC_STAT_HEIGHT]\n",
    "    keep   = (areas >= MIN_REGION_SIZE) & (hs >= MIN_REGION_HEIGHT)\n",
    "    if not keep.any():\n",
    "        out = np.zeros((H, W), dtype=bool)\n",
    "        return out\n",
    "\n",
    "    # Paint kept labels\n",
    "    kept_labels = (np.where(keep)[0] + 1).tolist()\n",
    "    for lbl in kept_labels:\n",
    "        good[lbls == lbl] = 1\n",
    "\n",
    "    out = np.zeros((H, W), dtype=bool)\n",
    "    out[y0:y1, x0:x1] = good.astype(bool)\n",
    "    return out\n",
    "\n",
    "\n",
    "def red_vs_green_score(red_mask, green_mask):\n",
    "    # cv2.blur is an optimized box filter; keep it for large kernels\n",
    "    k = (HEAT_BLUR_KSIZE, HEAT_BLUR_KSIZE)\n",
    "    r = cv2.blur(red_mask.astype(np.float32, copy=False), k)\n",
    "    g = cv2.blur(green_mask.astype(np.float32, copy=False), k)\n",
    "    diff = r - g\n",
    "    # Normalize to 0..255 without repeated max calls\n",
    "    amax = float(np.max(diff)) if np.max(diff) > -np.min(diff) else float(-np.min(diff))\n",
    "    amax = amax + 1e-6\n",
    "    norm = (diff / (2 * amax) + 0.5)\n",
    "    return np.clip(norm * 255.0, 0.0, 255.0).astype(np.uint8)\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def yolo_seg_once(img_bgr):\n",
    "    return yolo_model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE,\n",
    "        device=yolo_device, conf=CONF, iou=IOU,\n",
    "        max_det=30, verbose=False, half=half\n",
    "    )[0]\n",
    "\n",
    "\n",
    "def process_frame(img_bgr):\n",
    "    \"\"\"\n",
    "    Core pipeline used for timing:\n",
    "    • RF-DETR (Jake bbox) once (kept for parity)\n",
    "    • YOLO-seg once\n",
    "    • rail green/red + heat computation\n",
    "    • purple triangle logic with 15% total dark-area filter\n",
    "    No GUI work here.\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "\n",
    "    # RF-DETR: Jake bbox (PIL). We call it to mirror the viewer cost,\n",
    "    # but don't consume results here.\n",
    "    pil_img = Image.fromarray(cv2.cvtColor(img_bgr, cv2.COLOR_BGR2RGB))\n",
    "    try:\n",
    "        _ = jake_model.predict(pil_img, threshold=0.5, device=det_device)[0]\n",
    "    except NotImplementedError:\n",
    "        _ = jake_model.predict(pil_img, threshold=0.5, device=\"cpu\")[0]\n",
    "\n",
    "    # YOLO segmentation ONCE\n",
    "    res = yolo_seg_once(img_bgr)\n",
    "    masks_obj = res.masks\n",
    "    if masks_obj is None:\n",
    "        return  # nothing detected; timing still counts\n",
    "\n",
    "    # Prefer boxes.cls (always present); fall back if needed\n",
    "    try:\n",
    "        classes = res.boxes.cls.detach().cpu().numpy().astype(int)\n",
    "    except Exception:\n",
    "        classes = (masks_obj.cls.detach().cpu().numpy().astype(int)\n",
    "                   if hasattr(masks_obj, \"cls\") else None)\n",
    "\n",
    "    masks = masks_obj.data.detach().cpu().numpy().astype(bool)  # (N, h_m, w_m)\n",
    "    if classes is None or masks.size == 0:\n",
    "        return\n",
    "\n",
    "    # Rail union without Python loop\n",
    "    rail_idx = (classes == RAIL_ID)\n",
    "    if not rail_idx.any():\n",
    "        return\n",
    "    rail_union_small = np.any(masks[rail_idx, :, :], axis=0)\n",
    "    rail_mask = cv2.resize(rail_union_small.astype(np.uint8), (W, H),\n",
    "                           interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "    if not rail_mask.any():\n",
    "        return\n",
    "\n",
    "    # Green/red rails + heat\n",
    "    green = highlight_rails_mask_only_fast(img_bgr, rail_mask)\n",
    "    red   = rail_mask & ~green\n",
    "\n",
    "    if not red.any() and not green.any():\n",
    "        return\n",
    "\n",
    "    score = red_vs_green_score(red, green)\n",
    "\n",
    "    # Exclude top/bottom bands (index math only, no copies)\n",
    "    top_ex = int(H * EXCLUDE_TOP_FRAC)\n",
    "    bot_ex = int(H * EXCLUDE_BOTTOM_FRAC)\n",
    "    if top_ex > 0:\n",
    "        score[:top_ex, :] = 0\n",
    "    if bot_ex > 0:\n",
    "        score[H - bot_ex:, :] = 0\n",
    "\n",
    "    # Threshold to \"dark\" and do a single morphology open\n",
    "    # (branchless threshold via cv2 works fine; >= RED_SCORE_THRESH)\n",
    "    _, dark = cv2.threshold(score, RED_SCORE_THRESH - 1, 255, cv2.THRESH_BINARY)\n",
    "    if not np.any(dark):\n",
    "        return\n",
    "    dark = cv2.morphologyEx(dark, cv2.MORPH_OPEN, _MORPH_KERNEL, iterations=1)\n",
    "\n",
    "    # purple triangles (area checks incl. 15% of total dark area)\n",
    "    total_dark_area = int(cv2.countNonZero(dark))\n",
    "    if total_dark_area == 0:\n",
    "        return\n",
    "\n",
    "    frac_area_thresh = int(np.ceil(MIN_DARK_FRACTION * total_dark_area))\n",
    "    n_lbl, lbl_mat, stats, _ = cv2.connectedComponentsWithStats(dark, 8)\n",
    "    if n_lbl <= 1:\n",
    "        return\n",
    "\n",
    "    # Filter blobs via vector ops; we don't draw in this headless speed test\n",
    "    areas = stats[1:, cv2.CC_STAT_AREA]\n",
    "    keep  = (areas >= MIN_DARK_RED_AREA) & (areas >= frac_area_thresh)\n",
    "    # If needed, we could short-circuit on keep.any(), but either way we’re done.\n",
    "\n",
    "\n",
    "# ──────────────────────────────────────────────────────── Main test ──\n",
    "if __name__ == \"__main__\":\n",
    "    folder = (Path.home() / \"Documents\" / \"GitHub\" /\n",
    "              \"Ai-plays-SubwaySurfers\" / \"frames\")\n",
    "\n",
    "    if not folder.is_dir():\n",
    "        print(f\"[ERROR] frames folder not found: {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    # Prefer the \"frame_*.{jpg,png}\" pattern first; fallback to all images\n",
    "    paths = sorted(\n",
    "        glob.glob(str(folder / \"frame_*.jpg\"))\n",
    "        + glob.glob(str(folder / \"frame_*.png\"))\n",
    "        + glob.glob(str(folder / \"*.jpg\"))\n",
    "        + glob.glob(str(folder / \"*.png\"))\n",
    "    )\n",
    "    if not paths:\n",
    "        print(f\"[ERROR] No frame images found in {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    times = []\n",
    "    t_start = time.perf_counter()\n",
    "\n",
    "    # Run with warmup\n",
    "    for idx, p in enumerate(paths):\n",
    "        frame = cv2.imread(p, cv2.IMREAD_COLOR)\n",
    "        if frame is None:\n",
    "            print(f\"[WARN] unreadable image: {p}\")\n",
    "            continue\n",
    "\n",
    "        if idx < WARMUP_FRAMES:\n",
    "            process_frame(frame)\n",
    "            continue\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        process_frame(frame)\n",
    "        dt = time.perf_counter() - t0\n",
    "        times.append(dt)\n",
    "\n",
    "        # Per-frame timing print (post-warmup only)\n",
    "        print(f\"{os.path.basename(p):>20s}  {dt*1000:7.2f} ms\")\n",
    "\n",
    "    total_time = time.perf_counter() - t_start\n",
    "    timed_frames = len(times)\n",
    "\n",
    "    # ─────────────────────────────── Summary ─\n",
    "    if timed_frames == 0:\n",
    "        print(\"\\n[WARNING] fewer frames than WARMUP_FRAMES; nothing timed.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    ms = [t * 1_000 for t in times]\n",
    "    avg = statistics.mean(ms)\n",
    "    med = statistics.median(ms)\n",
    "    print(\"\\n───── Speed-test summary (warm-up skipped) ─────\")\n",
    "    print(f\"Total frames           : {len(paths)}\")\n",
    "    print(f\"Warm-up frames ignored : {WARMUP_FRAMES}\")\n",
    "    print(f\"Frames timed           : {timed_frames}\")\n",
    "    print(f\"Total wall-clock time  : {total_time:,.2f} s\")\n",
    "    print(f\"Average / frame        : {avg:,.2f} ms  ({1_000/avg:,.2f} FPS)\")\n",
    "    print(f\"Median                 : {med:,.2f} ms\")\n",
    "    print(f\"Fastest                : {min(ms):,.2f} ms\")\n",
    "    print(f\"Slowest                : {max(ms):,.2f} ms\")\n",
    "    print(\"───────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3a1bd42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Try to spread load of CPU and GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b12821d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n",
      "0.1188535001128912\n",
      "0.11889345804229379\n",
      "0.13747945800423622\n",
      "0.10379941714927554\n",
      "0.32318695820868015\n",
      "0.1396821248345077\n",
      "0.1398801659233868\n",
      "0.13820337504148483\n",
      "0.14115808298811316\n",
      "0.15825037518516183\n",
      "0.1237097498960793\n",
      "0.11256545828655362\n",
      "0.08685799967497587\n",
      "0.08897387469187379\n",
      "0.07186379190534353\n",
      "0.05843866616487503\n",
      "0.13990500010550022\n",
      "0.13132099993526936\n",
      "0.17975362529978156\n",
      "0.132533208001405\n",
      "0.1258506248705089\n",
      "0.11383724957704544\n",
      "0.1652112090960145\n",
      "0.1324077919125557\n",
      "0.14259725017473102\n",
      "0.6064161667600274\n",
      "0.13319541607052088\n",
      "0.15102970879524946\n",
      "0.23040816700085998\n",
      "0.535348791629076\n",
      "0.2573886248283088\n",
      "0.2950752913020551\n",
      "0.26299329195171595\n",
      "0.16859670775011182\n",
      "0.19214670918881893\n",
      "0.16971654072403908\n",
      "0.1831158329732716\n",
      "0.19154249969869852\n",
      "0.1816325830295682\n",
      "0.15910133300349116\n",
      "0.11648570885881782\n",
      "0.113538040779531\n",
      "0.11150704231113195\n",
      "0.10165433399379253\n",
      "0.14349041692912579\n",
      "0.1784573751501739\n",
      "0.11577291693538427\n",
      "0.11910324962809682\n",
      "0.12564041605219245\n",
      "0.10537316696718335\n",
      "0.10609808377921581\n",
      "0.10226729093119502\n",
      "0.15330808330327272\n",
      "0.1465577082708478\n",
      "0.12326375022530556\n",
      "0.1181462500244379\n",
      "0.08906170912086964\n",
      "0.08258874993771315\n",
      "0.1430730833671987\n",
      "0.1367954588495195\n",
      "0.11464137490838766\n",
      "0.10660608299076557\n",
      "0.15893075009807944\n",
      "0.13571137515828013\n",
      "0.15676241600885987\n",
      "0.13460541609674692\n",
      "0.13480429211631417\n",
      "0.12653533415868878\n",
      "0.055662957951426506\n",
      "0.045045957900583744\n",
      "0.09463862469419837\n",
      "0.09422695776447654\n",
      "0.09429137501865625\n",
      "0.08796879136934876\n",
      "0.08290599985048175\n",
      "0.08220941666513681\n",
      "0.09154374990612268\n",
      "0.07572012487798929\n",
      "0.07564804097637534\n",
      "0.0865311250090599\n",
      "0.11848637508228421\n",
      "0.11864870833232999\n",
      "0.08940212475135922\n",
      "0.07894612476229668\n",
      "0.11036641616374254\n",
      "0.10020629223436117\n",
      "0.10616758279502392\n",
      "0.10437474958598614\n",
      "0.14646824961528182\n",
      "0.132042083889246\n",
      "0.17052566725760698\n",
      "0.12248058384284377\n",
      "0.0874556670896709\n",
      "0.07434229180216789\n",
      "0.12261924985796213\n",
      "0.12591587472707033\n",
      "0.14421099983155727\n",
      "0.356674624606967\n",
      "0.17682704236358404\n",
      "0.1717393328435719\n",
      "0.18792870827019215\n",
      "0.14808091707527637\n",
      "0.1591664170846343\n",
      "0.16227054176852107\n",
      "0.11908275028690696\n",
      "0.11650620866566896\n",
      "0.15331474971026182\n",
      "0.14374474994838238\n",
      "0.15185724990442395\n",
      "0.15380429197102785\n",
      "0.137471083085984\n",
      "0.13322987500578165\n",
      "0.13848158298060298\n",
      "0.1473289579153061\n",
      "0.07098429184406996\n",
      "0.05735854059457779\n",
      "0.07274600025266409\n",
      "0.05209095776081085\n",
      "0.13426862517371774\n",
      "0.11601095786318183\n",
      "0.16199604189023376\n",
      "0.13836220791563392\n",
      "0.1413647923618555\n",
      "0.1325854999013245\n",
      "0.05742170801386237\n",
      "0.051773208659142256\n",
      "0.12251745769754052\n",
      "0.09785654209554195\n",
      "0.2567171659320593\n",
      "0.1265672082081437\n",
      "0.16979841608554125\n",
      "0.13391933403909206\n",
      "0.1396333328448236\n",
      "0.1337704169563949\n",
      "0.22322704177349806\n",
      "0.13799470802769065\n",
      "0.15888045774772763\n",
      "0.12895866576582193\n",
      "0.149747125338763\n",
      "0.13033587485551834\n",
      "0.09199800016358495\n",
      "0.07703249994665384\n",
      "0.021408540662378073\n",
      "0.019631875213235617\n",
      "0.019544291775673628\n",
      "0.019409167114645243\n",
      "0.018811125308275223\n",
      "0.018015417270362377\n",
      "\n",
      "───── Speed-test summary (warm-up skipped) ─────\n",
      "Total frames           : 158\n",
      "Warm-up frames ignored : 10\n",
      "Frames timed           : 148\n",
      "Total wall-clock time  : 26.36 s\n",
      "Average / frame        : 133.80 ms  (7.47 FPS)\n",
      "Median                 : 129.65 ms\n",
      "Fastest                : 18.01 ms\n",
      "Slowest                : 606.42 ms\n",
      "────────────────────────────────────────────────\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Speed-test: run the full segmentation/overlay pipeline on every image in\n",
    "~/Documents/GitHub/Ai-plays-SubwaySurfers/frames and print a timing\n",
    "summary at the end.\n",
    "\n",
    "Changes in this version\n",
    "────────────────────────\n",
    "• The first WARMUP_FRAMES (default 10) are *executed* but **not timed**,\n",
    "  so model/kernel warm-up and disk cache effects don’t skew results.\n",
    "• Everything else is unchanged: no GUI overhead, concise summary.\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, glob, time, statistics\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# ───────────────────────────────────────────────────────── Config ──\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40\n",
    "\n",
    "TARGET_COLORS_RGB  = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE          = 20.0\n",
    "MIN_REGION_SIZE    = 30\n",
    "MIN_REGION_HEIGHT  = 150\n",
    "\n",
    "HEAT_BLUR_KSIZE     = 51\n",
    "RED_SCORE_THRESH    = 220\n",
    "EXCLUDE_TOP_FRAC    = 0.40\n",
    "EXCLUDE_BOTTOM_FRAC = 0.15\n",
    "MIN_DARK_RED_AREA   = 1200\n",
    "TRI_SIZE_PX         = 18\n",
    "PURPLE              = (255, 0, 255)\n",
    "\n",
    "WARMUP_FRAMES       = 10   # how many frames to skip from timing stats\n",
    "\n",
    "obstacle_classes = {\n",
    "    0: \"BOOTS\", 1: \"GREYTRAIN\", 2: \"HIGHBARRIER1\", 3: \"JUMP\", 4: \"LOWBARRIER1\",\n",
    "    5: \"LOWBARRIER2\", 6: \"ORANGETRAIN\", 7: \"PILLAR\", 8: \"RAMP\", 9: \"RAILS\",\n",
    "    10: \"SIDEWALK\", 11: \"YELLOWTRAIN\"\n",
    "}\n",
    "CLASS_COLOURS = {\n",
    "    0: (255,255,0), 1: (192,192,192), 2: (0,128,255), 3: (0,255,0),\n",
    "    4: (255,0,255), 5: (0,255,255), 6: (255,128,0), 7: (128,0,255),\n",
    "    8: (0,0,128), 10: (128,128,0), 11: (255,255,102)\n",
    "}\n",
    "\n",
    "# ─────────────────────────────────────── Device / model init ─\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "model = YOLO(weights)\n",
    "try: model.fuse()\n",
    "except Exception: pass\n",
    "model.predict(\n",
    "    np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8),\n",
    "    task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, verbose=False, half=half\n",
    ")\n",
    "\n",
    "# ───────────────────────────── Helper processing functions ─\n",
    "def highlight_rails_mask_only_fast(img_bgr, rail_mask, target_colors_rgb,\n",
    "                                   tolerance=30.0,\n",
    "                                   min_region_size=50,\n",
    "                                   min_region_height=150):\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max()+1\n",
    "    x0, x1 = xs.min(), xs.max()+1\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb],\n",
    "                           dtype=np.float32)\n",
    "    diff  = img_roi.astype(np.float32)[:, :, None, :] - targets_bgr[None, None]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    colour_hit = np.any(dist2 <= tolerance**2, axis=-1)\n",
    "\n",
    "    combined = colour_hit & mask_roi\n",
    "    comp = combined.astype(np.uint8)\n",
    "    n, lbls, stats, _ = cv2.connectedComponentsWithStats(comp, 8)\n",
    "\n",
    "    good = np.zeros_like(combined)\n",
    "    for lbl in range(1, n):\n",
    "        area, h = stats[lbl, cv2.CC_STAT_AREA], stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and h >= min_region_height:\n",
    "            good[lbls == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = good\n",
    "    return filtered_full\n",
    "\n",
    "\n",
    "def red_vs_green_score(red_mask, green_mask, blur_ksize=51):\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    diff = (cv2.blur(red_mask.astype(np.float32), k) -\n",
    "            cv2.blur(green_mask.astype(np.float32), k))\n",
    "    norm = (diff / (2*(np.max(np.abs(diff))+1e-6)) + 0.5)\n",
    "    return np.clip(norm * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def process_frame(img_bgr):\n",
    "    res = model.predict(img_bgr, task=\"segment\", imgsz=IMG_SIZE,\n",
    "                        device=device, conf=CONF, iou=IOU,\n",
    "                        max_det=30, verbose=False, half=half)[0]\n",
    "    if res.masks is None:\n",
    "        return\n",
    "\n",
    "    masks   = res.masks.data.cpu().numpy()\n",
    "    classes = res.masks.cls.cpu().numpy() if hasattr(res.masks, \"cls\") \\\n",
    "              else res.boxes.cls.cpu().numpy()\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    rail_union = np.zeros(masks.shape[1:], bool)\n",
    "    for m, c in zip(masks, classes):\n",
    "        if int(c) == RAIL_ID:\n",
    "            rail_union |= m.astype(bool)\n",
    "\n",
    "    rail_mask = cv2.resize(rail_union.astype(np.uint8), (W, H),\n",
    "                           interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "    green = highlight_rails_mask_only_fast(img_bgr, rail_mask,\n",
    "                                           TARGET_COLORS_RGB, TOLERANCE,\n",
    "                                           MIN_REGION_SIZE, MIN_REGION_HEIGHT)\n",
    "    red   = rail_mask & ~green\n",
    "    score = red_vs_green_score(red, green, HEAT_BLUR_KSIZE)\n",
    "\n",
    "    # exclude top/bottom\n",
    "    score[:int(H*EXCLUDE_TOP_FRAC), :] = 0\n",
    "    score[H-int(H*EXCLUDE_BOTTOM_FRAC):, :] = 0\n",
    "\n",
    "    dark = (score >= RED_SCORE_THRESH).astype(np.uint8)\n",
    "    cv2.morphologyEx(\n",
    "        dark, cv2.MORPH_OPEN,\n",
    "        cv2.getStructuringElement(cv2.MORPH_RECT, (5, 9)),\n",
    "        dst=dark, iterations=1\n",
    "    )\n",
    "    # outputs are not displayed; function exists for timing only.\n",
    "\n",
    "# ─────────────────────────────────────────────────────── Main test ──\n",
    "if __name__ == \"__main__\":\n",
    "    folder = (Path.home() / \"Documents\" / \"GitHub\" /\n",
    "              \"Ai-plays-SubwaySurfers\" / \"frames\")\n",
    "\n",
    "    if not folder.is_dir():\n",
    "        print(f\"[ERROR] frames folder not found: {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    paths = sorted(\n",
    "        glob.glob(str(folder / \"frame_*.jpg\")) +\n",
    "        glob.glob(str(folder / \"frame_*.png\")) +\n",
    "        glob.glob(str(folder / \"*.jpg\")) +\n",
    "        glob.glob(str(folder / \"*.png\"))\n",
    "    )\n",
    "    if not paths:\n",
    "        print(f\"[ERROR] No frame images found in {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    times = []\n",
    "    t_start = time.perf_counter()\n",
    "\n",
    "    for idx, p in enumerate(paths):\n",
    "        frame = cv2.imread(p)\n",
    "\n",
    "        if idx < WARMUP_FRAMES:\n",
    "            # Warm-up: run but don't time\n",
    "            process_frame(frame)\n",
    "            continue\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        process_frame(frame)\n",
    "        times.append(time.perf_counter() - t0)\n",
    "        print(time.perf_counter() - t0)\n",
    "\n",
    "    total_time = time.perf_counter() - t_start\n",
    "    timed_frames = len(times)\n",
    "\n",
    "    # ─────────────────────────────── Summary ─\n",
    "    ms = [t * 1_000 for t in times]\n",
    "    if timed_frames == 0:\n",
    "        print(\"\\n[WARNING] fewer frames than WARMUP_FRAMES; nothing timed.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    print(\"\\n───── Speed-test summary (warm-up skipped) ─────\")\n",
    "    print(f\"Total frames           : {len(paths)}\")\n",
    "    print(f\"Warm-up frames ignored : {WARMUP_FRAMES}\")\n",
    "    print(f\"Frames timed           : {timed_frames}\")\n",
    "    print(f\"Total wall-clock time  : {total_time:,.2f} s\")\n",
    "    print(f\"Average / frame        : {statistics.mean(ms):,.2f} ms\"\n",
    "          f\"  ({1_000/statistics.mean(ms):,.2f} FPS)\")\n",
    "    print(f\"Median                 : {statistics.median(ms):,.2f} ms\")\n",
    "    print(f\"Fastest                : {min(ms):,.2f} ms\")\n",
    "    print(f\"Slowest                : {max(ms):,.2f} ms\")\n",
    "    print(\"────────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1378eec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#With prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7085e5f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Speed-test: run the full segmentation/overlay pipeline on every image in\n",
    "~/Documents/GitHub/Ai-plays-SubwaySurfers/frames, show each processed\n",
    "frame inline in Jupyter (stacked), and print a timing summary at the end.\n",
    "\n",
    "Warm-up frames are executed but not timed or displayed.\n",
    "\"\"\"\n",
    "\n",
    "import os, sys, glob, time, statistics\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Optional notebook display (safe fallback if not in Jupyter)\n",
    "try:\n",
    "    from IPython.display import display\n",
    "    from PIL import Image\n",
    "    _HAS_IPY = True\n",
    "except Exception:\n",
    "    _HAS_IPY = False\n",
    "\n",
    "# ───────────────────────────────────────────────────────── Config ──\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40\n",
    "\n",
    "TARGET_COLORS_RGB  = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE          = 20.0\n",
    "MIN_REGION_SIZE    = 30\n",
    "MIN_REGION_HEIGHT  = 150\n",
    "\n",
    "HEAT_BLUR_KSIZE     = 51\n",
    "RED_SCORE_THRESH    = 220\n",
    "EXCLUDE_TOP_FRAC    = 0.40\n",
    "EXCLUDE_BOTTOM_FRAC = 0.15\n",
    "MIN_DARK_RED_AREA   = 1200\n",
    "TRI_SIZE_PX         = 18\n",
    "PURPLE              = (255, 0, 255)\n",
    "\n",
    "WARMUP_FRAMES       = 10   # how many frames to skip from timing stats\n",
    "\n",
    "# ─────────────────────────────────────── Device / model init ─\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "model = YOLO(weights)\n",
    "try: model.fuse()\n",
    "except Exception: pass\n",
    "model.predict(\n",
    "    np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8),\n",
    "    task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, verbose=False, half=half\n",
    ")\n",
    "\n",
    "# ───────────────────────────── Helper processing functions ─\n",
    "def highlight_rails_mask_only_fast(img_bgr, rail_mask, target_colors_rgb,\n",
    "                                   tolerance=30.0,\n",
    "                                   min_region_size=50,\n",
    "                                   min_region_height=150):\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max()+1\n",
    "    x0, x1 = xs.min(), xs.max()+1\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb],\n",
    "                           dtype=np.float32)\n",
    "    diff  = img_roi.astype(np.float32)[:, :, None, :] - targets_bgr[None, None]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    colour_hit = np.any(dist2 <= tolerance**2, axis=-1)\n",
    "\n",
    "    combined = colour_hit & mask_roi\n",
    "    comp = combined.astype(np.uint8)\n",
    "    n, lbls, stats, _ = cv2.connectedComponentsWithStats(comp, 8)\n",
    "\n",
    "    good = np.zeros_like(combined)\n",
    "    for lbl in range(1, n):\n",
    "        area, h = stats[lbl, cv2.CC_STAT_AREA], stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and h >= min_region_height:\n",
    "            good[lbls == lbl] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = good\n",
    "    return filtered_full\n",
    "\n",
    "\n",
    "def red_vs_green_score(red_mask, green_mask, blur_ksize=51):\n",
    "    k = (blur_ksize, blur_ksize)\n",
    "    diff = (cv2.blur(red_mask.astype(np.float32), k) -\n",
    "            cv2.blur(green_mask.astype(np.float32), k))\n",
    "    norm = (diff / (2*(np.max(np.abs(diff))+1e-6)) + 0.5)\n",
    "    return np.clip(norm * 255, 0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "def make_overlay_panel(img_bgr, rail_mask, green_mask, score_u8):\n",
    "    \"\"\"Build a side-by-side panel: original | overlays (red rail, green keep, score heat).\"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "\n",
    "    # Color overlays\n",
    "    overlay = img_bgr.copy()\n",
    "    # red = rail minus green\n",
    "    red_mask = (rail_mask & ~green_mask)\n",
    "\n",
    "    # tint red & green\n",
    "    red_layer = np.zeros_like(img_bgr);    red_layer[:,:,2] = 255\n",
    "    green_layer = np.zeros_like(img_bgr);  green_layer[:,:,1] = 255\n",
    "\n",
    "    overlay = np.where(red_mask[...,None], cv2.addWeighted(overlay, 1-ALPHA, red_layer, ALPHA, 0), overlay)\n",
    "    overlay = np.where(green_mask[...,None], cv2.addWeighted(overlay, 1-ALPHA, green_layer, ALPHA, 0), overlay)\n",
    "\n",
    "    # score to heat (JET)\n",
    "    score_color = cv2.applyColorMap(score_u8, cv2.COLORMAP_JET)\n",
    "    score_vis = cv2.addWeighted(img_bgr, 0.55, score_color, 0.45, 0)\n",
    "\n",
    "    # stack: original | overlay | score\n",
    "    panel = np.concatenate([img_bgr, overlay, score_vis], axis=1)\n",
    "\n",
    "    # simple rulers/text\n",
    "    cv2.putText(panel, \"original\", (10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (32,32,255), 2, cv2.LINE_AA)\n",
    "    cv2.putText(panel, \"rail(red) / keep(green)\", (W+10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (32,255,32), 2, cv2.LINE_AA)\n",
    "    cv2.putText(panel, \"red-vs-green score\", (2*W+10, 28), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (255,255,32), 2, cv2.LINE_AA)\n",
    "\n",
    "    return panel\n",
    "\n",
    "\n",
    "def process_frame(img_bgr):\n",
    "    \"\"\"Run segmentation + masks + score; return rail_mask, green_mask, score_u8, and a display panel.\"\"\"\n",
    "    res = model.predict(img_bgr, task=\"segment\", imgsz=IMG_SIZE,\n",
    "                        device=device, conf=CONF, iou=IOU,\n",
    "                        max_det=30, verbose=False, half=half)[0]\n",
    "    if res.masks is None:\n",
    "        H, W = img_bgr.shape[:2]\n",
    "        empty = np.zeros((H, W), dtype=bool)\n",
    "        score = np.zeros((H, W), dtype=np.uint8)\n",
    "        panel = make_overlay_panel(img_bgr, empty, empty, score)\n",
    "        return empty, empty, score, panel\n",
    "\n",
    "    masks   = res.masks.data.cpu().numpy()\n",
    "    classes = res.masks.cls.cpu().numpy() if hasattr(res.masks, \"cls\") \\\n",
    "              else res.boxes.cls.cpu().numpy()\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    rail_union = np.zeros(masks.shape[1:], bool)\n",
    "    for m, c in zip(masks, classes):\n",
    "        if int(c) == RAIL_ID:\n",
    "            rail_union |= m.astype(bool)\n",
    "\n",
    "    rail_mask = cv2.resize(rail_union.astype(np.uint8), (W, H),\n",
    "                           interpolation=cv2.INTER_NEAREST).astype(bool)\n",
    "\n",
    "    green = highlight_rails_mask_only_fast(img_bgr, rail_mask,\n",
    "                                           TARGET_COLORS_RGB, TOLERANCE,\n",
    "                                           MIN_REGION_SIZE, MIN_REGION_HEIGHT)\n",
    "    red   = rail_mask & ~green\n",
    "    score = red_vs_green_score(red, green, HEAT_BLUR_KSIZE)\n",
    "\n",
    "    # exclude top/bottom bands from consideration\n",
    "    score[:int(H*EXCLUDE_TOP_FRAC), :] = 0\n",
    "    score[H-int(H*EXCLUDE_BOTTOM_FRAC):, :] = 0\n",
    "\n",
    "    panel = make_overlay_panel(img_bgr, rail_mask, green, score)\n",
    "    return rail_mask, green, score, panel\n",
    "\n",
    "# ─────────────────────────────────────────────────────── Main test ──\n",
    "if __name__ == \"__main__\":\n",
    "    folder = (Path.home() / \"Documents\" / \"GitHub\" /\n",
    "              \"Ai-plays-SubwaySurfers\" / \"frames\")\n",
    "\n",
    "    if not folder.is_dir():\n",
    "        print(f\"[ERROR] frames folder not found: {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    paths = sorted(\n",
    "        glob.glob(str(folder / \"frame_*.jpg\")) +\n",
    "        glob.glob(str(folder / \"frame_*.png\")) +\n",
    "        glob.glob(str(folder / \"*.jpg\")) +\n",
    "        glob.glob(str(folder / \"*.png\"))\n",
    "    )\n",
    "    if not paths:\n",
    "        print(f\"[ERROR] No frame images found in {folder}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "    times = []\n",
    "    t_start = time.perf_counter()\n",
    "\n",
    "    for idx, p in enumerate(paths):\n",
    "        frame = cv2.imread(p)\n",
    "\n",
    "        if idx < WARMUP_FRAMES:\n",
    "            # Warm-up: run but don't time or display\n",
    "            _ = process_frame(frame)\n",
    "            continue\n",
    "\n",
    "        t0 = time.perf_counter()\n",
    "        rail_mask, green_mask, score_u8, panel_bgr = process_frame(frame)\n",
    "        dt = time.perf_counter() - t0\n",
    "        times.append(dt)\n",
    "\n",
    "        # print timing line\n",
    "        print(f\"{idx:04d} | {Path(p).name} | {dt*1000:7.2f} ms\")\n",
    "\n",
    "        # show stacked image in Jupyter (if available)\n",
    "        if _HAS_IPY:\n",
    "            rgb = cv2.cvtColor(panel_bgr, cv2.COLOR_BGR2RGB)\n",
    "            display(Image.fromarray(rgb))\n",
    "\n",
    "    total_time = time.perf_counter() - t_start\n",
    "    timed_frames = len(times)\n",
    "\n",
    "    # ─────────────────────────────── Summary ─\n",
    "    ms = [t * 1_000 for t in times]\n",
    "    if timed_frames == 0:\n",
    "        print(\"\\n[WARNING] fewer frames than WARMUP_FRAMES; nothing timed.\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    print(\"\\n───── Speed-test summary (warm-up skipped) ─────\")\n",
    "    print(f\"Total frames           : {len(paths)}\")\n",
    "    print(f\"Warm-up frames ignored : {WARMUP_FRAMES}\")\n",
    "    print(f\"Frames timed           : {timed_frames}\")\n",
    "    print(f\"Total wall-clock time  : {total_time:,.2f} s\")\n",
    "    print(f\"Average / frame        : {statistics.mean(ms):,.2f} ms\"\n",
    "          f\"  ({1_000/statistics.mean(ms):,.2f} FPS)\")\n",
    "    print(f\"Median                 : {statistics.median(ms):,.2f} ms\")\n",
    "    print(f\"Fastest                : {min(ms):,.2f} ms\")\n",
    "    print(f\"Slowest                : {max(ms):,.2f} ms\")\n",
    "    print(\"────────────────────────────────────────────────\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85cf016",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
