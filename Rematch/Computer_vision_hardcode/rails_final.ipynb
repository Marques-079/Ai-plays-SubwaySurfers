{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529fab70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n",
      "WARNING ⚠️ NMS time limit 2.050s exceeded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 17:15:30.890 Python[21551:28423695] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-04 17:15:30.890 Python[21551:28423695] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, cv2, torch, numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512          # 448–640 depending on speed/accuracy needs\n",
    "CONF     = 0.30\n",
    "IOU      = 0.45\n",
    "ALPHA    = 0.40         # red rail overlay alpha\n",
    "\n",
    "# --- color-filter (from earlier prompt) ---\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]  # in RGB\n",
    "TOLERANCE         = 20.0                        # Euclidean distance in BGR space\n",
    "MIN_REGION_SIZE   = 50                          # px (connected component area)\n",
    "MIN_REGION_HEIGHT = 150                         # px (connected component bbox height)\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warmup)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(\n",
    "    _dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "    verbose=False, half=half\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# Color+size filter (fast version of your highlight_rails_mask_only_timed)\n",
    "# =======================\n",
    "def highlight_rails_mask_only_fast(\n",
    "    img_bgr: np.ndarray,\n",
    "    rail_mask: np.ndarray,\n",
    "    target_colors_rgb: list[tuple[int,int,int]],\n",
    "    tolerance: float = 30.0,\n",
    "    min_region_size: int = 50,\n",
    "    min_region_height: int = 150\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns boolean mask of rail pixels that:\n",
    "      (1) color-match any target color within tolerance, AND\n",
    "      (2) lie inside rail_mask, AND\n",
    "      (3) pass connected-component size/height filters.\n",
    "    \"\"\"\n",
    "    h, w = img_bgr.shape[:2]\n",
    "\n",
    "    # RGB -> BGR (OpenCV is BGR)\n",
    "    targets_bgr = [(rbg[2], rbg[1], rbg[0]) for rbg in target_colors_rgb]\n",
    "\n",
    "    # Vectorized color distance check\n",
    "    img_f = img_bgr.astype(np.float32)\n",
    "    color_mask = np.zeros((h, w), dtype=bool)\n",
    "    for tb in targets_bgr:\n",
    "        tb_arr = np.array(tb, dtype=np.float32).reshape((1, 1, 3))\n",
    "        dist = np.linalg.norm(img_f - tb_arr, axis=2)\n",
    "        color_mask |= (dist <= tolerance)\n",
    "\n",
    "    combined = rail_mask & color_mask\n",
    "\n",
    "    # Connected component filtering\n",
    "    comp_u8 = combined.astype(np.uint8)\n",
    "    n_labels, labels, stats, _ = cv2.connectedComponentsWithStats(comp_u8, 8)\n",
    "    filtered = np.zeros_like(combined)\n",
    "    for lbl in range(1, n_labels):\n",
    "        area   = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        height = stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and height >= min_region_height:\n",
    "            filtered[labels == lbl] = True\n",
    "\n",
    "    return filtered  # bool HxW\n",
    "\n",
    "# =======================\n",
    "# Per-frame pipeline\n",
    "# =======================\n",
    "def process_frame(img_bgr: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"YOLO rails overlay (red) + your color/size filtered regions (neon green).\"\"\"\n",
    "    res = model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "        conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "        max_det=20, verbose=False, half=half\n",
    "    )[0]\n",
    "\n",
    "    if res.masks is None:\n",
    "        return img_bgr  # nothing detected\n",
    "\n",
    "    # Union of all rail masks at model resolution → resize to frame\n",
    "    m = res.masks.data  # [N, h_m, w_m] tensor\n",
    "    union = (m.sum(dim=0) > 0).float().cpu().numpy()\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if union.shape != (H, W):\n",
    "        union = cv2.resize(union, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "    rail_mask = union.astype(bool)\n",
    "\n",
    "    # --- Your filter applied on the rails only (fast) ---\n",
    "    filtered_mask = highlight_rails_mask_only_fast(\n",
    "        img_bgr, rail_mask,\n",
    "        target_colors_rgb=TARGET_COLORS_RGB,\n",
    "        tolerance=TOLERANCE,\n",
    "        min_region_size=MIN_REGION_SIZE,\n",
    "        min_region_height=MIN_REGION_HEIGHT\n",
    "    )\n",
    "\n",
    "    # --- Compose visualization ---\n",
    "    out = img_bgr.copy()\n",
    "\n",
    "    # 1) red tint for all rails\n",
    "    overlay = out.copy()\n",
    "    overlay[rail_mask] = (0, 0, 255)\n",
    "    out = cv2.addWeighted(overlay, ALPHA, out, 1 - ALPHA, 0)\n",
    "\n",
    "    # 2) neon-green hard paint for filtered regions on top\n",
    "    out[filtered_mask] = (0, 255, 0)\n",
    "\n",
    "    return out\n",
    "\n",
    "# =======================\n",
    "# Example: directory poll (replace with your frame source)\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    import glob, time\n",
    "    image_dir = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    seen = set()\n",
    "\n",
    "    while True:\n",
    "        for p in sorted(glob.glob(os.path.join(image_dir, \"frame_*.jpg\"))):\n",
    "            if p in seen:\n",
    "                continue\n",
    "            img = cv2.imread(p)\n",
    "            if img is None:\n",
    "                continue\n",
    "\n",
    "            out = process_frame(img)\n",
    "            cv2.imshow(\"rails + filtered\", out)\n",
    "            cv2.waitKey(1)\n",
    "\n",
    "            seen.add(p)\n",
    "        time.sleep(0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16bf6bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimal function below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde081eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 17:22:27.525 Python[22068:28431849] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-04 17:22:27.525 Python[22068:28431849] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os, glob, sys, time\n",
    "import cv2, torch, numpy as np\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40  # red tint on rail mask\n",
    "\n",
    "# Color/size filter (RGB targets; converted to BGR internally)\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0      # color distance (BGR) – increase if too strict\n",
    "MIN_REGION_SIZE   = 30        # connected-component area in px\n",
    "MIN_REGION_HEIGHT = 150       # connected-component bbox height in px\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warmup)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(\n",
    "    _dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "    verbose=False, half=half\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# FAST color+size filter (vectorized + ROI-cropped)\n",
    "# =======================\n",
    "def highlight_rails_mask_only_fast(\n",
    "    img_bgr: np.ndarray,\n",
    "    rail_mask: np.ndarray,\n",
    "    target_colors_rgb: list[tuple[int,int,int]],\n",
    "    tolerance: float = 30.0,\n",
    "    min_region_size: int = 50,\n",
    "    min_region_height: int = 150\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Returns boolean mask of rail pixels that:\n",
    "      (1) color-match any target color within tolerance, AND\n",
    "      (2) lie inside rail_mask, AND\n",
    "      (3) pass connected-component size/height filters.\n",
    "    Vectorized color distance + restrict work to the rail ROI for speed.\n",
    "    \"\"\"\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "\n",
    "    # Early exit\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    # --- Crop to ROI to reduce work ---\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    # --- Vectorized color match against multiple targets ---\n",
    "    # Convert RGB targets -> BGR for OpenCV\n",
    "    targets_bgr = np.array([(rbg[2], rbg[1], rbg[0]) for rbg in target_colors_rgb],\n",
    "                           dtype=np.float32)  # (K,3)\n",
    "\n",
    "    img_f = img_roi.astype(np.float32)                    # (h,w,3)\n",
    "    diff  = img_f[:, :, None, :] - targets_bgr[None, None, :, :]  # (h,w,K,3)\n",
    "    dist2 = np.sum(diff * diff, axis=-1)                  # (h,w,K)\n",
    "    color_mask_roi = np.any(dist2 <= (tolerance * tolerance), axis=-1)  # (h,w)\n",
    "\n",
    "    combined_roi = mask_roi & color_mask_roi\n",
    "\n",
    "    # --- Connected component filtering in ROI ---\n",
    "    comp_u8 = combined_roi.astype(np.uint8)\n",
    "    n_labels, labels, stats, _ = cv2.connectedComponentsWithStats(comp_u8, 8)\n",
    "\n",
    "    filtered_roi = np.zeros_like(combined_roi)\n",
    "    for lbl in range(1, n_labels):\n",
    "        area   = stats[lbl, cv2.CC_STAT_AREA]\n",
    "        height = stats[lbl, cv2.CC_STAT_HEIGHT]\n",
    "        if area >= min_region_size and height >= min_region_height:\n",
    "            filtered_roi[labels == lbl] = True\n",
    "\n",
    "    # Put ROI back\n",
    "    filtered_full[y0:y1, x0:x1] = filtered_roi\n",
    "    return filtered_full\n",
    "\n",
    "# =======================\n",
    "# Per-frame pipeline\n",
    "# =======================\n",
    "def process_frame(img_bgr: np.ndarray) -> tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - out_bgr : original with red rail mask + neon green filtered regions\n",
    "      - rail_mask : boolean rail mask (for debugging if needed)\n",
    "    \"\"\"\n",
    "    res = model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "        conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "        max_det=20, verbose=False, half=half\n",
    "    )[0]\n",
    "\n",
    "    if res.masks is None:\n",
    "        return img_bgr, np.zeros(img_bgr.shape[:2], bool)\n",
    "\n",
    "    # Union of all rail masks → resize to frame size\n",
    "    m = res.masks.data\n",
    "    union = (m.sum(dim=0) > 0).float().cpu().numpy()\n",
    "\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if union.shape != (H, W):\n",
    "        union = cv2.resize(union, (W, H), interpolation=cv2.INTER_NEAREST)\n",
    "    rail_mask = union.astype(bool)\n",
    "\n",
    "    # Apply fast color/size filter inside rails\n",
    "    filtered_mask = highlight_rails_mask_only_fast(\n",
    "        img_bgr, rail_mask,\n",
    "        target_colors_rgb=TARGET_COLORS_RGB,\n",
    "        tolerance=TOLERANCE,\n",
    "        min_region_size=MIN_REGION_SIZE,\n",
    "        min_region_height=MIN_REGION_HEIGHT\n",
    "    )\n",
    "\n",
    "    # Compose visualization\n",
    "    out = img_bgr.copy()\n",
    "\n",
    "    # red tint for all rails\n",
    "    overlay = out.copy()\n",
    "    overlay[rail_mask] = (0, 0, 255)\n",
    "    out = cv2.addWeighted(overlay, ALPHA, out, 1 - ALPHA, 0)\n",
    "\n",
    "    # neon-green for filtered regions (hard paint)\n",
    "    out[filtered_mask] = (0, 255, 0)\n",
    "\n",
    "    return out, rail_mask\n",
    "\n",
    "# =======================\n",
    "# Run over a folder: show side-by-side and advance on SPACE\n",
    "# =======================\n",
    "if __name__ == \"__main__\":\n",
    "    image_dir = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(image_dir, \"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found in\", image_dir)\n",
    "        sys.exit(1)\n",
    "\n",
    "    for p in paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        vis, _ = process_frame(img)\n",
    "\n",
    "        # Left: original | Right: filtered+masked\n",
    "        combo = np.hstack((img, vis))\n",
    "        cv2.imshow(\"Original (Left)  |  Rails+Filtered (Right)\", combo)\n",
    "\n",
    "        # Wait for SPACE (next) or 'q' (quit)\n",
    "        while True:\n",
    "            key = cv2.waitKey(0) & 0xFF\n",
    "            if key == 32:        # SPACE → next image\n",
    "                break\n",
    "            elif key == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "        cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6411ae9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 17:40:01.491 Python[22735:28448242] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-04 17:40:01.491 Python[22735:28448242] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from skimage.morphology import skeletonize\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40  # red tint on rail mask\n",
    "\n",
    "# Color/size filter (RGB targets; converted to BGR internally)\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0      # color distance (BGR)\n",
    "MIN_REGION_SIZE   = 30        # connected-component area in px\n",
    "MIN_REGION_HEIGHT = 150       # connected-component bbox height in px\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warmup)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(\n",
    "    _dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "    verbose=False, half=half\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# FAST color+size filter\n",
    "# =======================\n",
    "def highlight_rails_mask_only_fast(\n",
    "    img_bgr: np.ndarray,\n",
    "    rail_mask: np.ndarray,\n",
    "    target_colors_rgb: list[tuple[int,int,int]],\n",
    "    tolerance: float,\n",
    "    min_region_size: int,\n",
    "    min_region_height: int\n",
    ") -> np.ndarray:\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    # ROI crop\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    # color match\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb], np.float32)\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:,:,None,:] - targets_bgr[None,None,:,:]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    color_mask = np.any(dist2 <= (tolerance*tolerance), axis=-1)\n",
    "\n",
    "    combined = mask_roi & color_mask\n",
    "\n",
    "    # component filter\n",
    "    comp_u8 = combined.astype(np.uint8)\n",
    "    n, labels, stats, _ = cv2.connectedComponentsWithStats(comp_u8, 8)\n",
    "    filt = np.zeros_like(combined)\n",
    "    for i in range(1,n):\n",
    "        area   = stats[i,cv2.CC_STAT_AREA]\n",
    "        height = stats[i,cv2.CC_STAT_HEIGHT]\n",
    "        if area>=min_region_size and height>=min_region_height:\n",
    "            filt[labels==i] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = filt\n",
    "    return filtered_full\n",
    "\n",
    "# =======================\n",
    "# Per-frame pipeline\n",
    "# =======================\n",
    "def process_frame(img_bgr: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns three things:\n",
    "      - vis: original tinted with rails (red) and filtered (green)\n",
    "      - rail_mask: bool mask of all rails\n",
    "      - filtered_mask: bool mask of color+size filtered rail pixels\n",
    "    \"\"\"\n",
    "    res = model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "        conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "        max_det=20, verbose=False, half=half\n",
    "    )[0]\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:\n",
    "        return img_bgr.copy(), np.zeros((H,W),bool), np.zeros((H,W),bool)\n",
    "\n",
    "    # union rail masks → resize\n",
    "    m = res.masks.data\n",
    "    union = (m.sum(dim=0)>0).float().cpu().numpy()\n",
    "    if union.shape != (H,W):\n",
    "        union = cv2.resize(union, (W,H), interpolation=cv2.INTER_NEAREST)\n",
    "    rail_mask = union.astype(bool)\n",
    "\n",
    "    # filter inside rail_mask\n",
    "    filtered_mask = highlight_rails_mask_only_fast(\n",
    "        img_bgr, rail_mask,\n",
    "        TARGET_COLORS_RGB,\n",
    "        TOLERANCE,\n",
    "        MIN_REGION_SIZE,\n",
    "        MIN_REGION_HEIGHT\n",
    "    )\n",
    "\n",
    "    # compose tinted view\n",
    "    vis = img_bgr.copy()\n",
    "    over = vis.copy(); over[rail_mask] = (0,0,255)\n",
    "    vis = cv2.addWeighted(over, ALPHA, vis, 1-ALPHA, 0)\n",
    "    vis[filtered_mask] = (0,255,0)\n",
    "\n",
    "    return vis, rail_mask, filtered_mask\n",
    "\n",
    "# =======================\n",
    "# Main: display 3 panels\n",
    "# =======================\n",
    "if __name__==\"__main__\":\n",
    "    image_dir = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(image_dir,\"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found\"); sys.exit(1)\n",
    "\n",
    "    for p in paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        vis, rail_mask, filtered_mask = process_frame(img)\n",
    "        lane_mask = rail_mask & (~filtered_mask)\n",
    "\n",
    "        # skeletonize lanes\n",
    "        sk = skeletonize(lane_mask).astype(np.uint8)\n",
    "        sk_vis = img.copy()\n",
    "        sk_vis[sk==1] = (255,255,0)  # BGR aqua\n",
    "\n",
    "        # top row: original | tinted\n",
    "        top = np.hstack((img, vis))\n",
    "\n",
    "        # bottom: skeleton overlay spanning both halves\n",
    "        blank = np.zeros_like(img)\n",
    "        bot   = np.hstack((sk_vis, blank))\n",
    "\n",
    "        canvas = np.vstack((top, bot))\n",
    "        cv2.imshow(\"Orig | Rails+Filt  (top)   Skeleton (bottom left)\", canvas)\n",
    "\n",
    "        # advance on SPACE, quit on 'q'\n",
    "        while True:\n",
    "            k = cv2.waitKey(0) & 0xFF\n",
    "            if k==32:    # space\n",
    "                break\n",
    "            if k==ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        time.sleep(0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5986fca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Height filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dabb5037",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 17:56:01.634 Python[23543:28466670] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-04 17:56:01.634 Python[23543:28466670] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from skimage.morphology import skeletonize\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40  # red tint on rail mask\n",
    "\n",
    "# Color/size filter (RGB targets; converted to BGR internally)\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0      # color distance (BGR)\n",
    "MIN_REGION_SIZE   = 30        # connected-component area in px\n",
    "MIN_REGION_HEIGHT = 150       # connected-component bbox height in px\n",
    "\n",
    "# Skeleton pruning\n",
    "MIN_SKELETON_LEN = 1000        # minimum skeleton‐pixel count to keep a branch\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warmup)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(\n",
    "    _dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "    verbose=False, half=half\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# FAST color+size filter\n",
    "# =======================\n",
    "def highlight_rails_mask_only_fast(\n",
    "    img_bgr: np.ndarray,\n",
    "    rail_mask: np.ndarray,\n",
    "    target_colors_rgb: list[tuple[int,int,int]],\n",
    "    tolerance: float,\n",
    "    min_region_size: int,\n",
    "    min_region_height: int\n",
    ") -> np.ndarray:\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    # ROI crop\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    # color match\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb], np.float32)\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:,:,None,:] - targets_bgr[None,None,:,:]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    color_mask = np.any(dist2 <= (tolerance*tolerance), axis=-1)\n",
    "\n",
    "    combined = mask_roi & color_mask\n",
    "\n",
    "    # component filter\n",
    "    comp_u8 = combined.astype(np.uint8)\n",
    "    n, labels, stats, _ = cv2.connectedComponentsWithStats(comp_u8, 8)\n",
    "    filt = np.zeros_like(combined)\n",
    "    for i in range(1,n):\n",
    "        area   = stats[i,cv2.CC_STAT_AREA]\n",
    "        height = stats[i,cv2.CC_STAT_HEIGHT]\n",
    "        if area>=min_region_size and height>=min_region_height:\n",
    "            filt[labels==i] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = filt\n",
    "    return filtered_full\n",
    "\n",
    "# =======================\n",
    "# Per-frame pipeline\n",
    "# =======================\n",
    "def process_frame(img_bgr: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - vis: original tinted with rails (red) and filtered (green)\n",
    "      - rail_mask: bool mask of all rails\n",
    "      - filtered_mask: bool mask of color+size filtered rail pixels\n",
    "    \"\"\"\n",
    "    res = model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "        conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "        max_det=20, verbose=False, half=half\n",
    "    )[0]\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:\n",
    "        return img_bgr.copy(), np.zeros((H,W),bool), np.zeros((H,W),bool)\n",
    "\n",
    "    # union rail masks → resize\n",
    "    m = res.masks.data\n",
    "    union = (m.sum(dim=0)>0).float().cpu().numpy()\n",
    "    if union.shape != (H,W):\n",
    "        union = cv2.resize(union, (W,H), interpolation=cv2.INTER_NEAREST)\n",
    "    rail_mask = union.astype(bool)\n",
    "\n",
    "    # filter inside rail_mask\n",
    "    filtered_mask = highlight_rails_mask_only_fast(\n",
    "        img_bgr, rail_mask,\n",
    "        TARGET_COLORS_RGB,\n",
    "        TOLERANCE,\n",
    "        MIN_REGION_SIZE,\n",
    "        MIN_REGION_HEIGHT\n",
    "    )\n",
    "\n",
    "    # compose tinted view\n",
    "    vis = img_bgr.copy()\n",
    "    over = vis.copy(); over[rail_mask] = (0,0,255)\n",
    "    vis = cv2.addWeighted(over, ALPHA, vis, 1-ALPHA, 0)\n",
    "    vis[filtered_mask] = (0,255,0)\n",
    "\n",
    "    return vis, rail_mask, filtered_mask\n",
    "\n",
    "# =======================\n",
    "# Main: display 3 panels with cleaned skeleton\n",
    "# =======================\n",
    "if __name__==\"__main__\":\n",
    "    image_dir = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(image_dir,\"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found\"); sys.exit(1)\n",
    "\n",
    "    for p in paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        vis, rail_mask, filtered_mask = process_frame(img)\n",
    "        lane_mask = rail_mask & (~filtered_mask)\n",
    "\n",
    "        # skeletonize lanes\n",
    "        sk = skeletonize(lane_mask).astype(np.uint8)\n",
    "\n",
    "        # prune short skeleton branches\n",
    "        n_s, lbls = cv2.connectedComponents(sk, connectivity=8)\n",
    "        sk_clean = np.zeros_like(sk)\n",
    "        for lbl in range(1, n_s):\n",
    "            if (lbls==lbl).sum() >= MIN_SKELETON_LEN:\n",
    "                sk_clean[lbls==lbl] = 1\n",
    "\n",
    "        # make skeleton overlay\n",
    "        sk_vis = img.copy()\n",
    "        sk_vis[sk_clean==1] = (255,255,0)  # BGR aqua\n",
    "\n",
    "        # top row: original | tinted+filtered\n",
    "        top = np.hstack((img, vis))\n",
    "\n",
    "        # bottom: cleaned skeleton overlay | blank\n",
    "        blank = np.zeros_like(img)\n",
    "        bottom = np.hstack((sk_vis, blank))\n",
    "\n",
    "        canvas = np.vstack((top, bottom))\n",
    "        cv2.imshow(\"Orig | Rails+Filt  (top)   Skeleton (bottom left)\", canvas)\n",
    "\n",
    "        # advance on SPACE, quit on 'q'\n",
    "        while True:\n",
    "            k = cv2.waitKey(0) & 0xFF\n",
    "            if k == 32:    # space\n",
    "                break\n",
    "            if k == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        time.sleep(0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08087d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO11n-seg summary (fused): 113 layers, 2,836,908 parameters, 0 gradients, 10.2 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 18:00:32.147 Python[23910:28473671] +[IMKClient subclass]: chose IMKClient_Legacy\n",
      "2025-08-04 18:00:32.147 Python[23910:28473671] +[IMKInputSession subclass]: chose IMKInputSession_Legacy\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "import os\n",
    "import glob\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "from skimage.morphology import skeletonize\n",
    "\n",
    "# =======================\n",
    "# Config\n",
    "# =======================\n",
    "home     = os.path.expanduser(\"~\")\n",
    "weights  = f\"{home}/models/jakes-loped/jakes-finder-mk1/1/weights.pt\"\n",
    "RAIL_ID  = 9\n",
    "\n",
    "IMG_SIZE = 512\n",
    "CONF, IOU = 0.30, 0.45\n",
    "ALPHA    = 0.40  # red tint on rail mask\n",
    "\n",
    "# Color/size filter (RGB targets; converted to BGR internally)\n",
    "TARGET_COLORS_RGB = [(119,104,67), (81,42,45)]\n",
    "TOLERANCE         = 20.0      # color distance (BGR)\n",
    "MIN_REGION_SIZE   = 30        # connected-component area in px\n",
    "MIN_REGION_HEIGHT = 150       # connected-component bbox height in px\n",
    "\n",
    "# Skeleton pruning by width\n",
    "MIN_SKELETON_WIDTH = 60       # minimum lane-width in px to keep a skeleton branch\n",
    "\n",
    "# =======================\n",
    "# Device / precision\n",
    "# =======================\n",
    "if torch.cuda.is_available():\n",
    "    device, half = 0, True\n",
    "elif getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available():\n",
    "    device, half = \"mps\", False\n",
    "else:\n",
    "    device, half = \"cpu\", False\n",
    "\n",
    "# =======================\n",
    "# Model (load, fuse, warmup)\n",
    "# =======================\n",
    "model = YOLO(weights)\n",
    "try:\n",
    "    model.fuse()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "_dummy = np.zeros((IMG_SIZE, IMG_SIZE, 3), np.uint8)\n",
    "_ = model.predict(\n",
    "    _dummy, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "    conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "    verbose=False, half=half\n",
    ")\n",
    "\n",
    "# =======================\n",
    "# FAST color+size filter\n",
    "# =======================\n",
    "def highlight_rails_mask_only_fast(\n",
    "    img_bgr: np.ndarray,\n",
    "    rail_mask: np.ndarray,\n",
    "    target_colors_rgb: list[tuple[int,int,int]],\n",
    "    tolerance: float,\n",
    "    min_region_size: int,\n",
    "    min_region_height: int\n",
    ") -> np.ndarray:\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    filtered_full = np.zeros((H, W), dtype=bool)\n",
    "    if not rail_mask.any():\n",
    "        return filtered_full\n",
    "\n",
    "    # ROI crop\n",
    "    ys, xs = np.where(rail_mask)\n",
    "    y0, y1 = ys.min(), ys.max() + 1\n",
    "    x0, x1 = xs.min(), xs.max() + 1\n",
    "    img_roi  = img_bgr[y0:y1, x0:x1]\n",
    "    mask_roi = rail_mask[y0:y1, x0:x1]\n",
    "\n",
    "    # color match\n",
    "    targets_bgr = np.array([(r,g,b)[::-1] for r,g,b in target_colors_rgb], np.float32)\n",
    "    img_f = img_roi.astype(np.float32)\n",
    "    diff  = img_f[:,:,None,:] - targets_bgr[None,None,:,:]\n",
    "    dist2 = np.sum(diff*diff, axis=-1)\n",
    "    color_mask = np.any(dist2 <= (tolerance*tolerance), axis=-1)\n",
    "\n",
    "    combined = mask_roi & color_mask\n",
    "\n",
    "    # component filter\n",
    "    comp_u8 = combined.astype(np.uint8)\n",
    "    n, labels, stats, _ = cv2.connectedComponentsWithStats(comp_u8, 8)\n",
    "    filt = np.zeros_like(combined)\n",
    "    for i in range(1,n):\n",
    "        area   = stats[i,cv2.CC_STAT_AREA]\n",
    "        height = stats[i,cv2.CC_STAT_HEIGHT]\n",
    "        if area>=min_region_size and height>=min_region_height:\n",
    "            filt[labels==i] = True\n",
    "\n",
    "    filtered_full[y0:y1, x0:x1] = filt\n",
    "    return filtered_full\n",
    "\n",
    "# =======================\n",
    "# Per-frame pipeline\n",
    "# =======================\n",
    "def process_frame(img_bgr: np.ndarray):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      - vis: original tinted with rails (red) and filtered (green)\n",
    "      - rail_mask: bool mask of all rails\n",
    "      - filtered_mask: bool mask of color+size filtered rail pixels\n",
    "    \"\"\"\n",
    "    res = model.predict(\n",
    "        img_bgr, task=\"segment\", imgsz=IMG_SIZE, device=device,\n",
    "        conf=CONF, iou=IOU, classes=[RAIL_ID],\n",
    "        max_det=20, verbose=False, half=half\n",
    "    )[0]\n",
    "    H, W = img_bgr.shape[:2]\n",
    "    if res.masks is None:\n",
    "        return img_bgr.copy(), np.zeros((H,W),bool), np.zeros((H,W),bool)\n",
    "\n",
    "    # union rail masks → resize\n",
    "    m = res.masks.data\n",
    "    union = (m.sum(dim=0)>0).float().cpu().numpy()\n",
    "    if union.shape != (H,W):\n",
    "        union = cv2.resize(union, (W,H), interpolation=cv2.INTER_NEAREST)\n",
    "    rail_mask = union.astype(bool)\n",
    "\n",
    "    # filter inside rail_mask\n",
    "    filtered_mask = highlight_rails_mask_only_fast(\n",
    "        img_bgr, rail_mask,\n",
    "        TARGET_COLORS_RGB,\n",
    "        TOLERANCE,\n",
    "        MIN_REGION_SIZE,\n",
    "        MIN_REGION_HEIGHT\n",
    "    )\n",
    "\n",
    "    # compose tinted view\n",
    "    vis = img_bgr.copy()\n",
    "    over = vis.copy(); over[rail_mask] = (0,0,255)\n",
    "    vis = cv2.addWeighted(over, ALPHA, vis, 1-ALPHA, 0)\n",
    "    vis[filtered_mask] = (0,255,0)\n",
    "\n",
    "    return vis, rail_mask, filtered_mask\n",
    "\n",
    "# =======================\n",
    "# Main: display 3 panels with width-pruned skeleton\n",
    "# =======================\n",
    "if __name__==\"__main__\":\n",
    "    image_dir = f\"{home}/SubwaySurfers/train_screenshots\"\n",
    "    paths = sorted(glob.glob(os.path.join(image_dir,\"frame_*.jpg\")))\n",
    "    if not paths:\n",
    "        print(\"No frames found\"); sys.exit(1)\n",
    "\n",
    "    for p in paths:\n",
    "        img = cv2.imread(p)\n",
    "        if img is None:\n",
    "            continue\n",
    "\n",
    "        vis, rail_mask, filtered_mask = process_frame(img)\n",
    "        lane_mask = rail_mask & (~filtered_mask)\n",
    "\n",
    "        # skeletonize lanes\n",
    "        sk = skeletonize(lane_mask).astype(np.uint8)\n",
    "\n",
    "        # compute distance‐transform of lane_mask\n",
    "        dist = cv2.distanceTransform((lane_mask*255).astype(np.uint8),\n",
    "                                     cv2.DIST_L2, cv2.DIST_MASK_PRECISE)\n",
    "\n",
    "        # prune skeleton branches narrower than MIN_SKELETON_WIDTH\n",
    "        n_s, lbls = cv2.connectedComponents(sk, connectivity=8)\n",
    "        sk_clean = np.zeros_like(sk)\n",
    "        for lbl in range(1, n_s):\n",
    "            comp = (lbls == lbl)\n",
    "            # measure maximum width along this branch\n",
    "            # width ≈ 2 * max(distance_transform)\n",
    "            max_radius = dist[comp].max() if comp.any() else 0\n",
    "            max_width = 2 * max_radius\n",
    "            if max_width >= MIN_SKELETON_WIDTH:\n",
    "                sk_clean[comp] = 1\n",
    "\n",
    "        # make skeleton overlay\n",
    "        sk_vis = img.copy()\n",
    "        sk_vis[sk_clean==1] = (255,255,0)  # BGR aqua\n",
    "\n",
    "        # top row: original | tinted+filtered\n",
    "        top = np.hstack((img, vis))\n",
    "\n",
    "        # bottom: width-pruned skeleton overlay | blank\n",
    "        blank = np.zeros_like(img)\n",
    "        bottom = np.hstack((sk_vis, blank))\n",
    "\n",
    "        canvas = np.vstack((top, bottom))\n",
    "        cv2.imshow(\"Orig | Rails+Filt (top)   Skeleton (bottom left)\", canvas)\n",
    "\n",
    "        # advance on SPACE, quit on 'q'\n",
    "        while True:\n",
    "            k = cv2.waitKey(0) & 0xFF\n",
    "            if k == 32:    # space\n",
    "                break\n",
    "            if k == ord('q'):\n",
    "                cv2.destroyAllWindows()\n",
    "                sys.exit(0)\n",
    "\n",
    "        cv2.destroyAllWindows()\n",
    "        time.sleep(0.02)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc8fd08",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
