hmmm wait so then cant we be greedy with our auto labeller? 
picking only the best state in the moment? Becuase when we train our transformer 
we give context 16ish frames and we predict what needs to happen in the given moment, 
regardless of the pass. A sucessfuly training siutation would be that we are mid air 
but want to move right we move right, and pass the past 8 frames (in which we jumped) 
into the transformer. So in game when a similar siutation comes up EG. 
we jumped (seen in last 8 frames) and now where do we want to move? to the right!

Establish a mechanism to label data given a single frame. Give nthe frame it will greedily pick the best region it wants 
to be in and then return the move which best achieves this goal. Very key stage here as not 
only labels our data but during inferemce this also makes decsions.

Train a transformer on CNN encoded frames paired with the augmented data labels created by pairing video frames with
the identification mechanism. This will allow for combos to be formed with a chain of images being the past moves
and the transformer predicts the next move (with context) unlike our greedy labeller. 

#24/04/2025

//Can we train our CNN on game state variables?

In inference we pass the frame into the CNN for encoding and pass those collection of frames into the transformer for prediction.
I dont think we shoud use the autolabeller in the actual inference pipeline because transformer should learn labels off the autolabeller
Im gonna use gamestate variables to train the auto labeller for high accuracy label generation.

//Starting on the auto labeller now 
Tossing up whether to do a decison tree or a hand coded if/else linchpin

Im thinking being able to identify high and low ground and currnent standing point is extremely crucial to labelling
ill need to implment a distance to consider jumping from also, game speed may be a complication
drawing out 3 possbile lines because subway surfers always curves left?

Project has real world applications - self driving cars 

#25/04/2025

Logically it makes sense to stay on train tops as long as possbile to avoid getting blocked in (worst case we bail side)

Today I made the desicion to switch to computer vision for obstacle detection - currently using roboflow to train models
spent a few hours labelling data, plan is after this is to use this mdoel which has low accuracy and use it to label more data 
which I will hand review (faster than me drawing on each individual image) and then ill retrain the model. 

Will keep ground neon green logic as I think it may be useful for pathfinding and bail situiations. 
Also will stick to the classic version of Subway Surfers due to plentful amounts of videos online (10+ hours at least)

#26/04/2025

Switching from LUT to Yolo11 Computer vision for higher accruacy. 
need to make a tinder like assited data labeller, results from intital testing of 300 epochs and 400ish images were promising

30/04/2025
Models for CV made for Jake identification and Obstacle dtectionm
For jake I noticed that when he jumps his hitbox moves above 25% bottom level of screen on Phone and thus we can tell
if hes on the ground or in the air based off his hitbox location

Notable our model is 88% correct this is slightly undersestiamte as during my labels I did make mistakes which the machine
actually corrected for me, slight detrimental effect on learning unfortunately,

Thinking of doing a voting system for transformer contxt, different perspective may mitgate certain detection errors
assuming a 5% error rate (worst case) we could expect a chance of a bad vote in 16 frames to be around 10^-11 which is good

Of course feeding as context windows should boost performance signifcantly in my mind
Fine tuning is also still relvant here mabe PPO for performance boost

//Median filtering: for each frame, take the majority vote of ±k neighbors.

Hidden Markov Model (HMM): treat class predictions as observations and smooth with an HMM/Viterbi.

Kalman filter / particle filter: for continuous states like bounding boxes or poses.
These filters correct occasional blips by relying on past+future context, clipping stray errors.

Suggestions for chat GPT, may help good to keep in mind

//1. Build a discrete state–action model
States
Each frame, represent the player by:

Current lane (0/1/2)

Vertical phase: on ground, in jump arc (and how many frames into it)

Actions
Five discrete moves: stay, left, right, jump, duck.

Dynamics
You know exactly how jump evolves (parabolic arc over N frames) and how obstacles scroll (constant pixels/frame).


Could be an interesting concept too. Predicting the future and acting accordingly?
OR we use transformer + PPO fine tuning <- stick with this I think


3/5/25

Data imbalance is a real issue here, transofmer cannot learn on a 10:90 split of data

To be honest maybe I can hard code the solution so how do we expand our horizons
My thoughts on what we can do - 

Be 100% honest. My cv models can only identify v1.0 ibjects of the game - 
it would be cool for it to be able to do all objects in everygame version. 
Objects come in similar shapes but varying colours and textures. Objects behave the identical version to version. 
Would it be possible to train transformer on my hardcoded model then take the trasnformer and slowly implment different 
framing and blocks from diffferent versions to help its generlaisation. Thus after many many versions it can play any 
subwaysurfers game from any version?


Given that when we test a new version of Subway surfers the textures change, doesnt that mean we have to retrain the Vision transformer and also vision transformer how do we even train. Frames + movement labels? But how does that help the ViT to encode for the actual transformer we go from scratch

is there a better way to approach this?

1/8/25
Back at the project 3 months later, spent the day refactoring the code base and transfering files over from local to VS code

2/8/25
Trying to get the emulator working

docker run --privileged -d \
  --name android-emu \
  -p 6080:6080 \      
  -p 5555:5555 \      
  -e DEVICE="Nexus 5" \
  -e ANDROID_VERSION=9.0 \
  budtmo/docker-android-x86-9.0

open http://localhost:6080

docker cp ~/Downloads/1359788067_Subway_Surf_ARMv7.apk android-emu:/tmp/app.apk
docker exec android-emu adb install -r /tmp/app.apk

3/8/25
Still trying to get the emulator, VMS, Andrio studio, Device farms.
All incompatoible due to M1s inability to run a 32arm setup, the game we need is Subway Surfers 1.0 which only comes in 32arm (2012)

Solution: Was to use Parsec to mirror over an old laptop to my mac with shared keystrokes. 
Optimal settings: Full screen, 1280x720 or 1280 for full length lower latency

