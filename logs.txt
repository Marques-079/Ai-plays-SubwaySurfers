TODO: 
-URGENT: Pillar movement not processing pyautogui
-resolve ihibitinga switchback to GL after airtime jumps

- Panic state and making use of percieved 10fps reaction times, no stall on proximity
- top logic
- seperate CV for specials
- regular bail logic no delay
 -optimise out recomputes got identified a few 
- combo bail logic for boxed in (low priority)
- I suspect rebounds, we need stricter protocl on cooldowns after moves for GL

-anto coin logic of magent is enabled (pixel checks) coins -> green? 
-Need guard against stinky red lamposts vprtexing


<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

Models:
-> alpha
runv3.py : legacy model, stable logic, mss grab
runv4.py : compute analysis, toggle frame saves
runv5.py : shared ring memory, slight recallibration of parameters + auto shutdown and killswitch
runv6.py : introducing inference and postproc optimisations
runv7.py : Ground logic improved across the board, top logic scanners instated and dynamic prox for high speeds + more
runv8.py : QOL 
runv9.py : Top logic fixed, pillar logic fixed, few discrepencies left 
runv10.py : frame save funnel to flash drive -> collate dataset
runv11.py : afk for frame stacking, few bugs in logic (tunneling) left to fix


scgrab : swift compiled for shared memory ring + needs to be booted for shared memory 
./scgrab --x 644 --y 77 --w 505 --h 906 --fps 60 --out /tmp/scap.ring --slots 3 --scale 2 
./scgrab --x 483 --y 75 --w 505 --h 906 --fps 60 --out /tmp/scap.ring --slots 3 --scale 2  <- No ads


ring_grab.py : pull single frame from shared memory

<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>

hmmm wait so then cant we be greedy with our auto labeller? 
picking only the best state in the moment? Becuase when we train our transformer 
we give context 16ish frames and we predict what needs to happen in the given moment, 
regardless of the pass. A sucessfuly training siutation would be that we are mid air 
but want to move right we move right, and pass the past 8 frames (in which we jumped) 
into the transformer. So in game when a similar siutation comes up EG. 
we jumped (seen in last 8 frames) and now where do we want to move? to the right!

Establish a mechanism to label data given a single frame. Give nthe frame it will greedily pick the best region it wants 
to be in and then return the move which best achieves this goal. Very key stage here as not 
only labels our data but during inferemce this also makes decsions.

Train a transformer on CNN encoded frames paired with the augmented data labels created by pairing video frames with
the identification mechanism. This will allow for combos to be formed with a chain of images being the past moves
and the transformer predicts the next move (with context) unlike our greedy labeller. 

#24/04/2025

//Can we train our CNN on game state variables?

In inference we pass the frame into the CNN for encoding and pass those collection of frames into the transformer for prediction.
I dont think we shoud use the autolabeller in the actual inference pipeline because transformer should learn labels off the autolabeller
Im gonna use gamestate variables to train the auto labeller for high accuracy label generation.

//Starting on the auto labeller now 
Tossing up whether to do a decison tree or a hand coded if/else linchpin

Im thinking being able to identify high and low ground and currnent standing point is extremely crucial to labelling
ill need to implment a distance to consider jumping from also, game speed may be a complication
drawing out 3 possbile lines because subway surfers always curves left?

Project has real world applications - self driving cars 

#25/04/2025

Logically it makes sense to stay on train tops as long as possbile to avoid getting blocked in (worst case we bail side)

Today I made the desicion to switch to computer vision for obstacle detection - currently using roboflow to train models
spent a few hours labelling data, plan is after this is to use this mdoel which has low accuracy and use it to label more data 
which I will hand review (faster than me drawing on each individual image) and then ill retrain the model. 

Will keep ground neon green logic as I think it may be useful for pathfinding and bail situiations. 
Also will stick to the classic version of Subway Surfers due to plentful amounts of videos online (10+ hours at least)

#26/04/2025

Switching from LUT to Yolo11 Computer vision for higher accruacy. 
need to make a tinder like assited data labeller, results from intital testing of 300 epochs and 400ish images were promising

30/04/2025
Models for CV made for Jake identification and Obstacle dtectionm
For jake I noticed that when he jumps his hitbox moves above 25% bottom level of screen on Phone and thus we can tell
if hes on the ground or in the air based off his hitbox location

Notable our model is 88% correct this is slightly undersestiamte as during my labels I did make mistakes which the machine
actually corrected for me, slight detrimental effect on learning unfortunately,

Thinking of doing a voting system for transformer contxt, different perspective may mitgate certain detection errors
assuming a 5% error rate (worst case) we could expect a chance of a bad vote in 16 frames to be around 10^-11 which is good

Of course feeding as context windows should boost performance signifcantly in my mind
Fine tuning is also still relvant here mabe PPO for performance boost

//Median filtering: for each frame, take the majority vote of ±k neighbors.

Hidden Markov Model (HMM): treat class predictions as observations and smooth with an HMM/Viterbi.

Kalman filter / particle filter: for continuous states like bounding boxes or poses.
These filters correct occasional blips by relying on past+future context, clipping stray errors.

Suggestions for chat GPT, may help good to keep in mind

//1. Build a discrete state–action model
States
Each frame, represent the player by:

Current lane (0/1/2)

Vertical phase: on ground, in jump arc (and how many frames into it)

Actions
Five discrete moves: stay, left, right, jump, duck.

Dynamics
You know exactly how jump evolves (parabolic arc over N frames) and how obstacles scroll (constant pixels/frame).


Could be an interesting concept too. Predicting the future and acting accordingly?
OR we use transformer + PPO fine tuning <- stick with this I think


3/5/25

Data imbalance is a real issue here, transofmer cannot learn on a 10:90 split of data

To be honest maybe I can hard code the solution so how do we expand our horizons
My thoughts on what we can do - 

Be 100% honest. My cv models can only identify v1.0 ibjects of the game - 
it would be cool for it to be able to do all objects in everygame version. 
Objects come in similar shapes but varying colours and textures. Objects behave the identical version to version. 
Would it be possible to train transformer on my hardcoded model then take the trasnformer and slowly implment different 
framing and blocks from diffferent versions to help its generlaisation. Thus after many many versions it can play any 
subwaysurfers game from any version?


Given that when we test a new version of Subway surfers the textures change, doesnt that mean we have to retrain the Vision transformer and also vision transformer how do we even train. Frames + movement labels? But how does that help the ViT to encode for the actual transformer we go from scratch

is there a better way to approach this?

1/8/25
Back at the project 3 months later, spent the day refactoring the code base and transfering files over from local to VS code

2/8/25
Trying to get the emulator working

docker run --privileged -d \
  --name android-emu \
  -p 6080:6080 \      
  -p 5555:5555 \      
  -e DEVICE="Nexus 5" \
  -e ANDROID_VERSION=9.0 \
  budtmo/docker-android-x86-9.0

open http://localhost:6080

docker cp ~/Downloads/1359788067_Subway_Surf_ARMv7.apk android-emu:/tmp/app.apk
docker exec android-emu adb install -r /tmp/app.apk

3/8/25
Still trying to get the emulator, VMS, Andrio studio, Device farms.
All incompatoible due to M1s inability to run a 32arm setup, the game we need is Subway Surfers 1.0 which only comes in 32arm (2012)

Solution: Was to use Parsec to mirror over an old laptop to my mac with shared keystrokes. 
Optimal settings: Full screen, 1280x720 or 1280x800 for full length lower latency

4/8/25
Got keystrokes working with mirror and the screnshot pipeline optimised to sub 150ms passes between the screen and our processor code
Got inference down to less than 50ms for a full shot which is extremely good 
Will need to improve rail detection and maybe use a curr lane state to know where i can move

12:00pm
Is there something I can do to pick up these generaly rectangular squares between rails, 
then all I do is draw an arrow inside the square and get a vector to point to the next square forward, if it find another square then we continue, 
if it finds an obstacle then we can halt search.

5/8/25
Spent the day optimising for speed, in the end got TRUE inference and post processing down to around 150-175ms per frame,
need wiggle room for the above ground logic -> TBF we only need to run one at once. 
All code under bruh_another_rails4.ipynb

6/8/25
Skipped

7/8/25
Assignments

8/8/25
Assignments

9/8/25
We can know our lane by keeping underlying state, We also know coords where the base of the heatmap of any detectable lanes would be present.
We can look at the heatmap 'jake' is currently in (using lane + preset heatmap start coords) -> To find which arrow is relevant to jake
ofc if we are lane 0 (leftmost) and an arrow is to the left then ignore. 

Try to stay optimal line aka clear path for as long as possible
If blocked then enter bail mode, scans for any rails on right if clear then move -> if avaliable but not clear but proximity alert then override
Check arrow right -> check rails block on right on coords + offset for transition? -> If clear: then can move

'''
As you can see here the purple triangles allow me to plot onto lanes, when a triangle is sitting under an object it gets pushed down by the obstacle this way we can simply look to the mask where the arrow points (above the arow) to locate the given obstacle, based off this we can choose optimal lanes to be in. 

How? We know Jakes lane because we can remeber what keys weve pressed, we also utilise a heatmap (seen in code above) which shows roughly a given rail. Combine this info the find which heatmapped/rail jake is on and we can use this to find if we are in a good lane or not. Then we can look at triangls positined left and right of our main jake-rail-triangle to find if other lanes are clear. 

The reason I need to tdo this is sometims triangles may appear on the side as misreading and this we need to know the triangle relevant to our lane so we can ignore others more left (if we are leftmost lane). Get what I mean? Explain it back to me so we are clear on how this should work.

Movement logic is the following.
Try to stay optimal line aka clear path for as long as possible
Check arrow right -> check rails blocked ahead or not. If so check other valid triangles lanes -> If mask there is rails aka space we can move into them move.
'''

When plotting the vector which matches degrees the closest for a given lane will be jake's 
'''
(800, 720) left lane : 10.7 degrees  
(890, 720) middle lane : 1.5 degrees
(980, 720) right lane : -15.0 degrees
'''

10/8/25
Finalised lanes, and corresponding arrows/obstacle detection 
Planning logic for ground movement:
 - If in greenlane then stay in greenlane 
 - <Prioritise moving up ramps?> 
 - Avoid being in redlanes -> If in redlane and proximity limit triggered look for any escape and execute 
 - For yellow indicators -> Jump when N pixels out from object and trigger down after 0.5 secs (minimise airtime) + Ignore all signals while in air
 - If switching lanes then run a check there is not an obstacle which hasnt moved out of view yet 
 - Ignore any vectors which arent between +/- 45
 - Always roll under High and Low barriers v1s, Jump over Low Barriers 2

 - Need a ramp state -> Can blip scan on hardcoded point for Jake's lane and have a trigger switch for consecutive ramp readings -> We know dismount because
 jumping between trains we discard scanning. If blip touches ground consecutive and != airtime then regrounded 
 - Isolate 3D plane into a unwarped 2D birdseye plane to find optimal 

11/08/25
Big progress on logic movement. 
First runs implmented im skeptical on the validitu although its showing some promise 
Moreso patching leaks and flaws in the ground hardcode. 

I think the greatest fix will be to buffer out the slower inference. This can be done by using speed to calculated when we should jump rather than a frame by frame basis. 

Proximity equation: y=0.00139754 * 483.35867^x

This shows that there is an exponential relationship between pxl distance and time till jumptime y is seconds till jumptime what does this show. 
Would the correct logic be to say if we have one triangle and red/yellow classed obstacles we engage our action timer. 

Splice our logic into two paths to sperate inference from reaction.
 - If only yellow triangles or one yellow triangle then activate poximity equation. 
 - If a single red or multiple red enter jump mode -> excepetionally fast scanning for immediate exit. 

'''
We need to do a complete logic overhaul here. For the following classes.
       2: "HIGHBARRIER1", 3: "JUMP",4 : "LOWBARRIER1", 5: "LOWBARRIER2",
When there is only a single triangle and it is detecting one of the following classes  it should set a global timer to initate the specific movement 'up arrow for jump and lowbarier2 and 
down arrow for lowbarrier1 and highbarrier1. the timer is callibrated to Proximity equation: y=0.00139754 * 483.35867^x where x is the positive distance away from the object  in pxls /100 
(how its calculated is in code already) and y * 100 * 1000 is the seconds away it is from impact. The timer is either overwritten or set if distance is > 400pxls away and < 800 pxls away. 
'''
 
 BEST_SCORE_ACHIEVED = 1530

Smaller increments on poximity equation
Inference lockdown between states after movements for 360ms

 BEST_SCORE_ACHIEVED - 2500

Need to add exit bail out now aka PANIC MODE

12/08
Very specific setup but is the following possible
Arm32 running bluestacks 5 to runsubway surfers, all while running a script in background which takes frames analyses in less than 
100ms using alot of compute for inference and then movign the character in gae live in bs with no overhead all in a VM? Is it possivble and how much deos it cost?

Considering our options the 32arm and access to a beefy GPU for a lowish costs is very difficult. 
I think the best would be to highly optimise our code and ensure very optimal flow of data and saturate my GPU and CPU much much better, also try to cut down in the transmmision time between
parsec and my local device. Maybe I can link my windows and m1 through parsec using a cable to eliminate latency times and feedback. Swap to a swift architiectue to cut down frame capture from 40ms to 2ms exactly

Planning architiectue for model and model runtime:
Requirements: 

OS
-Arm32 to emulate subwaysurfersv1.apk
-Powerful(ush) GPU to fully saturate for inference 
-Join Windows to M1 with cable to avoid WAN latency (if not runnung VM)

PIPELINE
-Use swift or fast language to capture and proccess frames 
-Highly optimised processing pipeline for hardcode


Options: 

Full local -> M1 MPS and Parsec mirroring.
Run everything local so +50ms parsec for the M1 <-> Windows loop + IDEALLY 20-50ms + travel times 50ms between cloud 
Running a full VM emulating the game + hooked to a GPU (hardest and expensive but best) Ultralow latency 

BEST_SCORE_ACHIEVED - 2931
BEST_SCORE_ACHIEVED - 3354
BEST_SCORE_ACHIEVED - 3666

13/08/25

Need to reaclibrate speed curve for higher speeds. Global timer etc
Need more gas for post proc inference by spreading and chaining onto GPU 

Need to improve baselie 'smarts' level
With higher sampling more room for error -> Prone to moving back into the corner of object due to coins 
Pillas yellow claass poximity to red?

So we have around 120% / 800% cores being used with runV3 and around 35% of our total GPU utlisation. Meaning we have a lot of room for ocmpute expansion

#sudo powermetrics --samplers gpu_power -i 200 for GPU usage although Activity Monitor MacOS also has a more accurate read on GPU utilisation

=======DEBUG FOR SHARE MEMORY RING=========
swiftc -O -parse-as-library -framework ScreenCaptureKit -framework AVFoundation -o scgrab scgrab.swift

pkill -f scgrab 2>/dev/null || true
pkill -f sc_reader.py 2>/dev/null || true

rm -f /tmp/scap.ring

tccutil reset ScreenCapture com.microsoft.VSCode

./scgrab --x 0 --y 0 --w 1010 --h 1812 --fps 120 --out /tmp/scap.ring --slots 3.  <---OLD 
./scgrab --x 644 --y 77 --w 504 --h 906 --fps 120 --out /tmp/scap.ring --slots 3. <---OLD



python3 sc_reader.py
python3 sc_grab10.py /tmp/scap.ring ./sc_frames_test

==========================================

14/08/25

Save frames with labels included in total time taken 


16/08/25
runv5.py introduces a new frame python/swift pipeline for fast frame retrival and usage

./scgrab --x 644 --y 77 --w 505 --h 906 --fps 60 --out /tmp/scap.ring --slots 3 --scale 2  <----Latest

swiftc -O -parse-as-library scgrab.swift -o scgrab \-framework ScreenCaptureKit -framework AVFoundation  <-- Compiler for dodgin @main error

prone to moving into trains with feint greens 

BEST_SCORE_ACHIEVED : 5274

17/08/25

BEST_SCORE_ACHIEVED : 5283 - 108.7ms

Observed that while ontop of trains we can hard code spefic lines to "thread the needle" all trains in a certain lane will guaranteed intersect a line at some point. 
Tiebreakers can look for majority within a certain line. Preventing misreads.
Observed that trains will always be the same type sequentially if connected immediatidely, trains of same type can exist side by side. Trains of different types can be in the same lane IF there is a gap between them. 
MID:
    {"name": "LEFT",  "start": (0, 1600), "length": 1000, "angle_deg": +19.0},
    {"name": "MID",   "start": (490, 1800), "length": 1300, "angle_deg":  -1.5},
    {"name": "RIGHT", "start": (1010, 1600), "length": 1000, "angle_deg": -23.0},
LEFT
    {"name": "LEFT",  "start": (375, 1800), "length": 1200, "angle_deg": +2.0},
    {"name": "MID",   "start": (1010, 1800), "length": 1300, "angle_deg":  -21.5},
RIGHT:
    {"name": "LEFT",  "start": (100, 1800), "length": 1100, "angle_deg": +12.3},
    {"name": "MID",   "start": (650, 1800), "length": 1200, "angle_deg":  -8.0},


Max speed is achieved around 2mins "allegedly" 

-Roll downwards on trainline ends or out of reach to give ground model best chance 

Added green trianlge screenigns to prevent gravity pulls into trains due to coins
Buffered Orange wall logic 

Score: 5439 on legacy 200ms 


18/8/25
We can lnow the size of the front face of trains due to the y-axis level it is on the screen <- irrelvsant in our logic

Findings conclude mvoement time on average is:
[
Can we control based on set pixel detecting white or not? 
0.332secs for side moves 
0.919secs for all jump moves train 

]

19/8/25
Dynamic adjusting proximity equation is redundant -> Maybe just use: When global time > X then adjust offset by X seconds in O(1) lookup

TODO TN: 
- Reinforce Jake lane logic for ontop of trains
- Apply function of O(1) proximity table with respect to time in calculating impact distance
- State for knowing if we are ontop of a train or not. 
{
while scanning: 
    if standing on train mask:
        we are on train
    elif:
        hit jump recently and pushing a switch then keep state
    else:
        ground logic

}
- Code logic for train to train movement. 1. Simple left right moves 2. Jumping straight / diagonal (2 move combo) moves. 
- Exit logic. If no more trains and on train then +0.1 to proximity hit roll 


Pillar regions:
{
    Proximity classifcation on pillars. Two options. 
    A. Yellow until less than N pixels then pilalr turns red. Forces jake into centre then flushes out to opposite side CV reliant
    B. Double moves only looking at rightmost or leftmost lane and double moving when either turns red. (need guard against running into a pillar nbut instant moves should negate this error)
}


24/08/25

Updating CV model
Bush1 - Stabdard single Bush
Bush2- Bush with red box in middle
Bush3 - Bush pair 
Bin - Rubbish Bin
Jump - Jump onto sidealks

25/08 

HIGH_SCORE : 7251

26/08
Refocus8 + runv8 
Improved ground logic 

Top of train laneswap logic should be, 
When can see ground masks after tip of yellow arrow - valid to arm timer set to jump and lock (1). 
While timer is active and still scanning reviews a band 1100 - 1300ish for any train marked lines, picks largest one and moves into that if Foundation
If nothing when timer triggers then scan for valid jump + move sequences (1) if none then roll down off train enter GL 


Pillar logic fires correctly but doesnt execute? 
Lane muddling on sidewalk regions

Solved Vortes, Sidewalks + pillar logic resolve?

28/08/25

TODO today streamlined:
- Panic mode bail logic
- Callibrate top logic lat movement
- Traceup jumping + conbos top movement 
- No train && top -> roll down








==================================================================================================DEBUG FOR SCGRAB STARTUP===============================================================================================================

(.venv) marcus@Marcuss-MacBook-Pro Ai-plays-SubwaySurfers % ls -lah ./scgrab

-rwxr-xr-x@ 1 marcus  staff    98K Aug 17 23:50 ./scgrab
(.venv) marcus@Marcuss-MacBook-Pro Ai-plays-SubwaySurfers % file ./scgrab

./scgrab: Mach-O 64-bit executable arm64
(.venv) marcus@Marcuss-MacBook-Pro Ai-plays-SubwaySurfers % otool -L ./scgrab

./scgrab:
        /System/Library/Frameworks/ScreenCaptureKit.framework/Versions/A/ScreenCaptureKit (compatibility version 1.0.0, current version 1.0.0)
        /System/Library/Frameworks/AVFoundation.framework/Versions/A/AVFoundation (compatibility version 1.0.0, current version 2.0.0)
        /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1351.0.0)
        /System/Library/Frameworks/CoreMedia.framework/Versions/A/CoreMedia (compatibility version 1.0.0, current version 3185.18.5)
        /System/Library/Frameworks/CoreVideo.framework/Versions/A/CoreVideo (compatibility version 1.2.0, current version 1.5.0)
        /System/Library/Frameworks/Foundation.framework/Versions/C/Foundation (compatibility version 300.0.0, current version 3208.0.0)
        /usr/lib/libobjc.A.dylib (compatibility version 1.0.0, current version 228.0.0)
        /usr/lib/swift/libswiftAVFoundation.dylib (compatibility version 1.0.0, current version 2305.18.1, weak)
        /usr/lib/swift/libswiftCore.dylib (compatibility version 1.0.0, current version 0.0.0)
        /usr/lib/swift/libswiftCoreAudio.dylib (compatibility version 1.0.0, current version 328.255.0, weak)
        /usr/lib/swift/libswiftCoreFoundation.dylib (compatibility version 1.0.0, current version 120.100.0, weak)
        /usr/lib/swift/libswiftCoreImage.dylib (compatibility version 1.0.0, current version 2.0.0, weak)
        /usr/lib/swift/libswiftCoreMIDI.dylib (compatibility version 1.0.0, current version 6.0.0, weak)
        /usr/lib/swift/libswiftCoreMedia.dylib (compatibility version 1.0.0, current version 3185.18.5)
        /usr/lib/swift/libswiftDarwin.dylib (compatibility version 1.0.0, current version 234.60.42)
        /usr/lib/swift/libswiftDispatch.dylib (compatibility version 1.0.0, current version 49.0.0)
        /usr/lib/swift/libswiftIOKit.dylib (compatibility version 1.0.0, current version 1.0.0, weak)
        /usr/lib/swift/libswiftMetal.dylib (compatibility version 1.0.0, current version 367.6.0, weak)
        /usr/lib/swift/libswiftOSLog.dylib (compatibility version 1.0.0, current version 8.0.0, weak)
        /usr/lib/swift/libswiftObjectiveC.dylib (compatibility version 1.0.0, current version 10.0.0, weak)
        /usr/lib/swift/libswiftQuartzCore.dylib (compatibility version 1.0.0, current version 4.0.0, weak)
        /usr/lib/swift/libswiftUniformTypeIdentifiers.dylib (compatibility version 1.0.0, current version 827.5.1, weak)
        /usr/lib/swift/libswiftXPC.dylib (compatibility version 1.0.0, current version 62.0.1, weak)
        /usr/lib/swift/libswift_Builtin_float.dylib (compatibility version 1.0.0, current version 0.0.0, weak)
        /usr/lib/swift/libswift_Concurrency.dylib (compatibility version 1.0.0, current version 0.0.0)
        /usr/lib/swift/libswift_errno.dylib (compatibility version 1.0.0, current version 234.60.42, weak)
        /usr/lib/swift/libswift_math.dylib (compatibility version 1.0.0, current version 234.60.42, weak)
        /usr/lib/swift/libswift_signal.dylib (compatibility version 1.0.0, current version 234.60.42, weak)
        /usr/lib/swift/libswift_stdio.dylib (compatibility version 1.0.0, current version 234.60.42)
        /usr/lib/swift/libswift_time.dylib (compatibility version 1.0.0, current version 234.60.42, weak)
        /usr/lib/swift/libswiftos.dylib (compatibility version 1.0.0, current version 1062.0.2, weak)
        /usr/lib/swift/libswiftsimd.dylib (compatibility version 1.0.0, current version 21.0.0, weak)
        /usr/lib/swift/libswiftsys_time.dylib (compatibility version 1.0.0, current version 234.60.42, weak)
        /usr/lib/swift/libswiftunistd.dylib (compatibility version 1.0.0, current version 234.60.42, weak)
(.venv) marcus@Marcuss-MacBook-Pro Ai-plays-SubwaySurfers % ./scgrab --help

SCStream running: ROI 640x480 → out 1280x960 @60 (scale 2.0x)
[producer] wrote 3 frames in the last second
[producer] wrote 2 frames in the last second
[producer] wrote 36 frames in the last second
[producer] wrote 42 frames in the last second
^C
(.venv) marcus@Marcuss-MacBook-Pro Ai-plays-SubwaySurfers % ./scgrab --x 644 --y 77 --w 505 --h 906 --fps 60 --out /tmp/scap.ring --slots 3 --scale 2 

18/09/25
Back at it again 

TODO:
Panic state 
Straight jumpings TL. 
Timing logic for TLS approx on time 
Diagonal TL jump + move combos 

19/09/25'
Setting up data pipeline
What is the most efficient way of being able to label frames with movements when made. Two requirements: Some movements are made in the TL script which are untracable to this code where our logic for saving will lie We already have the screenshot extracted ib BGR form as a variable so that is what we'll wnat to write to the drive with the correspond movement label At 0.25 sec intervals no movement snapshots should be taken, so if no movement in last 0.5 seconds then we start just takening snapshots and labelling accordingly. Dont write code but lets think about how to set this up. We will also want to encode composties AND basic movements, so we will want to hand it the follow in labels (just so i have it rather thannot). (Key pressed, Composite (LMSE codes), current lane. Anything to add to this list? Articulate thoughts and return to me a full response on thpughts suggesttions etc


28/09
dropoff will clear all framesd from top dismount 
death frame removals 
faster inference >20ms 

how to deal with multiple options
VID trans means more data -> what do we actually need and no overhead while train runs 

30/09

Tunnel logic is broken 
hypergreen ramps logic 

Everything else is primed for data funneling = collection 
