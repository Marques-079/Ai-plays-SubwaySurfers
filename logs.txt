hmmm wait so then cant we be greedy with our auto labeller? 
picking only the best state in the moment? Becuase when we train our transformer 
we give context 16ish frames and we predict what needs to happen in the given moment, 
regardless of the pass. A sucessfuly training siutation would be that we are mid air 
but want to move right we move right, and pass the past 8 frames (in which we jumped) 
into the transformer. So in game when a similar siutation comes up EG. 
we jumped (seen in last 8 frames) and now where do we want to move? to the right!

Establish a mechanism to label data given a single frame. Give nthe frame it will greedily pick the best region it wants 
to be in and then return the move which best achieves this goal. Very key stage here as not 
only labels our data but during inferemce this also makes decsions.

Train a transformer on CNN encoded frames paired with the augmented data labels created by pairing video frames with
the identification mechanism. This will allow for combos to be formed with a chain of images being the past moves
and the transformer predicts the next move (with context) unlike our greedy labeller. 

#24/04/2025

//Can we train our CNN on game state variables?

In inference we pass the frame into the CNN for encoding and pass those collection of frames into the transformer for prediction.
I dont think we shoud use the autolabeller in the actual inference pipeline because transformer should learn labels off the autolabeller
Im gonna use gamestate variables to train the auto labeller for high accuracy label generation.

//Starting on the auto labeller now 
Tossing up whether to do a decison tree or a hand coded if/else linchpin

Im thinking being able to identify high and low ground and currnent standing point is extremely crucial to labelling
ill need to implment a distance to consider jumping from also, game speed may be a complication
drawing out 3 possbile lines because subway surfers always curves left?

Project has real world applications - self driving cars 

#25/04/2025

Logically it makes sense to stay on train tops as long as possbile to avoid getting blocked in (worst case we bail side)

Today I made the desicion to switch to computer vision for obstacle detection - currently using roboflow to train models
spent a few hours labelling data, plan is after this is to use this mdoel which has low accuracy and use it to label more data 
which I will hand review (faster than me drawing on each individual image) and then ill retrain the model. 

Will keep ground neon green logic as I think it may be useful for pathfinding and bail situiations. 
Also will stick to the classic version of Subway Surfers due to plentful amounts of videos online (10+ hours at least)

#26/04/2025

Switching from LUT to Yolo11 Computer vision for higher accruacy. 
need to make a tinder like assited data labeller, results from intital testing of 300 epochs and 400ish images were promising

30/04/2025
Models for CV made for Jake identification and Obstacle dtectionm
For jake I noticed that when he jumps his hitbox moves above 25% bottom level of screen on Phone and thus we can tell
if hes on the ground or in the air based off his hitbox location

Notable our model is 88% correct this is slightly undersestiamte as during my labels I did make mistakes which the machine
actually corrected for me, slight detrimental effect on learning unfortunately,

Thinking of doing a voting system for transformer contxt, different perspective may mitgate certain detection errors
assuming a 5% error rate (worst case) we could expect a chance of a bad vote in 16 frames to be around 10^-11 which is good

Of course feeding as context windows should boost performance signifcantly in my mind
Fine tuning is also still relvant here mabe PPO for performance boost

//Median filtering: for each frame, take the majority vote of ±k neighbors.

Hidden Markov Model (HMM): treat class predictions as observations and smooth with an HMM/Viterbi.

Kalman filter / particle filter: for continuous states like bounding boxes or poses.
These filters correct occasional blips by relying on past+future context, clipping stray errors.

Suggestions for chat GPT, may help good to keep in mind

//1. Build a discrete state–action model
States
Each frame, represent the player by:

Current lane (0/1/2)

Vertical phase: on ground, in jump arc (and how many frames into it)

Actions
Five discrete moves: stay, left, right, jump, duck.

Dynamics
You know exactly how jump evolves (parabolic arc over N frames) and how obstacles scroll (constant pixels/frame).


Could be an interesting concept too. Predicting the future and acting accordingly?
OR we use transformer + PPO fine tuning <- stick with this I think


3/5/25

Data imbalance is a real issue here, transofmer cannot learn on a 10:90 split of data

To be honest maybe I can hard code the solution so how do we expand our horizons
My thoughts on what we can do - 

Be 100% honest. My cv models can only identify v1.0 ibjects of the game - 
it would be cool for it to be able to do all objects in everygame version. 
Objects come in similar shapes but varying colours and textures. Objects behave the identical version to version. 
Would it be possible to train transformer on my hardcoded model then take the trasnformer and slowly implment different 
framing and blocks from diffferent versions to help its generlaisation. Thus after many many versions it can play any 
subwaysurfers game from any version?


Given that when we test a new version of Subway surfers the textures change, doesnt that mean we have to retrain the Vision transformer and also vision transformer how do we even train. Frames + movement labels? But how does that help the ViT to encode for the actual transformer we go from scratch

is there a better way to approach this?

1/8/25
Back at the project 3 months later, spent the day refactoring the code base and transfering files over from local to VS code

2/8/25
Trying to get the emulator working

docker run --privileged -d \
  --name android-emu \
  -p 6080:6080 \      
  -p 5555:5555 \      
  -e DEVICE="Nexus 5" \
  -e ANDROID_VERSION=9.0 \
  budtmo/docker-android-x86-9.0

open http://localhost:6080

docker cp ~/Downloads/1359788067_Subway_Surf_ARMv7.apk android-emu:/tmp/app.apk
docker exec android-emu adb install -r /tmp/app.apk

3/8/25
Still trying to get the emulator, VMS, Andrio studio, Device farms.
All incompatoible due to M1s inability to run a 32arm setup, the game we need is Subway Surfers 1.0 which only comes in 32arm (2012)

Solution: Was to use Parsec to mirror over an old laptop to my mac with shared keystrokes. 
Optimal settings: Full screen, 1280x720 or 1280x800 for full length lower latency

4/8/25
Got keystrokes working with mirror and the screnshot pipeline optimised to sub 150ms passes between the screen and our processor code
Got inference down to less than 50ms for a full shot which is extremely good 
Will need to improve rail detection and maybe use a curr lane state to know where i can move

12:00pm
Is there something I can do to pick up these generaly rectangular squares between rails, 
then all I do is draw an arrow inside the square and get a vector to point to the next square forward, if it find another square then we continue, 
if it finds an obstacle then we can halt search.

5/8/25
Spent the day optimising for speed, in the end got TRUE inference and post processing down to around 150-175ms per frame,
need wiggle room for the above ground logic -> TBF we only need to run one at once. 
All code under bruh_another_rails4.ipynb

6/8/25
Skipped

7/8/25
Assignments

8/8/25
Assignments

9/8/25
We can know our lane by keeping underlying state, We also know coords where the base of the heatmap of any detectable lanes would be present.
We can look at the heatmap 'jake' is currently in (using lane + preset heatmap start coords) -> To find which arrow is relevant to jake
ofc if we are lane 0 (leftmost) and an arrow is to the left then ignore. 

Try to stay optimal line aka clear path for as long as possible
If blocked then enter bail mode, scans for any rails on right if clear then move -> if avaliable but not clear but proximity alert then override
Check arrow right -> check rails block on right on coords + offset for transition? -> If clear: then can move

'''
As you can see here the purple triangles allow me to plot onto lanes, when a triangle is sitting under an object it gets pushed down by the obstacle this way we can simply look to the mask where the arrow points (above the arow) to locate the given obstacle, based off this we can choose optimal lanes to be in. 

How? We know Jakes lane because we can remeber what keys weve pressed, we also utilise a heatmap (seen in code above) which shows roughly a given rail. Combine this info the find which heatmapped/rail jake is on and we can use this to find if we are in a good lane or not. Then we can look at triangls positined left and right of our main jake-rail-triangle to find if other lanes are clear. 

The reason I need to tdo this is sometims triangles may appear on the side as misreading and this we need to know the triangle relevant to our lane so we can ignore others more left (if we are leftmost lane). Get what I mean? Explain it back to me so we are clear on how this should work.

Movement logic is the following.
Try to stay optimal line aka clear path for as long as possible
Check arrow right -> check rails blocked ahead or not. If so check other valid triangles lanes -> If mask there is rails aka space we can move into them move.
'''

When plotting the vector which matches degrees the closest for a given lane will be jake's 
'''
(800, 720) left lane : 10.7 degrees  
(890, 720) middle lane : 1.5 degrees
(980, 720) right lane : -15.0 degrees
'''

10/8/25
Finalised lanes, and corresponding arrows/obstacle detection 
Planning logic for ground movement:
 - If in greenlane then stay in greenlane 
 - <Prioritise moving up ramps?> 
 - Avoid being in redlanes -> If in redlane and proximity limit triggered look for any escape and execute 
 - For yellow indicators -> Jump when N pixels out from object and trigger down after 0.5 secs (minimise airtime) + Ignore all signals while in air
 - If switching lanes then run a check there is not an obstacle which hasnt moved out of view yet 
 - Ignore any vectors which arent between +/- 45
 - Always roll under High and Low barriers v1s, Jump over Low Barriers 2

 - Need a ramp state -> Can blip scan on hardcoded point for Jake's lane and have a trigger switch for consecutive ramp readings -> We know dismount because
 jumping between trains we discard scanning. If blip touches ground consecutive and != airtime then regrounded 
 - Isolate 3D plane into a unwarped 2D birdseye plane to find optimal 

11/08/25
Big progress on logic movement. 
First runs implmented im skeptical on the validitu although its showing some promise 
Moreso patching leaks and flaws in the ground hardcode. 

I think the greatest fix will be to buffer out the slower inference. This can be done by using speed to calculated when we should jump rather than a frame by frame basis. 

Proximity equation: y=0.00139754 * 483.35867^x

This shows that there is an exponential relationship between pxl distance and time till jumptime y is seconds till jumptime what does this show. 
Would the correct logic be to say if we have one triangle and red/yellow classed obstacles we engage our action timer. 

Splice our logic into two paths to sperate inference from reaction.
 - If only yellow triangles or one yellow triangle then activate poximity equation. 
 - If a single red or multiple red enter jump mode -> excepetionally fast scanning for immediate exit. 

'''
We need to do a complete logic overhaul here. For the following classes.
       2: "HIGHBARRIER1", 3: "JUMP",4 : "LOWBARRIER1", 5: "LOWBARRIER2",
When there is only a single triangle and it is detecting one of the following classes  it should set a global timer to initate the specific movement 'up arrow for jump and lowbarier2 and 
down arrow for lowbarrier1 and highbarrier1. the timer is callibrated to Proximity equation: y=0.00139754 * 483.35867^x where x is the positive distance away from the object  in pxls /100 
(how its calculated is in code already) and y * 100 * 1000 is the seconds away it is from impact. The timer is either overwritten or set if distance is > 400pxls away and < 800 pxls away. 
'''
 
 BEST_SCORE_ACHIEVED = 1530

Smaller increments on poximity equation
Inference lockdown between states after movements for 360ms

 BEST_SCORE_ACHIEVED - 2500

Need to add exit bail out now aka PANIC MODE

12/08
Very specific setup but is the following possible
Arm32 running bluestacks 5 to runsubway surfers, all while running a script in background which takes frames analyses in less than 
100ms using alot of compute for inference and then movign the character in gae live in bs with no overhead all in a VM? Is it possivble and how much deos it cost?

Considering our options the 32arm and access to a beefy GPU for a lowish costs is very difficult. 
I think the best would be to highly optimise our code and ensure very optimal flow of data and saturate my GPU and CPU much much better, also try to cut down in the transmmision time between
parsec and my local device. Maybe I can link my windows and m1 through parsec using a cable to eliminate latency times and feedback. Swap to a swift architiectue to cut down frame capture from 40ms to 2ms exactly

Planning architiectue for model and model runtime:
Requirements: 

OS
-Arm32 to emulate subwaysurfersv1.apk
-Powerful(ush) GPU to fully saturate for inference 
-Join Windows to M1 with cable to avoid WAN latency (if not runnung VM)

PIPELINE
-Use swift or fast language to capture and proccess frames 
-Highly optimised processing pipeline for hardcode


Options: 

Full local -> M1 MPS and Parsec mirroring.
Run everything local so +50ms parsec for the M1 <-> Windows loop + IDEALLY 20-50ms + travel times 50ms between cloud 
Running a full VM emulating the game + hooked to a GPU (hardest and expensive but best) Ultralow latency 

BEST_SCORE_ACHIEVED - 2931
BEST_SCORE_ACHIEVED - 3354
BEST_SCORE_ACHIEVED - 3666

13/08/25

Need to reaclibrate speed curve for higher speeds. Global timer etc
Need more gas for post proc inference by spreading and chaining onto GPU 

Need to improve baselie 'smarts' level
With higher sampling more room for error -> Prone to moving back into the corner of object due to coins 
Pillas yellow claass poximity to red?

So we have around 120% / 800% cores being used with runV3 and around 35% of our total GPU utlisation. Meaning we have a lot of room for ocmpute expansion


